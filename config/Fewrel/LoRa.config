[train] #train parameters
epoch = 20
batch_size = 4

shuffle = True

reader_num = 4


optimizer = AdamW
learning_rate = 1e-3
weight_decay = 1e-5
step_size = 1
lr_multiplier = 1

max_len = 128

warmup_steps=2000
training_steps=20000
max_grad_norm=1.0
fp16=False

mlm_prob = 0.15

valid_mode=batch

lora_r=32
lora_alpha=64

nquery=5
nways=5
nshot=1

[eval] #eval parameters
batch_size = 8

shuffle = False

reader_num = 4

[distributed]
use = True
backend = nccl

[data] #data parameters
train_dataset_type = Fewrel
train_formatter_type = Fewrel
train_data_path = /data2/private/zengzhiyuan/datasets/FewRel/train_wiki.json

valid_dataset_type = Fewrel
valid_formatter_type = Fewrel
valid_data_path = /data2/private/zengzhiyuan/datasets/FewRel/val_wiki.json

test_dataset_type = Fewrel
test_formatter_type = Fewrel
test_data_path = /data2/private/zengzhiyuan/datasets/FewRel/test_fewPubmed.json

[model] #model parameters
model_name = Fewrel
pretrained_model=roberta-base

[output] #output parameters
output_time = 100
test_time = 1

model_path = checkpoints
model_name = Fewrel-Lora

output_function = binary

