[train] #train parameters
epoch = 20
batch_size = 32

shuffle = True

reader_num = 8

grad_accumulate = 2

optimizer = AdamW
learning_rate = 3e-4
weight_decay = 1e-5
step_size = 1
lr_multiplier = 1

doc_len = 512

warmup_steps=3000
training_steps=200000
max_grad_norm=1.0
fp16=False

mux_num = 4
mlm_prob = 0.15

valid_mode=step
step_epoch=1000

[eval] #eval parameters
batch_size = 64

shuffle = False

reader_num = 4

[distributed]
use = True
backend = nccl

[data] #data parameters
train_dataset_type = kara
train_formatter_type = mlm
train_data_path = /data/home/scv0540/xcj/ReadOnce/data/wikipedia/wiki-kara
kara_namespace = wikipedia
kara_dataset = wiki-512
kara_version = 1st

valid_dataset_type = Raw
valid_formatter_type = mlm
valid_data_path = /data/home/scv0540/xcj/datamux/data/squad_docs.json

test_dataset_type = Raw
test_formatter_type = mapper-os
test_data_path = /data/home/scv0540/xcj/ReadOnce/data/RACE/test.json


[model] #model parameters
model_name = mlm
pretrained_model=/data/home/scv0540/xcj/PLMs/bert-base-uncased

[output] #output parameters
output_time = 100
test_time = 1

model_path = /data/home/scv0540/xcj/datamux/checkpoint
model_name = mlm-scratch

output_function = avgloss

