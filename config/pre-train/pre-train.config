[train] #train parameters
epoch = 20
batch_size = 32

shuffle = True

reader_num = 4


optimizer = AdamW
learning_rate = 1e-5
weight_decay = 1e-5
step_size = 1
lr_multiplier = 1

ctx_len = 400
query_len = 128

warmup_steps=20000
training_steps=200000
max_grad_norm=1.0
fp16=False


valid_mode=step
step_epoch=5000


grad_accumulate=1

no_valid = True

[eval] #eval parameters
batch_size = 8

shuffle = False

reader_num = 4

[distributed]
use = True
backend = nccl

[data] #data parameters
train_dataset_type = kara
train_formatter_type = MLMPlugD
train_data_path = /data/xiaochaojun/Rerank/data
train_kara_namespace = wiki
train_kara_dataset = train
train_kara_version = 1st

valid_dataset_type = kara
valid_formatter_type = MLMPlugD
valid_data_path = /data/xiaochaojun/Rerank/data
valid_kara_namespace = wiki
valid_kara_dataset = train
valid_kara_version = 1st

[model] #model parameters
model_name = PlugD
pretrained_model=/data/xiaochaojun/PLMs/bert-base-chinese/

[output] #output parameters
output_time = 10
test_time = 1

model_path = ../checkpoints/
model_name = BERTPlugD

output_function = binary
