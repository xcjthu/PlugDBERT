/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
None
None
None
None
None
None
NoneNone

04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/05/2022 15:14:21 - INFO - __main__ -   CUDA available: True
04/05/2022 15:14:21 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
04/05/2022 15:14:21 - INFO - __main__ -   CUDA available: True
04/05/2022 15:14:21 - INFO - __main__ -   CUDA available: True
04/05/2022 15:14:21 - INFO - __main__ -   CUDA available: True
04/05/2022 15:14:21 - INFO - __main__ -   CUDA available: True
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/05/2022 15:14:21 - INFO - __main__ -   CUDA available: True
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/05/2022 15:14:21 - INFO - __main__ -   CUDA available: True
04/05/2022 15:14:21 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/05/2022 15:14:21 - INFO - __main__ -   CUDA available: True
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
04/05/2022 15:14:21 - INFO - tools.init_tool -   Begin to initialize models...
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
ignore 0 parameters
ignore parameters with nograd in name, and only 206 parameters are turned
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
None
04/05/2022 15:14:36 - INFO - tools.init_tool -   Initialize done.
None
None
[train]
epoch: 20
batch_size: 32
shuffle: True
reader_num: 8
grad_accumulate: 4
optimizer: AdamW
learning_rate: 3e-4
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 3000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mux_num: 4
mlm_prob: 0.15
reuse_num: 4
valid_mode: step
step_epoch: 1000
========
[eval]
batch_size: 64
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: mlm
train_data_path: /data/home/scv0540/xcj/datamux/data/wiki-webtext
kara_namespace: pretrain
kara_dataset: corpus
kara_version: 1st
valid_dataset_type: Raw
valid_formatter_type: mlm
valid_data_path: /data/home/scv0540/xcj/datamux/data/valid_text.jsonl
========
[model]
model_name: mlm
pretrained_model: /data/home/scv0540/xcj/PLMs/bert-base-uncased
========
[output]
output_time: 100
test_time: 1
model_path: /data/home/scv0540/xcj/datamux/checkpoint
model_name: mlm-webtext
output_function: avgloss
========
None
04/05/2022 15:14:36 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 1000
04/05/2022 15:14:36 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
None
None
None
None
3         train   1/63405      0:22/23264:44      2.699   {"mlm": 2.6969969272613525, "mse": 0.002186843426898122}
04/05/2022 15:14:59 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/05/2022 15:14:59 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/05/2022 15:14:59 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/05/2022 15:14:59 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/05/2022 15:14:59 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/05/2022 15:14:59 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/05/2022 15:14:59 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/05/2022 15:14:59 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
3         train   101/63405    1:18/822:36        2.692   {"mlm": 2.690251619508951, "mse": 0.0021737437432465872}
3         train   201/63405    2:15/710:27        2.688   {"mlm": 2.6860003495097753, "mse": 0.002175640823338088}
3         train   301/63405    3:14/680:18        2.688   {"mlm": 2.6860874522960065, "mse": 0.0021705841138940367}
3         train   401/63405    4:14/665:40        2.689   {"mlm": 2.686774147418966, "mse": 0.0021698035231844854}
3         train   501/63405    5:12/653:24        2.689   {"mlm": 2.687142293610259, "mse": 0.002169799874536693}
3         train   601/63405    6:10/644:58        2.688   {"mlm": 2.6858509217641515, "mse": 0.002167856410229439}
3         train   701/63405    7:07/637:16        2.686   {"mlm": 2.684167569101962, "mse": 0.002168868888254555}
3         train   801/63405    8:03/630:22        2.686   {"mlm": 2.6843092393934653, "mse": 0.002167728566477417}
3         train   901/63405    9:00/624:43        2.687   {"mlm": 2.684580155403315, "mse": 0.002166675055889489}
3         train   1001/63405   9:56/619:52        2.685   {"mlm": 2.682912916808457, "mse": 0.0021666070715255373}
3         train   1101/63405  10:52/615:40        2.684   {"mlm": 2.681775632714489, "mse": 0.002168075823784213}
3         train   1201/63405  11:48/611:59        2.685   {"mlm": 2.683057620860853, "mse": 0.0021684231615774984}
3         train   1301/63405  12:45/608:42        2.686   {"mlm": 2.684092133343907, "mse": 0.0021685766075686948}
3         train   1401/63405  13:41/605:49        2.686   {"mlm": 2.6837495799408395, "mse": 0.00216933597119553}
3         train   1501/63405  14:37/603:07        2.686   {"mlm": 2.6836769908051106, "mse": 0.002170184901034426}
3         train   1601/63405  15:33/600:37        2.686   {"mlm": 2.6840417027696826, "mse": 0.002170481686864274}
3         train   1701/63405  16:29/598:18        2.687   {"mlm": 2.6848590616476247, "mse": 0.002171569929878692}
3         train   1801/63405  17:25/596:09        2.687   {"mlm": 2.6849239071099906, "mse": 0.0021721946172129727}
3         train   1901/63405  18:21/594:06        2.686   {"mlm": 2.684297881961684, "mse": 0.002172746307139179}
3         train   2001/63405  19:17/592:07        2.686   {"mlm": 2.684078150424643, "mse": 0.002173788163744412}
3         train   2101/63405  20:13/590:18        2.686   {"mlm": 2.6814014387130736, "mse": 0.0021771092852577566}
3         train   2201/63405  21:09/588:31        2.686   {"mlm": 2.683467756509781, "mse": 0.0021755149541422725}
3         train   2301/63405  22:05/586:48        2.686   {"mlm": 2.6819292155901593, "mse": 0.002177785575234642}
3         train   2401/63405  23:01/585:08        2.686   {"mlm": 2.683354678750038, "mse": 0.002181559843593277}
3         train   2501/63405  23:57/583:33        2.687   {"mlm": 2.6855392751693725, "mse": 0.0021829181774519384}
3         train   2601/63405  24:53/581:59        2.687   {"mlm": 2.6851490000883738, "mse": 0.0021851224234948553}
3         train   2701/63405  25:49/580:28        2.687   {"mlm": 2.685891030175345, "mse": 0.0021845475647465458}
3         train   2801/63405  26:45/579:02        2.687   {"mlm": 2.685425868034363, "mse": 0.0021863187113194725}
3         train   2901/63405  27:41/577:36        2.687   {"mlm": 2.687697922123803, "mse": 0.002189303987090372}
3         train   3001/63405  28:37/576:09        2.688   {"mlm": 2.6883079171180726, "mse": 0.0021910019144415855}
3         train   3101/63405  29:33/574:46        2.687   {"mlm": 2.6867576440897856, "mse": 0.00219112973025238}
3         train   3201/63405  30:29/573:26        2.687   {"mlm": 2.685520138343175, "mse": 0.0021907667452857518}
3         train   3301/63405  31:25/572:06        2.687   {"mlm": 2.6860815125245314, "mse": 0.002191445894109515}
3         train   3401/63405  32:21/570:47        2.687   {"mlm": 2.68482391834259, "mse": 0.002192674526844972}
3         train   3501/63405  33:17/569:30        2.687   {"mlm": 2.686439504146576, "mse": 0.002193590172256033}
3         train   3601/63405  34:12/568:14        2.687   {"mlm": 2.685773181468248, "mse": 0.002194640036614146}
3         train   3701/63405  35:08/567:00        2.687   {"mlm": 2.6846879563612096, "mse": 0.0021953209066380036}
3         train   3801/63405  36:04/565:46        2.686   {"mlm": 2.6838257016075984, "mse": 0.0021957419973073736}
3         train   3901/63405  37:00/564:32        2.685   {"mlm": 2.68243402694401, "mse": 0.002197545446381953}

3         valid   1/15         0:13/ 3:09         2.630   
3         valid   15/15        0:30/ 0:00         2.763   {"mlm": 2.7583333333333333, "mse": 2.725, "train": 0.0}
3         train   4001/63405  38:29/571:31        2.685   {"mlm": 2.5495941638946533, "mse": 0.0021899626590311527}
3         train   4101/63405  39:26/570:18        2.685   {"mlm": 2.696180527753169, "mse": 0.0022309451562232607}
3         train   4201/63405  40:22/569:06        2.686   {"mlm": 2.6935996333164955, "mse": 0.0022463971640062125}
slurmstepd: error: *** JOB 168257 ON g0004 CANCELLED AT 2022-04-05T15:55:08 ***
