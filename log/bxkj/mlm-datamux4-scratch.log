/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
None
None
None
None
None
None
None
None
04/02/2022 10:23:40 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
04/02/2022 10:23:40 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
04/02/2022 10:23:40 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
04/02/2022 10:23:40 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
04/02/2022 10:23:40 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
04/02/2022 10:23:41 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
04/02/2022 10:23:41 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
04/02/2022 10:23:41 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
04/02/2022 10:23:41 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/02/2022 10:23:41 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/02/2022 10:23:41 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/02/2022 10:23:41 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/02/2022 10:23:41 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/02/2022 10:23:41 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/02/2022 10:23:41 - INFO - __main__ -   CUDA available: True
04/02/2022 10:23:41 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
04/02/2022 10:23:41 - INFO - __main__ -   CUDA available: True
04/02/2022 10:23:41 - INFO - __main__ -   CUDA available: True
04/02/2022 10:23:41 - INFO - __main__ -   CUDA available: True
04/02/2022 10:23:41 - INFO - __main__ -   CUDA available: True
04/02/2022 10:23:41 - INFO - __main__ -   CUDA available: True
04/02/2022 10:23:41 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/02/2022 10:23:41 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
04/02/2022 10:23:41 - INFO - __main__ -   CUDA available: True
04/02/2022 10:23:41 - INFO - __main__ -   CUDA available: True
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/kara_storage/abc/kara.py:15: UserWarning: KaraStorage.open is deprecated, and will be removed in the future.
Please use KaraStorage.open_dataset instead.
  warnings.warn(
04/02/2022 10:23:41 - INFO - tools.init_tool -   Begin to initialize models...
ignore parameters with nograd in name, and only 206 parameters are turned
ignore parameters with nograd in name, and only 206 parameters are turned
ignore parameters with nograd in name, and only 206 parameters are turned
ignore parameters with nograd in name, and only 206 parameters are turned
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
ignore parameters with nograd in name, and only 206 parameters are turned
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
ignore parameters with nograd in name, and only 206 parameters are turned
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
ignore parameters with nograd in name, and only 206 parameters are turned
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
ignore parameters with nograd in name, and only 206 parameters are turned
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/data/home/scv0540/xcj/envs/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:321: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  super().__init__(params, defaults)
04/02/2022 10:23:56 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 32
shuffle: True
reader_num: 8
grad_accumulate: 2
optimizer: AdamW
learning_rate: 3e-4
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 3000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mux_num: 4
mlm_prob: 0.15
valid_mode: step
step_epoch: 1000
========
[eval]
batch_size: 64
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: mlm
train_data_path: /data/home/scv0540/xcj/ReadOnce/data/wikipedia/wiki-kara
kara_namespace: wikipedia
kara_dataset: wiki-512
kara_version: 1st
valid_dataset_type: Raw
valid_formatter_type: mlm
valid_data_path: /data/home/scv0540/xcj/datamux/data/squad_docs.json
test_dataset_type: Raw
test_formatter_type: mapper-os
test_data_path: /data/home/scv0540/xcj/ReadOnce/data/RACE/test.json
========
[model]
model_name: mlm
pretrained_model: /data/home/scv0540/xcj/PLMs/bert-base-uncased
========
[output]
output_time: 100
test_time: 1
model_path: /data/home/scv0540/xcj/datamux/checkpoint
model_name: mlm-scratch
output_function: avgloss
========
None
04/02/2022 10:23:56 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 1000
04/02/2022 10:23:56 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
None
None
None
None
None
None
None
5         train   1/20003      0:20/6703:56       3.075   {"mlm": 3.0740082263946533, "mse": 0.0006974269635975361}
04/02/2022 10:24:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/02/2022 10:24:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/02/2022 10:24:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/02/2022 10:24:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/02/2022 10:24:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/02/2022 10:24:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/02/2022 10:24:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
04/02/2022 10:24:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
5         train   101/20003    1:12/237:51        2.933   {"mlm": 2.931897857401631, "mse": 0.0006652469003324093}
5         train   201/20003    2:03/203:08        2.934   {"mlm": 2.933000742499508, "mse": 0.0006681112416171983}
5         train   301/20003    2:55/191:08        2.940   {"mlm": 2.9386639610873506, "mse": 0.0006700906600103871}
5         train   401/20003    3:46/184:42        2.942   {"mlm": 2.940491708436809, "mse": 0.000671654915120081}
5         train   501/20003    4:38/180:31        2.944   {"mlm": 2.9424607049443288, "mse": 0.0006722507249199583}
5         train   601/20003    5:29/177:26        2.941   {"mlm": 2.9398013287891764, "mse": 0.0006733959674960724}
5         train   701/20003    6:21/175:00        2.939   {"mlm": 2.937968238784311, "mse": 0.0006743987943158723}
5         train   801/20003    7:12/172:57        2.938   {"mlm": 2.9364945183086038, "mse": 0.0006758670028143292}
5         train   901/20003    8:04/171:11        2.936   {"mlm": 2.9344812860499476, "mse": 0.0006762393550625842}
5         train   1001/20003   8:56/169:37        2.934   {"mlm": 2.9329094060293803, "mse": 0.0006781495225101293}
5         train   1101/20003   9:47/168:11        2.932   {"mlm": 2.9305608757618447, "mse": 0.0006799310429184973}
5         train   1201/20003  10:39/166:51        2.933   {"mlm": 2.9313754211556007, "mse": 0.0006823483354363166}
5         train   1301/20003  11:31/165:34        2.932   {"mlm": 2.930823654692325, "mse": 0.0006855629415567721}
5         train   1401/20003  12:22/164:22        2.933   {"mlm": 2.931581683366491, "mse": 0.0006876112560430498}
5         train   1501/20003  13:14/163:13        2.933   {"mlm": 2.932106464882837, "mse": 0.0006900034166781293}
5         train   1601/20003  14:06/162:06        2.933   {"mlm": 2.931884770986067, "mse": 0.000692722714789093}
5         train   1701/20003  14:57/161:00        2.933   {"mlm": 2.9316573747111234, "mse": 0.0006950956983571168}
5         train   1801/20003  15:49/159:57        2.933   {"mlm": 2.931494745792514, "mse": 0.0006976643540244394}
5         train   1901/20003  16:41/158:56        2.932   {"mlm": 2.9310554254061296, "mse": 0.0007003257243094504}

5         valid   1/37         0:10/ 6:04         3.083   
5         valid   37/37        0:22/ 0:00         3.198   {"mlm": 3.195945945945946, "mse": 0.0, "train": 0.0}
5         train   2001/20003  17:59/161:50        2.933   {"mlm": 2.952357530593872, "mse": 0.0007346691563725471}
5         train   2101/20003  18:51/160:39        2.934   {"mlm": 2.9456427734677155, "mse": 0.0007589913542742039}
5         train   2201/20003  19:43/159:30        2.934   {"mlm": 2.943431799684591, "mse": 0.0007631390553028022}
5         train   2301/20003  20:35/158:22        2.934   {"mlm": 2.9401626729489956, "mse": 0.0007654842558979022}
5         train   2401/20003  21:27/157:15        2.934   {"mlm": 2.9393269159550086, "mse": 0.000769154673518887}
5         train   2501/20003  22:18/156:09        2.934   {"mlm": 2.938677336165529, "mse": 0.0007738155941850694}
5         train   2601/20003  23:10/155:03        2.934   {"mlm": 2.9374926641657826, "mse": 0.000777952023200728}
5         train   2701/20003  24:02/154:00        2.935   {"mlm": 2.9385410065997855, "mse": 0.000781238747587276}
5         train   2801/20003  24:54/152:57        2.935   {"mlm": 2.9390839786267606, "mse": 0.0007859243214602458}
5         train   2901/20003  25:46/151:54        2.935   {"mlm": 2.9378843548295235, "mse": 0.0007913936006802482}
5         train   3001/20003  26:37/150:53        2.935   {"mlm": 2.937197838868056, "mse": 0.0007962729729182616}
5         train   3101/20003  27:29/149:52        2.935   {"mlm": 2.9357519366327574, "mse": 0.0008004945874671797}
5         train   3201/20003  28:21/148:51        2.934   {"mlm": 2.935360200002132, "mse": 0.0008048909440245805}
5         train   3301/20003  29:13/147:51        2.935   {"mlm": 2.9357523348219665, "mse": 0.0008094593083760132}
5         train   3401/20003  30:05/146:52        2.935   {"mlm": 2.9366064883401615, "mse": 0.0008141236831111664}
5         train   3501/20003  30:57/145:53        2.936   {"mlm": 2.9376857196546093, "mse": 0.0008197873993668156}
5         train   3601/20003  31:48/144:54        2.936   {"mlm": 2.937756357306171, "mse": 0.0008251960781245241}
5         train   3701/20003  32:40/143:55        2.936   {"mlm": 2.937142730950608, "mse": 0.0008308905542831975}
5         train   3801/20003  33:32/142:57        2.935   {"mlm": 2.9361163126899426, "mse": 0.0008367025309246445}
5         train   3901/20003  34:24/142:00        2.936   {"mlm": 2.936636609945844, "mse": 0.0008431102492984214}

5         valid   1/37         0:10/ 6:12         3.256   
5         valid   37/37        0:22/ 0:00         3.176   {"mlm": 3.175675675675676, "mse": 0.0, "train": 0.0}
5         train   4001/20003  35:41/142:45        2.935   {"mlm": 2.898440361022949, "mse": 0.0009901339653879404}
5         train   4101/20003  36:33/141:45        2.936   {"mlm": 2.948478828562368, "mse": 0.001000165496145993}
5         train   4201/20003  37:25/140:45        2.937   {"mlm": 2.9646797073421194, "mse": 0.0010132577289961539}
5         train   4301/20003  38:17/139:46        2.938   {"mlm": 2.9707021863753615, "mse": 0.0010265358124219154}
5         train   4401/20003  39:09/138:47        2.939   {"mlm": 2.96887469232231, "mse": 0.0010373187990234398}
5         train   4501/20003  40:00/137:49        2.939   {"mlm": 2.969689916469856, "mse": 0.0010484323309160055}
5         train   4601/20003  40:52/136:50        2.940   {"mlm": 2.9683529814943896, "mse": 0.0010588134470637485}
5         train   4701/20003  41:44/135:52        2.941   {"mlm": 2.9717201985918336, "mse": 0.0010699604646390445}
5         train   4801/20003  42:36/134:54        2.942   {"mlm": 2.9733223840687306, "mse": 0.0010806839696140948}
5         train   4901/20003  43:28/133:57        2.943   {"mlm": 2.9734356318673867, "mse": 0.001092407888007742}
5         train   5001/20003  44:20/132:59        2.944   {"mlm": 2.974952486011532, "mse": 0.0011042925660250955}
5         train   5101/20003  45:11/132:02        2.945   {"mlm": 2.9759823409781254, "mse": 0.0011160157048980875}
5         train   5201/20003  46:03/131:06        2.946   {"mlm": 2.978250135490043, "mse": 0.001129402031842464}
5         train   5301/20003  46:55/130:09        2.947   {"mlm": 2.9808332637857236, "mse": 0.001143898446118956}
5         train   5401/20003  47:47/129:12        2.948   {"mlm": 2.982819973954467, "mse": 0.0011579573426916158}
5         train   5501/20003  48:39/128:16        2.950   {"mlm": 2.9849409804512548, "mse": 0.0011726843056592874}
5         train   5601/20003  49:31/127:20        2.951   {"mlm": 2.98761008531879, "mse": 0.0011870212592856421}
5         train   5701/20003  50:23/126:24        2.952   {"mlm": 2.9888804595236356, "mse": 0.0012014498778450237}
5         train   5801/20003  51:15/125:28        2.953   {"mlm": 2.9899008793012753, "mse": 0.0012164775044983072}
5         train   5901/20003  52:07/124:32        2.954   {"mlm": 2.9915546126769508, "mse": 0.0012319655642822201}

5         valid   1/37         0:10/ 6:15         3.000   
5         valid   37/37        0:22/ 0:00         3.176   {"mlm": 3.175675675675676, "mse": 0.0, "train": 0.0}
5         train   6001/20003  53:24/124:36        2.955   {"mlm": 2.9840035438537598, "mse": 0.0015970258973538876}
5         train   6101/20003  54:16/123:39        2.957   {"mlm": 3.0253740796948425, "mse": 0.001572444312283677}
5         train   6201/20003  55:08/122:43        2.957   {"mlm": 3.0100978488352763, "mse": 0.0015759386228795965}
5         train   6301/20003  56:00/121:47        2.957   {"mlm": 2.9860338119177325, "mse": 0.0015824611400845044}
5         train   6401/20003  56:52/120:50        2.957   {"mlm": 2.980970073519205, "mse": 0.0015919832562089448}
5         train   6501/20003  57:44/119:54        2.957   {"mlm": 2.9751440641170968, "mse": 0.00160089274078325}
5         train   6601/20003  58:35/118:58        2.957   {"mlm": 2.9731949875239723, "mse": 0.0016129194741786618}
5         train   6701/20003  59:27/118:02        2.957   {"mlm": 2.9708893101837766, "mse": 0.0016262020923053784}
5         train   6801/20003  60:19/117:06        2.957   {"mlm": 2.9659602097357705, "mse": 0.0016397906850428134}
5         train   6901/20003  61:11/116:11        2.957   {"mlm": 2.962930696785913, "mse": 0.0016528599604563197}
5         train   7001/20003  62:03/115:15        2.957   {"mlm": 2.9604142002768805, "mse": 0.0016668961178948666}
5         train   7101/20003  62:55/114:19        2.956   {"mlm": 2.9581205968311113, "mse": 0.0016817232586703959}
5         train   7201/20003  63:47/113:24        2.956   {"mlm": 2.9583841420331662, "mse": 0.001698703135884864}
5         train   7301/20003  64:39/112:29        2.957   {"mlm": 2.960098592617437, "mse": 0.0017171702913218374}
5         train   7401/20003  65:31/111:34        2.957   {"mlm": 2.9623442787004315, "mse": 0.0017375586863353232}
5         train   7501/20003  66:23/110:38        2.958   {"mlm": 2.9638314525101044, "mse": 0.0017573614327111675}
5         train   7601/20003  67:15/109:43        2.958   {"mlm": 2.964960930125554, "mse": 0.0017769126400996062}
5         train   7701/20003  68:07/108:48        2.958   {"mlm": 2.964868420085649, "mse": 0.0017962973942785255}
5         train   7801/20003  68:58/107:54        2.958   {"mlm": 2.9647669822622973, "mse": 0.0018149368269219043}
5         train   7901/20003  69:50/106:59        2.959   {"mlm": 2.964847834093956, "mse": 0.0018337680544235843}

5         valid   1/37         0:10/ 6:17         3.249   
5         valid   37/37        0:22/ 0:00         3.135   {"mlm": 3.1317567567567566, "mse": 0.0, "train": 0.0}
5         train   8001/20003  71:08/106:42        2.959   {"mlm": 3.0451338291168213, "mse": 0.0023779901675879955}
5         train   8101/20003  72:00/105:46        2.958   {"mlm": 2.927539355684035, "mse": 0.0022310923282425885}
5         train   8201/20003  72:51/104:51        2.958   {"mlm": 2.9254338385453864, "mse": 0.0022481862902733966}
5         train   8301/20003  73:43/103:56        2.957   {"mlm": 2.91482281447249, "mse": 0.002235884488903704}
5         train   8401/20003  74:35/103:00        2.956   {"mlm": 2.90036566418008, "mse": 0.002224532874565841}
5         train   8501/20003  75:27/102:05        2.955   {"mlm": 2.887987311014872, "mse": 0.0022250156493638148}
5         train   8601/20003  76:19/101:10        2.953   {"mlm": 2.876050755505554, "mse": 0.0022272927246479546}
5         train   8701/20003  77:11/100:15        2.952   {"mlm": 2.8703043923398397, "mse": 0.002232797550816041}
5         train   8801/20003  78:03/99:20         2.950   {"mlm": 2.863266336486283, "mse": 0.0022386987923241025}
5         train   8901/20003  78:55/98:25         2.949   {"mlm": 2.859881976071526, "mse": 0.0022458175665489113}
5         train   9001/20003  79:46/97:31         2.948   {"mlm": 2.8564000248789907, "mse": 0.0022540136205998216}
5         train   9101/20003  80:38/96:36         2.946   {"mlm": 2.852353390512631, "mse": 0.0022665480359860923}
5         train   9201/20003  81:30/95:41         2.945   {"mlm": 2.8490512960657886, "mse": 0.002277645798513261}
5         train   9301/20003  82:22/94:47         2.944   {"mlm": 2.852127285941943, "mse": 0.0022931409134263952}
5         train   9401/20003  83:14/93:52         2.944   {"mlm": 2.8549336815288795, "mse": 0.0023102250108382348}
5         train   9501/20003  84:06/92:57         2.943   {"mlm": 2.8574664106693053, "mse": 0.0023263221538981187}
5         train   9601/20003  84:58/92:03         2.943   {"mlm": 2.859973525345735, "mse": 0.0023417206164647237}
5         train   9701/20003  85:49/91:08         2.943   {"mlm": 2.862497192351416, "mse": 0.0023585929694656133}
5         train   9801/20003  86:41/90:14         2.942   {"mlm": 2.8652808033447013, "mse": 0.002374694278403736}
5         train   9901/20003  87:33/89:20         2.942   {"mlm": 2.8677802730522175, "mse": 0.0023909973513152744}

5         valid   1/37         0:10/ 6:13         3.020   
5         valid   37/37        0:22/ 0:00         3.136   {"mlm": 3.135135135135135, "mse": 0.0, "train": 0.0}
5         train   10001/20003 88:51/88:51         2.942   {"mlm": 2.826932668685913, "mse": 0.002669637557119131}
5         train   10101/20003 89:43/87:57         2.941   {"mlm": 2.8790867541095997, "mse": 0.002710084274116129}
5         train   10201/20003 90:35/87:02         2.941   {"mlm": 2.8884193517675447, "mse": 0.0027340842012445726}
5         train   10301/20003 91:27/86:08         2.940   {"mlm": 2.890248558449983, "mse": 0.0027500769700483725}
5         train   10401/20003 92:19/85:13         2.940   {"mlm": 2.882670692672159, "mse": 0.002763614636216489}
5         train   10501/20003 93:11/84:19         2.939   {"mlm": 2.8822851813957837, "mse": 0.0027752570471467966}
5         train   10601/20003 94:03/83:24         2.939   {"mlm": 2.881883702936664, "mse": 0.0027877419459449125}
5         train   10701/20003 94:55/82:30         2.938   {"mlm": 2.879842083056201, "mse": 0.002800867463011672}
5         train   10801/20003 95:47/81:36         2.937   {"mlm": 2.875884844569231, "mse": 0.002812706790376134}
5         train   10901/20003 96:38/80:41         2.936   {"mlm": 2.8731780054831213, "mse": 0.0028259992368465554}
5         train   11001/20003 97:30/79:47         2.936   {"mlm": 2.8708988041072696, "mse": 0.0028377897538743413}
5         train   11101/20003 98:22/78:53         2.935   {"mlm": 2.870630900498199, "mse": 0.0028499829606977813}
5         train   11201/20003 99:14/77:59         2.935   {"mlm": 2.869664320640024, "mse": 0.0028619080507425246}
5         train   11301/20003 100:06/77:05        2.934   {"mlm": 2.868051114584463, "mse": 0.00287591420106166}
5         train   11401/20003 100:58/76:11        2.933   {"mlm": 2.8663499171524536, "mse": 0.002888111067572868}
5         train   11501/20003 101:50/75:16        2.933   {"mlm": 2.866342148885657, "mse": 0.0029010056920702023}
5         train   11601/20003 102:42/74:22        2.932   {"mlm": 2.864783763885498, "mse": 0.0029129408541915566}
5         train   11701/20003 103:34/73:28        2.931   {"mlm": 2.8649345700142317, "mse": 0.0029256137829549887}
5         train   11801/20003 104:25/72:34        2.931   {"mlm": 2.863340365231401, "mse": 0.0029379902649277834}
5         train   11901/20003 105:17/71:41        2.930   {"mlm": 2.862604466819061, "mse": 0.0029517155563677303}

5         valid   1/37         0:10/ 6:15         3.142   
5         valid   37/37        0:22/ 0:00         3.098   {"mlm": 3.097972972972973, "mse": 0.0, "train": 0.0}
5         train   12001/20003 106:35/71:04        2.929   {"mlm": 2.7129874229431152, "mse": 0.002857644809409976}
5         train   12101/20003 107:26/70:09        2.929   {"mlm": 2.828741203440298, "mse": 0.0031975871228640622}
5         train   12201/20003 108:18/69:15        2.927   {"mlm": 2.8049825079998567, "mse": 0.0032055968782442865}
5         train   12301/20003 109:10/68:21        2.926   {"mlm": 2.7850745199526665, "mse": 0.0032065100735826943}
5         train   12401/20003 110:02/67:27        2.925   {"mlm": 2.7761005398043967, "mse": 0.0032123022345812093}
5         train   12501/20003 110:54/66:33        2.923   {"mlm": 2.7678450329337054, "mse": 0.0032127660642491426}
5         train   12601/20003 111:46/65:39        2.922   {"mlm": 2.7647827937083314, "mse": 0.003216238559135383}
5         train   12701/20003 112:37/64:45        2.920   {"mlm": 2.758417156725569, "mse": 0.0032250705648734423}
5         train   12801/20003 113:29/63:51        2.919   {"mlm": 2.7549437018071816, "mse": 0.0032334628755624375}
5         train   12901/20003 114:21/62:57        2.918   {"mlm": 2.754033790973659, "mse": 0.0032436056558733245}
5         train   13001/20003 115:13/62:03        2.916   {"mlm": 2.7512942196486834, "mse": 0.003253370922591005}
5         train   13101/20003 116:05/61:09        2.915   {"mlm": 2.748172248308492, "mse": 0.0032643474220593444}
5         train   13201/20003 116:57/60:15        2.913   {"mlm": 2.745964300821068, "mse": 0.003272415202756185}
5         train   13301/20003 117:49/59:21        2.912   {"mlm": 2.7445155683615683, "mse": 0.00328372470224899}
5         train   13401/20003 118:40/58:28        2.911   {"mlm": 2.7439964728045685, "mse": 0.0032926165970104835}
5         train   13501/20003 119:32/57:34        2.909   {"mlm": 2.742209119212222, "mse": 0.0033011778971635745}
5         train   13601/20003 120:24/56:40        2.908   {"mlm": 2.740533646459061, "mse": 0.0033108647999997663}
5         train   13701/20003 121:16/55:46        2.907   {"mlm": 2.7406713247719687, "mse": 0.00332041977311231}
5         train   13801/20003 122:08/54:53        2.905   {"mlm": 2.7395661130075917, "mse": 0.0033283647403704298}
5         train   13901/20003 122:59/53:59        2.904   {"mlm": 2.7386297711819365, "mse": 0.003337801726454617}

5         valid   1/37         0:10/ 6:13         3.117   
5         valid   37/37        0:22/ 0:00         3.049   {"mlm": 3.0472972972972974, "mse": 0.0, "train": 0.0}
5         train   14001/20003 124:17/53:16        2.900   {"mlm": 2.116691827774048, "mse": 0.00359036261215806}
5         train   14101/20003 125:09/52:22        2.894   {"mlm": 1.9969626299225458, "mse": 0.003726849088094907}
5         train   14201/20003 126:01/51:29        2.887   {"mlm": 1.9651388914430912, "mse": 0.0037659480542278113}
5         train   14301/20003 126:52/50:35        2.880   {"mlm": 1.9290544646126884, "mse": 0.003786310846716215}
5         train   14401/20003 127:44/49:41        2.872   {"mlm": 1.8862861650543024, "mse": 0.003800276617526525}
5         train   14501/20003 128:36/48:47        2.864   {"mlm": 1.8543054465048328, "mse": 0.0038151343909841396}
5         train   14601/20003 129:28/47:54        2.856   {"mlm": 1.8321558155751665, "mse": 0.0038238235206161926}
5         train   14701/20003 130:20/47:00        2.849   {"mlm": 1.8212235067438296, "mse": 0.0038340782365375086}
5         train   14801/20003 131:12/46:06        2.842   {"mlm": 1.8175965863071875, "mse": 0.0038472184473613787}
5         train   14901/20003 132:03/45:13        2.834   {"mlm": 1.8061339126178348, "mse": 0.0038569215974552386}
5         train   15001/20003 132:55/44:19        2.827   {"mlm": 1.7972116625154173, "mse": 0.003866075023359337}
5         train   15101/20003 133:47/43:25        2.821   {"mlm": 1.8140693534838948, "mse": 0.003867936082485451}
5         train   15201/20003 134:39/42:32        2.819   {"mlm": 1.860979304424829, "mse": 0.0038629876880857668}
5         train   15301/20003 135:31/41:38        2.818   {"mlm": 1.9225135942681948, "mse": 0.003862954815921247}
5         train   15401/20003 136:23/40:45        2.817   {"mlm": 1.9797289852241717, "mse": 0.003861477366714425}
5         train   15501/20003 137:14/39:51        2.817   {"mlm": 2.0343176313911098, "mse": 0.0038608715040851223}
5         train   15601/20003 138:06/38:58        2.817   {"mlm": 2.084344655033352, "mse": 0.00386391701842824}
5         train   15701/20003 138:58/38:04        2.817   {"mlm": 2.1290025710358753, "mse": 0.0038675310211732485}
5         train   15801/20003 139:50/37:11        2.818   {"mlm": 2.16859430044906, "mse": 0.0038738275688350496}
5         train   15901/20003 140:42/36:17        2.818   {"mlm": 2.20481484448013, "mse": 0.003879850028868547}

5         valid   1/37         0:10/ 6:33         3.066   
5         valid   37/37        0:23/ 0:00         3.065   {"mlm": 3.064189189189189, "mse": 0.0, "train": 0.0}
5         train   16001/20003 141:59/35:30        2.818   {"mlm": 2.7302088737487793, "mse": 0.003797390963882208}
5         train   16101/20003 142:51/34:37        2.818   {"mlm": 2.7752421445185593, "mse": 0.003960158364699766}
slurmstepd: error: *** JOB 166874 ON g0001 CANCELLED AT 2022-04-02T12:46:59 ***
