/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin


/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config fromread config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config 

config/pre-train/adapter.config
read config from config/pre-train/adapter.config
NoneNoneNoneNoneNone



None
None

None
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
            value = d[option]    value = d[option]value = d[option]value = d[option]



      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]    
value = d[option]  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
                return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__



  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError        raise KeyError(key)raise KeyError(key): 

'max_len'
    
During handling of the above exception, another exception occurred:

    raise KeyError(key)Traceback (most recent call last):
raise KeyError(key)
KeyError
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
KeyError        : raise KeyError(key)raise KeyError(key)'max_len':     

'max_len'

During handling of the above exception, another exception occurred:


raise KeyError(key)KeyError
During handling of the above exception, another exception occurred:

Traceback (most recent call last):

: Traceback (most recent call last):
KeyError'max_len'  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
KeyError
:   File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
'max_len'
During handling of the above exception, another exception occurred:

: 
'max_len'Traceback (most recent call last):
KeyError
During handling of the above exception, another exception occurred:


KeyError
During handling of the above exception, another exception occurred:

: :   File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
Traceback (most recent call last):
Traceback (most recent call last):
'max_len''max_len'

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func

During handling of the above exception, another exception occurred:

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
            return getattr(self.config, func_name)(*args, **kwargs)return getattr(self.config, func_name)(*args, **kwargs)return getattr(self.config, func_name)(*args, **kwargs)    
    return getattr(self.config, func_name)(*args, **kwargs)

return getattr(self.config, func_name)(*args, **kwargs)
          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.config, func_name)(*args, **kwargs)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
        return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,    return self._get_conv(section, option, int, raw=raw, vars=vars,


    return self._get_conv(section, option, int, raw=raw, vars=vars,  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv

          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,    
return self._get(section, conv, option, raw=raw, vars=vars,      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get

return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,return self._get(section, conv, option, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))    
return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        return conv(self.get(section, option, **kwargs))return conv(self.get(section, option, **kwargs))

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    raise NoOptionError(option, section)
configparser.NoOptionError    : raise NoOptionError(option, section)No option 'max_len' in section: 'train'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
    raise NoOptionError(option, section)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
configparser
.configparserNoOptionError.: NoOptionErrorNo option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

: No option 'max_len' in section: 'train'Traceback (most recent call last):

    
During handling of the above exception, another exception occurred:

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
raise NoOptionError(option, section)Traceback (most recent call last):

    raise NoOptionError(option, section)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get

configparser        raise NoOptionError(option, section)raise NoOptionError(option, section)
.
configparserNoOptionErrorconfigparser: .No option 'max_len' in section: 'train'NoOptionError
configparser.
During handling of the above exception, another exception occurred:

NoOptionError: Traceback (most recent call last):
.No option 'max_len' in section: 'train'NoOptionError
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
: 
During handling of the above exception, another exception occurred:

No option 'max_len' in section: 'train'
Traceback (most recent call last):
: 
During handling of the above exception, another exception occurred:

No option 'max_len' in section: 'train'  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):


During handling of the above exception, another exception occurred:

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
    value = d[option]  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    
value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return self.__missing__(key)            # support subclasses that define __missing__
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError    :     raise KeyError(key)'max_len'raise KeyError(key)



During handling of the above exception, another exception occurred:

KeyErrorKeyErrorTraceback (most recent call last):
: : 'max_len'  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
'max_len'


During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
      File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
raise KeyError(key)
KeyError: 'max_len'    
raise KeyError(key)
During handling of the above exception, another exception occurred:

    
Traceback (most recent call last):
raise KeyError(key)KeyError
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
: KeyError'max_len'
: 
During handling of the above exception, another exception occurred:

'max_len'
Traceback (most recent call last):

During handling of the above exception, another exception occurred:

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
Traceback (most recent call last):
    return getattr(self.local_config, func_name)(*args, **kwargs)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
        return getattr(self.local_config, func_name)(*args, **kwargs)return getattr(self.local_config, func_name)(*args, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.local_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self.__missing__(key)            # support subclasses that define __missing__    return getattr(self.local_config, func_name)(*args, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return getattr(self.local_config, func_name)(*args, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.local_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self._get_conv(section, option, int, raw=raw, vars=vars,
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,

      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
    return getattr(self.local_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
        return self._get(section, conv, option, raw=raw, vars=vars,return self._get(section, conv, option, raw=raw, vars=vars,

        return self._get(section, conv, option, raw=raw, vars=vars,  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
return self._get(section, conv, option, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))    
return conv(self.get(section, option, **kwargs))    
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
return conv(self.get(section, option, **kwargs))
    return conv(self.get(section, option, **kwargs))  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get

    return conv(self.get(section, option, **kwargs))  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    raise NoOptionError(option, section)
    configparserraise NoOptionError(option, section).
    NoOptionError    raise NoOptionError(option, section)    configparserraise NoOptionError(option, section): 
raise NoOptionError(option, section).
No option 'max_len' in section: 'train'
configparserNoOptionErrorconfigparser
configparser..: .NoOptionErrorNo option 'max_len' in section: 'train'
During handling of the above exception, another exception occurred:


NoOptionErrorNoOptionError: Traceback (most recent call last):

During handling of the above exception, another exception occurred:

No option 'max_len' in section: 'train': : 
Traceback (most recent call last):
No option 'max_len' in section: 'train'  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
No option 'max_len' in section: 'train'
During handling of the above exception, another exception occurred:


  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return conv(self.get(section, option, **kwargs))    value = d[option]

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
        value = d[option]    value = d[option]
value = d[option]    

value = d[option]  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
        return self.__missing__(key)            # support subclasses that define __missing__raise NoOptionError(option, section)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
        return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__

          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError: 'max_len'    
raise KeyError(key)
During handling of the above exception, another exception occurred:


Traceback (most recent call last):
KeyError:   File "train.py", line 139, in <module>
'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
            raise KeyError(key)main()raise KeyError(key)


KeyError    KeyError  File "train.py", line 86, in main
    main(): : raise KeyError(key)
'max_len''max_len'
      File "train.py", line 86, in main
KeyError

raise KeyError(key): 
During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:


'max_len'Traceback (most recent call last):
Traceback (most recent call last):


During handling of the above exception, another exception occurred:

  File "train.py", line 139, in <module>
  File "train.py", line 139, in <module>
Traceback (most recent call last):
KeyError:   File "train.py", line 139, in <module>
'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    main()
  File "train.py", line 86, in main
        parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
                main()main()main()main()



  File "train.py", line 86, in main
  File "train.py", line 86, in main
  File "train.py", line 86, in main
      File "train.py", line 86, in main
parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
                parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)



  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 86, in main
    parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
            result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)        result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)    

result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)    result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset


  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
    result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
            init_formatter(config, ["train", "valid"], *args, **params)init_formatter(config, ["train", "valid"], *args, **params)init_formatter(config, ["train", "valid"], *args, **params)
    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter

init_formatter(config, ["train", "valid"], *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
    init_formatter(config, ["train", "valid"], *args, **params)
    init_formatter(config, ["train", "valid"], *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
    
init_formatter(config, ["train", "valid"], *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
    formatter[task] = form.init_formatter(config, task, *args, **params)    
formatter[task] = form.init_formatter(config, task, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
      File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
formatter[task] = form.init_formatter(config, task, *args, **params)    
formatter[task] = form.init_formatter(config, task, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
    init_formatter(config, ["train", "valid"], *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
            formatter[task] = form.init_formatter(config, task, *args, **params)formatter[task] = form.init_formatter(config, task, *args, **params)formatter[task] = form.init_formatter(config, task, *args, **params)


  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
    formatter[task] = form.init_formatter(config, task, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
            formatter = formatter_list[which](config, mode, *args, **params)formatter = formatter_list[which](config, mode, *args, **params)formatter = formatter_list[which](config, mode, *args, **params)


        formatter = formatter_list[which](config, mode, *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
formatter = formatter_list[which](config, mode, *args, **params)
    
formatter = formatter_list[which](config, mode, *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
    formatter = formatter_list[which](config, mode, *args, **params)    formatter = formatter_list[which](config, mode, *args, **params)

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
                self.max_len = config.getint("train", "max_len")self.max_len = config.getint("train", "max_len")self.max_len = config.getint("train", "max_len")    

self.max_len = config.getint("train", "max_len")    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
self.max_len = config.getint("train", "max_len")
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
self.max_len = config.getint("train", "max_len")  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func


  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
    self.max_len = config.getint("train", "max_len")
      File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
self.max_len = config.getint("train", "max_len")
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
    return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.default_config, func_name)(*args, **kwargs)    
return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.default_config, func_name)(*args, **kwargs)return getattr(self.default_config, func_name)(*args, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.default_config, func_name)(*args, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
        return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get(section, conv, option, raw=raw, vars=vars,
    return self._get(section, conv, option, raw=raw, vars=vars,  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
        return self._get(section, conv, option, raw=raw, vars=vars,return self._get(section, conv, option, raw=raw, vars=vars,

      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
        return conv(self.get(section, option, **kwargs))return conv(self.get(section, option, **kwargs))

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        return conv(self.get(section, option, **kwargs))return conv(self.get(section, option, **kwargs))

      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        raise NoOptionError(option, section)raise NoOptionError(option, section)

configparserconfigparser..NoOptionErrorNoOptionError: : No option 'max_len' in section: 'train'No option 'max_len' in section: 'train'

    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        raise NoOptionError(option, section)raise NoOptionError(option, section)

configparser    .configparser    raise NoOptionError(option, section)NoOptionError.raise NoOptionError(option, section)

NoOptionError: configparserNo option 'max_len' in section: 'train'.: 
NoOptionErrorNo option 'max_len' in section: 'train'configparser
: .No option 'max_len' in section: 'train'NoOptionError
: No option 'max_len' in section: 'train'
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 48301) of binary: /home/xiaochaojun/env/miniconda3/bin/python3
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 48302)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 48303)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 48304)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 48305)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 48306)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 48307)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 48308)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 48301)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin


/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
None
None
NoneNoneNone


None
None
None
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
 roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:11:16 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,038 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,038 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,039 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,052 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,052 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,052 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,075 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,075 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,075 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,076 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,076 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,076 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,077 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,077 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,077 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,079 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,079 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,079 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,958 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,959 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,959 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:29,245 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:29,245 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:29,245 >> Static Memory 0.00 GB, Max Memory 0.00 GB
09/22/2022 15:11:35 - WARNING - tools.init_tool -   Cannot load checkpoint file with error [Errno 2] No such file or directory: 'checkpoints/BERT-Adapter'
09/22/2022 15:11:35 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 16
shuffle: True
reader_num: 8
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
========
[eval]
batch_size: 16
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:11:35 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
Traceback (most recent call last):
  File "train.py", line 139, in <module>
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    for step, data in enumerate(dataset):
    main()      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__

  File "train.py", line 95, in main
main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):    
for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
        data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
        return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
        data.reraise()    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
            raise exception
data.reraise()raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers


  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    raise exception    main()
  File "train.py", line 95, in main

TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin





/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from read config fromconfig/pre-train/adapter.config
 config/pre-train/adapter.configread config fromread config from
read config from  config/pre-train/adapter.configconfig/pre-train/adapter.config 

config/pre-train/adapter.config
read config from config/pre-train/adapter.config
NoneNone

None
None
NoneNone

None
None
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
formatter roberta-base
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:13:56 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:13:59,689 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:13:59,689 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:13:59,689 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:01,358 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:01,358 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:01,358 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:02,515 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:02,515 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:02,515 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:03,386 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:03,386 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:03,387 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:03,582 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:03,582 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:03,582 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:05,181 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:05,181 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:05,181 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:05,399 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:05,399 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:05,399 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:13,116 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:13,116 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:13,116 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:14:18 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:14:18 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 16
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
========
[eval]
batch_size: 16
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:14:18 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:14:18 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    return forward_call(*input, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    main()    
return forward_call(*input, **kwargs)  File "train.py", line 95, in main

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    main()
  File "train.py", line 95, in main
        return forward_call(*input, **kwargs)train(parameters, config, gpu_list, do_test, args.local_rank)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
                output = self.module(*inputs[0], **kwargs[0])output = self.module(*inputs[0], **kwargs[0])output = self.module(*inputs[0], **kwargs[0])output = self.module(*inputs[0], **kwargs[0])



  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    output = self.module(*inputs[0], **kwargs[0])    
output = self.module(*inputs[0], **kwargs[0])
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
output = self.module(*inputs[0], **kwargs[0])  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    
output = self.module(*inputs[0], **kwargs[0])
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    out = self.model(
        out = self.model(  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl

out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        return forward_call(*input, **kwargs)
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
        out = self.model(return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
            outputs = self.roberta(outputs = self.roberta(
outputs = self.roberta(
    
outputs = self.roberta(  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    
outputs = self.roberta(  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
outputs = self.roberta(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        outputs = self.roberta(outputs = self.roberta(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(    
encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    layer_outputs = layer_module(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        layer_outputs = layer_module(layer_outputs = layer_module(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    layer_outputs = layer_module(    
layer_outputs = layer_module(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        encoder_outputs = self.encoder(encoder_outputs = self.encoder(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    layer_outputs = layer_module(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
        layer_output = apply_chunking_to_forward(layer_output = apply_chunking_to_forward(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
        layer_output = apply_chunking_to_forward(layer_output = apply_chunking_to_forward(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
        layer_outputs = layer_module(layer_outputs = layer_module(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
        layer_output = apply_chunking_to_forward(layer_output = apply_chunking_to_forward(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)
          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)return forward_fn(*input_tensors)return forward_fn(*input_tensors)


  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)    
intermediate_output = self.intermediate(attention_output)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    hidden_states = self.intermediate_act_fn(hidden_states)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
intermediate_output = self.intermediate(attention_output)
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 7; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return torch._C._nn.gelu(input)
RuntimeError    return torch._C._nn.gelu(input): 
CUDA out of memory. Tried to allocate 96.00 MiB (GPU 2; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
RuntimeError:     CUDA out of memory. Tried to allocate 96.00 MiB (GPU 4; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF    return torch._C._nn.gelu(input)return torch._C._nn.gelu(input)


RuntimeError: RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 96.00 MiB (GPU 3; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 5; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 1; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53274 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53275 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53276 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53277 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53279 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53280 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53281 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 4 (pid: 53278) of binary: /home/xiaochaojun/env/miniconda3/bin/python3
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-09-22_15:14:35
  host      : 103server
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 53278)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from read config fromconfig/pre-train/adapter.config
 config/pre-train/adapter.config
NoneNone

None
None
NoneNone

None
None
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:15:36 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:16:03 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,256 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,256 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,256 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,259 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,260 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,260 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,268 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,268 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,268 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,583 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,583 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,583 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,862 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,862 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,862 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:16,074 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:16,074 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:16,074 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:18,862 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:18,863 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:18,863 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:19,199 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:19,200 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:19,200 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:16:24 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:16:24 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:16:24 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:16:24 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
0         train   1/102467     0:13/23906:25      1.770  tensor(0.7647, device='cuda:0'){"mlm": 1.7702429294586182, "mse": 0.0}
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   101/102467   0:49/838:11        1.473  tensor(0.7233, device='cuda:0'){"mlm": 1.472854727565652, "mse": 0.0}
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
formatter roberta-base
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.

formatter roberta-base
formatterformatter  roberta-baseroberta-base

09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:20:04 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:11,392 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:11,392 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:11,392 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:11,393 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:11,393 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:11,393 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:12,270 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:12,270 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:12,270 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:12,568 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:12,568 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:12,568 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:12,927 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:12,927 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:12,927 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:13,782 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:13,782 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:13,782 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:18,604 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:18,605 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:18,605 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:19,961 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:19,961 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:19,961 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:20:24 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:20:24 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
grad_accumulate: 2
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:20:24 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:20:24 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
0         train   1/102467     0:14/23973:33      1.728         {"mlm": 1.728100299835205, "mse": 0.0}  None
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   101/102467   0:49/837:49        1.471         {"mlm": 1.4712648244187383, "mse": 0.0}  None
0         train   201/102467   1:25/723:44        1.488         {"mlm": 1.4879416129482326, "mse": 0.0}  None
0         train   301/102467   2:01/685:56        1.493         {"mlm": 1.49250820051396, "mse": 0.0}  None
0         train   401/102467   2:37/666:43        1.501         {"mlm": 1.500505304841924, "mse": 0.0}  None
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin


/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

NoneNone

NoneNone

None
None
NoneNone

09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-baseformatter
 roberta-base
formatter roberta-base
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:25:01 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:08,647 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:08,647 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:08,647 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:08,651 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:08,651 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:08,651 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:08,651 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:08,652 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:08,652 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:09,065 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:09,065 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:09,065 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:10,173 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:10,173 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:10,173 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:12,721 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:12,721 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:12,721 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:13,659 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:13,659 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:13,660 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:13,860 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:13,860 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:13,860 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:25:19 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:25:19 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
grad_accumulate: 2
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:25:19 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:25:19 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   100/102467   0:48/833:03        1.474         {"mlm": 1.4740522986650466, "mse": 0.0}  0.5573
0         train   200/102467   1:24/721:01        1.490         {"mlm": 1.4902419385313987, "mse": 0.0}  0.5464
0         train   300/102467   2:00/683:15        1.492         {"mlm": 1.4918665808439255, "mse": 0.0}  0.5164
0         train   400/102467   2:36/664:17        1.500         {"mlm": 1.5003701157867908, "mse": 0.0}  0.505
0         train   500/102467   3:12/652:41        1.497         {"mlm": 1.4966139410734176, "mse": 0.0}  0.5092
0         train   600/102467   3:47/644:44        1.495         {"mlm": 1.4946815793712933, "mse": 0.0}  0.4998
0         train   700/102467   4:23/639:02        1.489         {"mlm": 1.4892618253401348, "mse": 0.0}  0.495
0         train   800/102467   4:59/634:45        1.488         {"mlm": 1.4882808303087949, "mse": 0.0}  0.4915
0         train   900/102467   5:35/631:11        1.483         {"mlm": 1.4828980328639347, "mse": 0.0}  0.482
0         train   1000/102467  6:11/628:14        1.481         {"mlm": 1.4813558894991874, "mse": 0.0}  0.498
0         train   1100/102467  6:47/625:47        1.480         {"mlm": 1.4798838514089585, "mse": 0.0}  0.505
0         train   1200/102467  7:23/623:34        1.479         {"mlm": 1.478777339408795, "mse": 0.0}  0.5017
0         train   1300/102467  7:59/621:49        1.480         {"mlm": 1.4798490166205627, "mse": 0.0}  0.4951
0         train   1400/102467  8:35/620:09        1.478         {"mlm": 1.477761804163456, "mse": 0.0}  0.5024
0         train   1500/102467  9:11/618:42        1.476         {"mlm": 1.4762078045606613, "mse": 0.0}  0.4886
0         train   1600/102467  9:47/617:14        1.475         {"mlm": 1.4748854521289467, "mse": 0.0}  0.5048
0         train   1700/102467 10:23/616:00        1.475         {"mlm": 1.474520916202489, "mse": 0.0}  0.6301
0         train   1800/102467 10:59/614:48        1.473         {"mlm": 1.4730305129620764, "mse": 0.0}  0.494
0         train   1900/102467 11:35/613:47        1.472         {"mlm": 1.471604926617522, "mse": 0.0}  0.5237
0         train   2000/102467 12:11/612:47        1.471         {"mlm": 1.4712602557241916, "mse": 0.0}  0.4868
0         train   2100/102467 12:48/611:48        1.470         {"mlm": 1.4499794341096974, "mse": 0.0}  0.4899
0         train   2200/102467 13:24/610:51        1.470         {"mlm": 1.452539246585501, "mse": 0.0}  0.5048
0         train   2300/102467 14:00/609:55        1.470         {"mlm": 1.4576741355318688, "mse": 0.0}  0.4969
0         train   2400/102467 14:36/609:04        1.469         {"mlm": 1.4561040973603576, "mse": 0.0}  0.5525
0         train   2500/102467 15:12/608:11        1.468         {"mlm": 1.4551511199058655, "mse": 0.0}  0.4754
0         train   2600/102467 15:48/607:20        1.466         {"mlm": 1.4484001490627982, "mse": 0.0}  0.5064
0         train   2700/102467 16:24/606:30        1.465         {"mlm": 1.4469215601298941, "mse": 0.0}  0.4666
0         train   2800/102467 17:01/605:43        1.464         {"mlm": 1.4460571742475554, "mse": 0.0}  0.4517
0         train   2900/102467 17:37/605:01        1.463         {"mlm": 1.4429334470374433, "mse": 0.0}  0.4652
0         train   3000/102467 18:13/604:15        1.462         {"mlm": 1.4434500427336783, "mse": 0.0}  0.7303
0         train   3100/102467 18:49/603:31        1.462         {"mlm": 1.4449343807703805, "mse": 0.0}  0.5075
0         train   3200/102467 19:25/602:48        1.462         {"mlm": 1.4460735543257401, "mse": 0.0}  0.5602
0         train   3300/102467 20:02/602:04        1.461         {"mlm": 1.4440977411420644, "mse": 0.0}  0.4593
0         train   3400/102467 20:38/601:21        1.460         {"mlm": 1.4447737657401798, "mse": 0.0}  0.4726
0         train   3500/102467 21:14/600:43        1.460         {"mlm": 1.4444062738040035, "mse": 0.0}  0.5796
0         train   3600/102467 21:50/599:59        1.459         {"mlm": 1.4436605030704543, "mse": 0.0}  0.4497
0         train   3700/102467 22:27/599:17        1.459         {"mlm": 1.4434094078545012, "mse": 0.0}  0.4531
0         train   3800/102467 23:03/598:36        1.458         {"mlm": 1.4424468032050755, "mse": 0.0}  0.4909
0         train   3900/102467 23:39/597:55        1.458         {"mlm": 1.4439159507121206, "mse": 0.0}  0.43
0         train   4000/102467 24:15/597:13        1.458         {"mlm": 1.4439913727093363, "mse": 0.0}  0.4542
0         train   4100/102467 24:51/596:33        1.458         {"mlm": 1.4605077249663216, "mse": 0.0}  0.4588
0         train   4200/102467 25:28/595:53        1.457         {"mlm": 1.4423224673126682, "mse": 0.0}  0.4715
0         train   4300/102467 26:04/595:12        1.456         {"mlm": 1.4351306489650035, "mse": 0.0}  0.4597
0         train   4400/102467 26:40/594:34        1.456         {"mlm": 1.4342835937912142, "mse": 0.0}  0.4639
0         train   4500/102467 27:16/593:54        1.455         {"mlm": 1.433157334126622, "mse": 0.0}  0.4374
0         train   4600/102467 27:53/593:14        1.455         {"mlm": 1.4341992943183235, "mse": 0.0}  0.4629
0         train   4700/102467 28:29/592:36        1.454         {"mlm": 1.436011611390592, "mse": 0.0}  0.464
0         train   4800/102467 29:05/591:56        1.454         {"mlm": 1.4375047198214328, "mse": 0.0}  0.4417
0         train   4900/102467 29:41/591:18        1.454         {"mlm": 1.437394497853876, "mse": 0.0}  0.4716
0         train   5000/102467 30:17/590:38        1.454         {"mlm": 1.4376146355587878, "mse": 0.0}  0.4867
0         train   5100/102467 30:54/590:00        1.454         {"mlm": 1.4388890674096857, "mse": 0.0}  0.4286
0         train   5200/102467 31:30/589:22        1.453         {"mlm": 1.4390857067947993, "mse": 0.0}  0.4332
0         train   5300/102467 32:06/588:44        1.453         {"mlm": 1.4388145082077737, "mse": 0.0}  0.4617
0         train   5400/102467 32:43/588:05        1.453         {"mlm": 1.4388225605133096, "mse": 0.0}  0.4802
0         train   5500/102467 33:19/587:28        1.453         {"mlm": 1.4390266538064216, "mse": 0.0}  0.5629
0         train   5600/102467 33:55/586:50        1.452         {"mlm": 1.4387523926915753, "mse": 0.0}  0.4686
0         train   5700/102467 34:31/586:11        1.452         {"mlm": 1.439138399522633, "mse": 0.0}  0.4929
0         train   5800/102467 35:08/585:35        1.452         {"mlm": 1.4405069799722898, "mse": 0.0}  0.4671
0         train   5900/102467 35:44/584:58        1.452         {"mlm": 1.4397405920779869, "mse": 0.0}  0.4485
0         train   6000/102467 36:20/584:20        1.452         {"mlm": 1.4398357368505992, "mse": 0.0}  0.4811
0         train   6100/102467 36:56/583:42        1.451         {"mlm": 1.4145087993022092, "mse": 0.0}  0.4605
0         train   6200/102467 37:33/583:03        1.451         {"mlm": 1.4314469307812336, "mse": 0.0}  0.4498
0         train   6300/102467 38:09/582:27        1.451         {"mlm": 1.4295868747162097, "mse": 0.0}  0.4545
0         train   6400/102467 38:45/581:49        1.450         {"mlm": 1.4279280231341307, "mse": 0.0}  0.4714
0         train   6500/102467 39:21/581:11        1.451         {"mlm": 1.4353301028610475, "mse": 0.0}  0.4838
0         train   6600/102467 39:58/580:34        1.450         {"mlm": 1.4328683972159022, "mse": 0.0}  0.436
0         train   6700/102467 40:34/579:56        1.449         {"mlm": 1.428573836555098, "mse": 0.0}  0.4684
0         train   6800/102467 41:10/579:21        1.448         {"mlm": 1.4227293659483025, "mse": 0.0}  0.4441
0         train   6900/102467 41:47/578:43        1.448         {"mlm": 1.4222491256635192, "mse": 0.0}  0.5278
0         train   7000/102467 42:23/578:06        1.448         {"mlm": 1.4242600817139912, "mse": 0.0}  0.4678
0         train   7100/102467 42:59/577:29        1.447         {"mlm": 1.4232053792574455, "mse": 0.0}  0.4441
0         train   7200/102467 43:35/576:51        1.447         {"mlm": 1.4243093149405075, "mse": 0.0}  0.4764
0         train   7300/102467 44:12/576:15        1.447         {"mlm": 1.4232442519162927, "mse": 0.0}  0.4687
0         train   7400/102467 44:48/575:37        1.447         {"mlm": 1.4239836336035514, "mse": 0.0}  0.4712
0         train   7500/102467 45:24/575:01        1.446         {"mlm": 1.4220335766245065, "mse": 0.0}  0.4401
0         train   7600/102467 46:00/574:24        1.445         {"mlm": 1.4204617793961622, "mse": 0.0}  0.4538
0         train   7700/102467 46:37/573:46        1.445         {"mlm": 1.4200042478463617, "mse": 0.0}  0.4728
0         train   7800/102467 47:13/573:09        1.445         {"mlm": 1.4204094618243779, "mse": 0.0}  0.5188
0         train   7900/102467 47:49/572:31        1.444         {"mlm": 1.4204013509755393, "mse": 0.0}  0.4811
0         train   8000/102467 48:25/571:53        1.443         {"mlm": 1.4186086538856364, "mse": 0.0}  0.4533
0         train   8100/102467 49:02/571:16        1.443         {"mlm": 1.4217858004073303, "mse": 0.0}  0.4501
0         train   8200/102467 49:38/570:39        1.443         {"mlm": 1.4348319227598152, "mse": 0.0}  0.4693
0         train   8300/102467 50:14/570:02        1.443         {"mlm": 1.4423862361424677, "mse": 0.0}  0.4703
0         train   8400/102467 50:50/569:25        1.443         {"mlm": 1.4395986500412528, "mse": 0.0}  0.4825
0         train   8500/102467 51:27/568:48        1.443         {"mlm": 1.429281784041274, "mse": 0.0}  0.4668
0         train   8600/102467 52:03/568:11        1.443         {"mlm": 1.4322738552453533, "mse": 0.0}  0.4763
0         train   8700/102467 52:39/567:33        1.442         {"mlm": 1.431348700718633, "mse": 0.0}  0.4915
0         train   8800/102467 53:15/566:57        1.442         {"mlm": 1.4281548515486358, "mse": 0.0}  0.4628
0         train   8900/102467 53:52/566:19        1.442         {"mlm": 1.428122375493071, "mse": 0.0}  0.4677
0         train   9000/102467 54:28/565:42        1.442         {"mlm": 1.4293670387990982, "mse": 0.0}  0.499
0         train   9100/102467 55:04/565:04        1.442         {"mlm": 1.428195640064069, "mse": 0.0}  0.6129
0         train   9200/102467 55:40/564:27        1.441         {"mlm": 1.42745227588459, "mse": 0.0}  0.4496
0         train   9300/102467 56:17/563:50        1.441         {"mlm": 1.4263065657865854, "mse": 0.0}  0.4789
0         train   9400/102467 56:53/563:13        1.441         {"mlm": 1.4253967715197784, "mse": 0.0}  0.444
0         train   9500/102467 57:29/562:37        1.440         {"mlm": 1.4233882632006936, "mse": 0.0}  0.4637
0         train   9600/102467 58:05/562:01        1.440         {"mlm": 1.422558939994726, "mse": 0.0}  0.502
0         train   9700/102467 58:42/561:23        1.440         {"mlm": 1.4224568187206421, "mse": 0.0}  0.4795
0         train   9800/102467 59:18/560:48        1.440         {"mlm": 1.4229151600917889, "mse": 0.0}  0.4892
0         train   9900/102467 59:54/560:11        1.440         {"mlm": 1.4237580154452645, "mse": 0.0}  0.4711
0         train   10000/102467 60:30/559:33       1.439         {"mlm": 1.4234325475468186, "mse": 0.0}  0.4592

09/22/2022 16:25:50 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        0:36/479:29        1.566           None
0         valid   101/781      0:52/ 5:51         1.311           None
0         valid   201/781      1:07/ 3:14         1.323           None
0         valid   301/781      1:22/ 2:11         1.314           None
0         valid   401/781      1:38/ 1:32         1.313           None
0         valid   501/781      1:53/ 1:03         1.319           None
0         valid   601/781      2:08/ 0:38         1.316           None
0         valid   701/781      2:24/ 0:16         1.319           None
0         valid   781/781      2:47/ 0:00         1.314         {"mlm": 1.314020486555698, "mse": 0.0, "train": 0.0}  None
0         train   10100/102467 63:54/584:25       1.439         {"mlm": 1.3767614805698394, "mse": 0.0}  0.4857
0         train   10200/102467 64:30/583:29       1.439         {"mlm": 1.4018088710308074, "mse": 0.0}  0.498
0         train   10300/102467 65:06/582:33       1.439         {"mlm": 1.413335347175598, "mse": 0.0}  0.5721
0         train   10400/102467 65:42/581:37       1.438         {"mlm": 1.4062170577049256, "mse": 0.0}  0.5047
0         train   10500/102467 66:18/580:42       1.438         {"mlm": 1.4021923825740814, "mse": 0.0}  0.4763
0         train   10600/102467 66:53/579:47       1.437         {"mlm": 1.4031041344006856, "mse": 0.0}  0.4981
0         train   10700/102467 67:29/578:53       1.437         {"mlm": 1.4024774508816855, "mse": 0.0}  0.488
0         train   10800/102467 68:05/577:59       1.437         {"mlm": 1.4051273906230926, "mse": 0.0}  0.4803
0         train   10900/102467 68:41/577:05       1.437         {"mlm": 1.407434586683909, "mse": 0.0}  0.4879
0         train   11000/102467 69:17/576:13       1.436         {"mlm": 1.4063273681402206, "mse": 0.0}  0.516
0         train   11100/102467 69:53/575:21       1.436         {"mlm": 1.4049431361393494, "mse": 0.0}  0.5013
0         train   11200/102467 70:30/574:30       1.436         {"mlm": 1.4070098571976026, "mse": 0.0}  0.7143
0         train   11300/102467 71:06/573:40       1.436         {"mlm": 1.4071624355132764, "mse": 0.0}  0.6332
0         train   11400/102467 71:42/572:50       1.436         {"mlm": 1.4076740432637078, "mse": 0.0}  0.8773
0         train   11500/102467 72:18/571:59       1.435         {"mlm": 1.4080048172473907, "mse": 0.0}  0.5025
0         train   11600/102467 72:54/571:10       1.435         {"mlm": 1.4088416571170093, "mse": 0.0}  0.5049
0         train   11700/102467 73:31/570:21       1.435         {"mlm": 1.4073767280227998, "mse": 0.0}  0.5425
0         train   11800/102467 74:07/569:32       1.435         {"mlm": 1.407599762181441, "mse": 0.0}  0.51
0         train   11900/102467 74:43/568:43       1.434         {"mlm": 1.4060033723241405, "mse": 0.0}  0.4846
0         train   12000/102467 75:19/567:54       1.434         {"mlm": 1.405276476264, "mse": 0.0}  0.5009
0         train   12100/102467 75:56/567:06       1.433         {"mlm": 1.3817591305935022, "mse": 0.0}  0.4796
