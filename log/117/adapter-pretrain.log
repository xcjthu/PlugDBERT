/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin


/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config fromread config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config 

config/pre-train/adapter.config
read config from config/pre-train/adapter.config
NoneNoneNoneNoneNone



None
None

None
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
            value = d[option]    value = d[option]value = d[option]value = d[option]



      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]    
value = d[option]  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
                return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__



  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError        raise KeyError(key)raise KeyError(key): 

'max_len'
    
During handling of the above exception, another exception occurred:

    raise KeyError(key)Traceback (most recent call last):
raise KeyError(key)
KeyError
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
KeyError        : raise KeyError(key)raise KeyError(key)'max_len':     

'max_len'

During handling of the above exception, another exception occurred:


raise KeyError(key)KeyError
During handling of the above exception, another exception occurred:

Traceback (most recent call last):

: Traceback (most recent call last):
KeyError'max_len'  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
KeyError
:   File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
'max_len'
During handling of the above exception, another exception occurred:

: 
'max_len'Traceback (most recent call last):
KeyError
During handling of the above exception, another exception occurred:


KeyError
During handling of the above exception, another exception occurred:

: :   File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
Traceback (most recent call last):
Traceback (most recent call last):
'max_len''max_len'

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func

During handling of the above exception, another exception occurred:

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
            return getattr(self.config, func_name)(*args, **kwargs)return getattr(self.config, func_name)(*args, **kwargs)return getattr(self.config, func_name)(*args, **kwargs)    
    return getattr(self.config, func_name)(*args, **kwargs)

return getattr(self.config, func_name)(*args, **kwargs)
          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.config, func_name)(*args, **kwargs)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
        return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,    return self._get_conv(section, option, int, raw=raw, vars=vars,


    return self._get_conv(section, option, int, raw=raw, vars=vars,  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv

          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,    
return self._get(section, conv, option, raw=raw, vars=vars,      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get

return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,return self._get(section, conv, option, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))    
return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        return conv(self.get(section, option, **kwargs))return conv(self.get(section, option, **kwargs))

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    raise NoOptionError(option, section)
configparser.NoOptionError    : raise NoOptionError(option, section)No option 'max_len' in section: 'train'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
    raise NoOptionError(option, section)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
configparser
.configparserNoOptionError.: NoOptionErrorNo option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

: No option 'max_len' in section: 'train'Traceback (most recent call last):

    
During handling of the above exception, another exception occurred:

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
raise NoOptionError(option, section)Traceback (most recent call last):

    raise NoOptionError(option, section)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get

configparser        raise NoOptionError(option, section)raise NoOptionError(option, section)
.
configparserNoOptionErrorconfigparser: .No option 'max_len' in section: 'train'NoOptionError
configparser.
During handling of the above exception, another exception occurred:

NoOptionError: Traceback (most recent call last):
.No option 'max_len' in section: 'train'NoOptionError
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
: 
During handling of the above exception, another exception occurred:

No option 'max_len' in section: 'train'
Traceback (most recent call last):
: 
During handling of the above exception, another exception occurred:

No option 'max_len' in section: 'train'  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):


During handling of the above exception, another exception occurred:

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
    value = d[option]  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    
value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return self.__missing__(key)            # support subclasses that define __missing__
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError    :     raise KeyError(key)'max_len'raise KeyError(key)



During handling of the above exception, another exception occurred:

KeyErrorKeyErrorTraceback (most recent call last):
: : 'max_len'  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
'max_len'


During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
      File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
raise KeyError(key)
KeyError: 'max_len'    
raise KeyError(key)
During handling of the above exception, another exception occurred:

    
Traceback (most recent call last):
raise KeyError(key)KeyError
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
: KeyError'max_len'
: 
During handling of the above exception, another exception occurred:

'max_len'
Traceback (most recent call last):

During handling of the above exception, another exception occurred:

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
Traceback (most recent call last):
    return getattr(self.local_config, func_name)(*args, **kwargs)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
        return getattr(self.local_config, func_name)(*args, **kwargs)return getattr(self.local_config, func_name)(*args, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.local_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self.__missing__(key)            # support subclasses that define __missing__    return getattr(self.local_config, func_name)(*args, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return getattr(self.local_config, func_name)(*args, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.local_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self._get_conv(section, option, int, raw=raw, vars=vars,
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,

      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
    return getattr(self.local_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
        return self._get(section, conv, option, raw=raw, vars=vars,return self._get(section, conv, option, raw=raw, vars=vars,

        return self._get(section, conv, option, raw=raw, vars=vars,  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
return self._get(section, conv, option, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))    
return conv(self.get(section, option, **kwargs))    
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
return conv(self.get(section, option, **kwargs))
    return conv(self.get(section, option, **kwargs))  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get

    return conv(self.get(section, option, **kwargs))  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    raise NoOptionError(option, section)
    configparserraise NoOptionError(option, section).
    NoOptionError    raise NoOptionError(option, section)    configparserraise NoOptionError(option, section): 
raise NoOptionError(option, section).
No option 'max_len' in section: 'train'
configparserNoOptionErrorconfigparser
configparser..: .NoOptionErrorNo option 'max_len' in section: 'train'
During handling of the above exception, another exception occurred:


NoOptionErrorNoOptionError: Traceback (most recent call last):

During handling of the above exception, another exception occurred:

No option 'max_len' in section: 'train': : 
Traceback (most recent call last):
No option 'max_len' in section: 'train'  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
No option 'max_len' in section: 'train'
During handling of the above exception, another exception occurred:


  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return conv(self.get(section, option, **kwargs))    value = d[option]

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
        value = d[option]    value = d[option]
value = d[option]    

value = d[option]  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
        return self.__missing__(key)            # support subclasses that define __missing__raise NoOptionError(option, section)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
        return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__

          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError: 'max_len'    
raise KeyError(key)
During handling of the above exception, another exception occurred:


Traceback (most recent call last):
KeyError:   File "train.py", line 139, in <module>
'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
            raise KeyError(key)main()raise KeyError(key)


KeyError    KeyError  File "train.py", line 86, in main
    main(): : raise KeyError(key)
'max_len''max_len'
      File "train.py", line 86, in main
KeyError

raise KeyError(key): 
During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:


'max_len'Traceback (most recent call last):
Traceback (most recent call last):


During handling of the above exception, another exception occurred:

  File "train.py", line 139, in <module>
  File "train.py", line 139, in <module>
Traceback (most recent call last):
KeyError:   File "train.py", line 139, in <module>
'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    main()
  File "train.py", line 86, in main
        parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
                main()main()main()main()



  File "train.py", line 86, in main
  File "train.py", line 86, in main
  File "train.py", line 86, in main
      File "train.py", line 86, in main
parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
                parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)



  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 86, in main
    parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
            result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)        result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)    

result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)    result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset


  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
    result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
            init_formatter(config, ["train", "valid"], *args, **params)init_formatter(config, ["train", "valid"], *args, **params)init_formatter(config, ["train", "valid"], *args, **params)
    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter

init_formatter(config, ["train", "valid"], *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
    init_formatter(config, ["train", "valid"], *args, **params)
    init_formatter(config, ["train", "valid"], *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
    
init_formatter(config, ["train", "valid"], *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
    formatter[task] = form.init_formatter(config, task, *args, **params)    
formatter[task] = form.init_formatter(config, task, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
      File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
formatter[task] = form.init_formatter(config, task, *args, **params)    
formatter[task] = form.init_formatter(config, task, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
    init_formatter(config, ["train", "valid"], *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
            formatter[task] = form.init_formatter(config, task, *args, **params)formatter[task] = form.init_formatter(config, task, *args, **params)formatter[task] = form.init_formatter(config, task, *args, **params)


  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
    formatter[task] = form.init_formatter(config, task, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
            formatter = formatter_list[which](config, mode, *args, **params)formatter = formatter_list[which](config, mode, *args, **params)formatter = formatter_list[which](config, mode, *args, **params)


        formatter = formatter_list[which](config, mode, *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
formatter = formatter_list[which](config, mode, *args, **params)
    
formatter = formatter_list[which](config, mode, *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
    formatter = formatter_list[which](config, mode, *args, **params)    formatter = formatter_list[which](config, mode, *args, **params)

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
                self.max_len = config.getint("train", "max_len")self.max_len = config.getint("train", "max_len")self.max_len = config.getint("train", "max_len")    

self.max_len = config.getint("train", "max_len")    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
self.max_len = config.getint("train", "max_len")
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
self.max_len = config.getint("train", "max_len")  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func


  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
    self.max_len = config.getint("train", "max_len")
      File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
self.max_len = config.getint("train", "max_len")
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
    return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.default_config, func_name)(*args, **kwargs)    
return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.default_config, func_name)(*args, **kwargs)return getattr(self.default_config, func_name)(*args, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.default_config, func_name)(*args, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
        return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get(section, conv, option, raw=raw, vars=vars,
    return self._get(section, conv, option, raw=raw, vars=vars,  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
        return self._get(section, conv, option, raw=raw, vars=vars,return self._get(section, conv, option, raw=raw, vars=vars,

      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
        return conv(self.get(section, option, **kwargs))return conv(self.get(section, option, **kwargs))

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        return conv(self.get(section, option, **kwargs))return conv(self.get(section, option, **kwargs))

      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        raise NoOptionError(option, section)raise NoOptionError(option, section)

configparserconfigparser..NoOptionErrorNoOptionError: : No option 'max_len' in section: 'train'No option 'max_len' in section: 'train'

    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        raise NoOptionError(option, section)raise NoOptionError(option, section)

configparser    .configparser    raise NoOptionError(option, section)NoOptionError.raise NoOptionError(option, section)

NoOptionError: configparserNo option 'max_len' in section: 'train'.: 
NoOptionErrorNo option 'max_len' in section: 'train'configparser
: .No option 'max_len' in section: 'train'NoOptionError
: No option 'max_len' in section: 'train'
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 48301) of binary: /home/xiaochaojun/env/miniconda3/bin/python3
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 48302)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 48303)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 48304)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 48305)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 48306)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 48307)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 48308)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 48301)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin


/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
None
None
NoneNoneNone


None
None
None
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
 roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:11:16 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,038 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,038 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,039 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,052 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,052 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,052 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,075 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,075 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,075 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,076 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,076 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,076 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,077 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,077 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,077 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,079 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,079 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,079 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,958 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,959 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,959 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:29,245 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:29,245 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:29,245 >> Static Memory 0.00 GB, Max Memory 0.00 GB
09/22/2022 15:11:35 - WARNING - tools.init_tool -   Cannot load checkpoint file with error [Errno 2] No such file or directory: 'checkpoints/BERT-Adapter'
09/22/2022 15:11:35 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 16
shuffle: True
reader_num: 8
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
========
[eval]
batch_size: 16
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:11:35 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
Traceback (most recent call last):
  File "train.py", line 139, in <module>
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    for step, data in enumerate(dataset):
    main()      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__

  File "train.py", line 95, in main
main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):    
for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
        data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
        return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
        data.reraise()    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
            raise exception
data.reraise()raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers


  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    raise exception    main()
  File "train.py", line 95, in main

TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin





/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from read config fromconfig/pre-train/adapter.config
 config/pre-train/adapter.configread config fromread config from
read config from  config/pre-train/adapter.configconfig/pre-train/adapter.config 

config/pre-train/adapter.config
read config from config/pre-train/adapter.config
NoneNone

None
None
NoneNone

None
None
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
formatter roberta-base
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:13:56 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:13:59,689 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:13:59,689 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:13:59,689 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:01,358 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:01,358 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:01,358 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:02,515 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:02,515 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:02,515 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:03,386 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:03,386 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:03,387 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:03,582 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:03,582 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:03,582 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:05,181 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:05,181 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:05,181 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:05,399 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:05,399 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:05,399 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:13,116 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:13,116 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:13,116 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:14:18 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:14:18 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 16
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
========
[eval]
batch_size: 16
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:14:18 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:14:18 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    return forward_call(*input, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    main()    
return forward_call(*input, **kwargs)  File "train.py", line 95, in main

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    main()
  File "train.py", line 95, in main
        return forward_call(*input, **kwargs)train(parameters, config, gpu_list, do_test, args.local_rank)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
                output = self.module(*inputs[0], **kwargs[0])output = self.module(*inputs[0], **kwargs[0])output = self.module(*inputs[0], **kwargs[0])output = self.module(*inputs[0], **kwargs[0])



  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    output = self.module(*inputs[0], **kwargs[0])    
output = self.module(*inputs[0], **kwargs[0])
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
output = self.module(*inputs[0], **kwargs[0])  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    
output = self.module(*inputs[0], **kwargs[0])
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    out = self.model(
        out = self.model(  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl

out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        return forward_call(*input, **kwargs)
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
        out = self.model(return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
            outputs = self.roberta(outputs = self.roberta(
outputs = self.roberta(
    
outputs = self.roberta(  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    
outputs = self.roberta(  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
outputs = self.roberta(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        outputs = self.roberta(outputs = self.roberta(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(    
encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    layer_outputs = layer_module(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        layer_outputs = layer_module(layer_outputs = layer_module(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    layer_outputs = layer_module(    
layer_outputs = layer_module(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        encoder_outputs = self.encoder(encoder_outputs = self.encoder(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    layer_outputs = layer_module(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
        layer_output = apply_chunking_to_forward(layer_output = apply_chunking_to_forward(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
        layer_output = apply_chunking_to_forward(layer_output = apply_chunking_to_forward(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
        layer_outputs = layer_module(layer_outputs = layer_module(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
        layer_output = apply_chunking_to_forward(layer_output = apply_chunking_to_forward(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)
          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)return forward_fn(*input_tensors)return forward_fn(*input_tensors)


  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)    
intermediate_output = self.intermediate(attention_output)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    hidden_states = self.intermediate_act_fn(hidden_states)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
intermediate_output = self.intermediate(attention_output)
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 7; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return torch._C._nn.gelu(input)
RuntimeError    return torch._C._nn.gelu(input): 
CUDA out of memory. Tried to allocate 96.00 MiB (GPU 2; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
RuntimeError:     CUDA out of memory. Tried to allocate 96.00 MiB (GPU 4; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF    return torch._C._nn.gelu(input)return torch._C._nn.gelu(input)


RuntimeError: RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 96.00 MiB (GPU 3; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 5; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 1; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53274 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53275 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53276 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53277 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53279 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53280 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53281 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 4 (pid: 53278) of binary: /home/xiaochaojun/env/miniconda3/bin/python3
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-09-22_15:14:35
  host      : 103server
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 53278)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from read config fromconfig/pre-train/adapter.config
 config/pre-train/adapter.config
NoneNone

None
None
NoneNone

None
None
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:15:36 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:16:03 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,256 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,256 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,256 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,259 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,260 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,260 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,268 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,268 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,268 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,583 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,583 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,583 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,862 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,862 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,862 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:16,074 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:16,074 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:16,074 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:18,862 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:18,863 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:18,863 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:19,199 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:19,200 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:19,200 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:16:24 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:16:24 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:16:24 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:16:24 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
0         train   1/102467     0:13/23906:25      1.770  tensor(0.7647, device='cuda:0'){"mlm": 1.7702429294586182, "mse": 0.0}
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   101/102467   0:49/838:11        1.473  tensor(0.7233, device='cuda:0'){"mlm": 1.472854727565652, "mse": 0.0}
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
formatter roberta-base
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.

formatter roberta-base
formatterformatter  roberta-baseroberta-base

09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:20:04 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:11,392 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:11,392 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:11,392 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:11,393 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:11,393 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:11,393 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:12,270 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:12,270 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:12,270 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:12,568 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:12,568 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:12,568 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:12,927 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:12,927 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:12,927 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:13,782 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:13,782 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:13,782 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:18,604 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:18,605 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:18,605 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:19,961 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:19,961 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:19,961 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:20:24 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:20:24 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
grad_accumulate: 2
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:20:24 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:20:24 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
0         train   1/102467     0:14/23973:33      1.728         {"mlm": 1.728100299835205, "mse": 0.0}  None
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   101/102467   0:49/837:49        1.471         {"mlm": 1.4712648244187383, "mse": 0.0}  None
0         train   201/102467   1:25/723:44        1.488         {"mlm": 1.4879416129482326, "mse": 0.0}  None
0         train   301/102467   2:01/685:56        1.493         {"mlm": 1.49250820051396, "mse": 0.0}  None
0         train   401/102467   2:37/666:43        1.501         {"mlm": 1.500505304841924, "mse": 0.0}  None
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin


/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

NoneNone

NoneNone

None
None
NoneNone

09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-baseformatter
 roberta-base
formatter roberta-base
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:25:01 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:08,647 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:08,647 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:08,647 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:08,651 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:08,651 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:08,651 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:08,651 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:08,652 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:08,652 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:09,065 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:09,065 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:09,065 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:10,173 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:10,173 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:10,173 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:12,721 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:12,721 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:12,721 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:13,659 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:13,659 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:13,660 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:13,860 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:13,860 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:13,860 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:25:19 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:25:19 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
grad_accumulate: 2
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:25:19 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:25:19 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
