/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin


/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config fromread config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config 

config/pre-train/adapter.config
read config from config/pre-train/adapter.config
NoneNoneNoneNoneNone



None
None

None
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:09:25 - INFO - __main__ -   CUDA available: True
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
            value = d[option]    value = d[option]value = d[option]value = d[option]



      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]    
value = d[option]  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
                return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__



  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError        raise KeyError(key)raise KeyError(key): 

'max_len'
    
During handling of the above exception, another exception occurred:

    raise KeyError(key)Traceback (most recent call last):
raise KeyError(key)
KeyError
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
KeyError        : raise KeyError(key)raise KeyError(key)'max_len':     

'max_len'

During handling of the above exception, another exception occurred:


raise KeyError(key)KeyError
During handling of the above exception, another exception occurred:

Traceback (most recent call last):

: Traceback (most recent call last):
KeyError'max_len'  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
KeyError
:   File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
'max_len'
During handling of the above exception, another exception occurred:

: 
'max_len'Traceback (most recent call last):
KeyError
During handling of the above exception, another exception occurred:


KeyError
During handling of the above exception, another exception occurred:

: :   File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
Traceback (most recent call last):
Traceback (most recent call last):
'max_len''max_len'

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func

During handling of the above exception, another exception occurred:

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 32, in func
            return getattr(self.config, func_name)(*args, **kwargs)return getattr(self.config, func_name)(*args, **kwargs)return getattr(self.config, func_name)(*args, **kwargs)    
    return getattr(self.config, func_name)(*args, **kwargs)

return getattr(self.config, func_name)(*args, **kwargs)
          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.config, func_name)(*args, **kwargs)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
        return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,    return self._get_conv(section, option, int, raw=raw, vars=vars,


    return self._get_conv(section, option, int, raw=raw, vars=vars,  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv

          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,    
return self._get(section, conv, option, raw=raw, vars=vars,      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get

return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,return self._get(section, conv, option, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))    
return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        return conv(self.get(section, option, **kwargs))return conv(self.get(section, option, **kwargs))

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    raise NoOptionError(option, section)
configparser.NoOptionError    : raise NoOptionError(option, section)No option 'max_len' in section: 'train'


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
    raise NoOptionError(option, section)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
configparser
.configparserNoOptionError.: NoOptionErrorNo option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

: No option 'max_len' in section: 'train'Traceback (most recent call last):

    
During handling of the above exception, another exception occurred:

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
raise NoOptionError(option, section)Traceback (most recent call last):

    raise NoOptionError(option, section)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get

configparser        raise NoOptionError(option, section)raise NoOptionError(option, section)
.
configparserNoOptionErrorconfigparser: .No option 'max_len' in section: 'train'NoOptionError
configparser.
During handling of the above exception, another exception occurred:

NoOptionError: Traceback (most recent call last):
.No option 'max_len' in section: 'train'NoOptionError
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
: 
During handling of the above exception, another exception occurred:

No option 'max_len' in section: 'train'
Traceback (most recent call last):
: 
During handling of the above exception, another exception occurred:

No option 'max_len' in section: 'train'  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):


During handling of the above exception, another exception occurred:

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    value = d[option]
    value = d[option]  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    
value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return self.__missing__(key)            # support subclasses that define __missing__
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__    
return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError    :     raise KeyError(key)'max_len'raise KeyError(key)



During handling of the above exception, another exception occurred:

KeyErrorKeyErrorTraceback (most recent call last):
: : 'max_len'  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
'max_len'


During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
      File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
raise KeyError(key)
KeyError: 'max_len'    
raise KeyError(key)
During handling of the above exception, another exception occurred:

    
Traceback (most recent call last):
raise KeyError(key)KeyError
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
: KeyError'max_len'
: 
During handling of the above exception, another exception occurred:

'max_len'
Traceback (most recent call last):

During handling of the above exception, another exception occurred:

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
Traceback (most recent call last):
    return getattr(self.local_config, func_name)(*args, **kwargs)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
        return getattr(self.local_config, func_name)(*args, **kwargs)return getattr(self.local_config, func_name)(*args, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.local_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self.__missing__(key)            # support subclasses that define __missing__    return getattr(self.local_config, func_name)(*args, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return getattr(self.local_config, func_name)(*args, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.local_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self._get_conv(section, option, int, raw=raw, vars=vars,
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,

      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 35, in func
    return getattr(self.local_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
        return self._get(section, conv, option, raw=raw, vars=vars,return self._get(section, conv, option, raw=raw, vars=vars,

        return self._get(section, conv, option, raw=raw, vars=vars,  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
return self._get(section, conv, option, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))    
return conv(self.get(section, option, **kwargs))    
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
return conv(self.get(section, option, **kwargs))
    return conv(self.get(section, option, **kwargs))  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get

    return conv(self.get(section, option, **kwargs))  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    raise NoOptionError(option, section)
    configparserraise NoOptionError(option, section).
    NoOptionError    raise NoOptionError(option, section)    configparserraise NoOptionError(option, section): 
raise NoOptionError(option, section).
No option 'max_len' in section: 'train'
configparserNoOptionErrorconfigparser
configparser..: .NoOptionErrorNo option 'max_len' in section: 'train'
During handling of the above exception, another exception occurred:


NoOptionErrorNoOptionError: Traceback (most recent call last):

During handling of the above exception, another exception occurred:

No option 'max_len' in section: 'train': : 
Traceback (most recent call last):
No option 'max_len' in section: 'train'  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
No option 'max_len' in section: 'train'
During handling of the above exception, another exception occurred:


  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return conv(self.get(section, option, **kwargs))    value = d[option]

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
        value = d[option]    value = d[option]
value = d[option]    

value = d[option]  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
        return self.__missing__(key)            # support subclasses that define __missing__raise NoOptionError(option, section)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
configparser.NoOptionError: No option 'max_len' in section: 'train'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 789, in get
        return self.__missing__(key)            # support subclasses that define __missing__return self.__missing__(key)            # support subclasses that define __missing__

          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
return self.__missing__(key)            # support subclasses that define __missing__

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError: 'max_len'    
raise KeyError(key)
During handling of the above exception, another exception occurred:


Traceback (most recent call last):
KeyError:   File "train.py", line 139, in <module>
'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
            raise KeyError(key)main()raise KeyError(key)


KeyError    KeyError  File "train.py", line 86, in main
    main(): : raise KeyError(key)
'max_len''max_len'
      File "train.py", line 86, in main
KeyError

raise KeyError(key): 
During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:


'max_len'Traceback (most recent call last):
Traceback (most recent call last):


During handling of the above exception, another exception occurred:

  File "train.py", line 139, in <module>
  File "train.py", line 139, in <module>
Traceback (most recent call last):
KeyError:   File "train.py", line 139, in <module>
'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    value = d[option]
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 898, in __getitem__
    main()
  File "train.py", line 86, in main
        parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
                main()main()main()main()



  File "train.py", line 86, in main
  File "train.py", line 86, in main
  File "train.py", line 86, in main
      File "train.py", line 86, in main
parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
                parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)



  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
    return self.__missing__(key)            # support subclasses that define __missing__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/collections/__init__.py", line 890, in __missing__
    raise KeyError(key)
KeyError: 'max_len'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 86, in main
    parameters = init_all(config, gpu_list, args.checkpoint, "train", local_rank = args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/init_tool.py", line 21, in init_all
            result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)        result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)    

result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)    result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset


  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
    result["train_dataset"], result["valid_dataset"] = init_dataset(config, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 99, in init_dataset
            init_formatter(config, ["train", "valid"], *args, **params)init_formatter(config, ["train", "valid"], *args, **params)init_formatter(config, ["train", "valid"], *args, **params)
    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter

init_formatter(config, ["train", "valid"], *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
    init_formatter(config, ["train", "valid"], *args, **params)
    init_formatter(config, ["train", "valid"], *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
    
init_formatter(config, ["train", "valid"], *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
    formatter[task] = form.init_formatter(config, task, *args, **params)    
formatter[task] = form.init_formatter(config, task, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
      File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
formatter[task] = form.init_formatter(config, task, *args, **params)    
formatter[task] = form.init_formatter(config, task, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
    init_formatter(config, ["train", "valid"], *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/reader/reader.py", line 16, in init_formatter
            formatter[task] = form.init_formatter(config, task, *args, **params)formatter[task] = form.init_formatter(config, task, *args, **params)formatter[task] = form.init_formatter(config, task, *args, **params)


  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
    formatter[task] = form.init_formatter(config, task, *args, **params)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/__init__.py", line 26, in init_formatter
            formatter = formatter_list[which](config, mode, *args, **params)formatter = formatter_list[which](config, mode, *args, **params)formatter = formatter_list[which](config, mode, *args, **params)


        formatter = formatter_list[which](config, mode, *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
formatter = formatter_list[which](config, mode, *args, **params)
    
formatter = formatter_list[which](config, mode, *args, **params)  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
    formatter = formatter_list[which](config, mode, *args, **params)    formatter = formatter_list[which](config, mode, *args, **params)

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/SQuAD/SQuADFormatter.py", line 16, in __init__
                self.max_len = config.getint("train", "max_len")self.max_len = config.getint("train", "max_len")self.max_len = config.getint("train", "max_len")    

self.max_len = config.getint("train", "max_len")    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
self.max_len = config.getint("train", "max_len")
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
self.max_len = config.getint("train", "max_len")  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func


  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
    self.max_len = config.getint("train", "max_len")
      File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
self.max_len = config.getint("train", "max_len")
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/config_parser/parser.py", line 37, in func
    return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.default_config, func_name)(*args, **kwargs)    
return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.default_config, func_name)(*args, **kwargs)return getattr(self.default_config, func_name)(*args, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return getattr(self.default_config, func_name)(*args, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
return getattr(self.default_config, func_name)(*args, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 818, in getint
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
        return self._get_conv(section, option, int, raw=raw, vars=vars,return self._get_conv(section, option, int, raw=raw, vars=vars,

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get_conv(section, option, int, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 808, in _get_conv
    return self._get(section, conv, option, raw=raw, vars=vars,
    return self._get(section, conv, option, raw=raw, vars=vars,  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
        return self._get(section, conv, option, raw=raw, vars=vars,return self._get(section, conv, option, raw=raw, vars=vars,

      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
    return self._get(section, conv, option, raw=raw, vars=vars,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 803, in _get
        return conv(self.get(section, option, **kwargs))return conv(self.get(section, option, **kwargs))

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        return conv(self.get(section, option, **kwargs))return conv(self.get(section, option, **kwargs))

      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        raise NoOptionError(option, section)raise NoOptionError(option, section)

configparserconfigparser..NoOptionErrorNoOptionError: : No option 'max_len' in section: 'train'No option 'max_len' in section: 'train'

    return conv(self.get(section, option, **kwargs))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/configparser.py", line 792, in get
        raise NoOptionError(option, section)raise NoOptionError(option, section)

configparser    .configparser    raise NoOptionError(option, section)NoOptionError.raise NoOptionError(option, section)

NoOptionError: configparserNo option 'max_len' in section: 'train'.: 
NoOptionErrorNo option 'max_len' in section: 'train'configparser
: .No option 'max_len' in section: 'train'NoOptionError
: No option 'max_len' in section: 'train'
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'
    raise NoOptionError(option, section)
configparser.NoOptionError: No option 'max_len' in section: 'train'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 48301) of binary: /home/xiaochaojun/env/miniconda3/bin/python3
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 48302)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 48303)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 48304)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 48305)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 48306)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 48307)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 48308)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-09-22_15:09:29
  host      : 103server
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 48301)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin


/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
None
None
NoneNoneNone


None
None
None
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter09/22/2022 15:10:49 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
 roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:10:49 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:11:16 - INFO - tools.init_tool -   Begin to initialize models...
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,038 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,038 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,039 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,052 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,052 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,052 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,075 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,075 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,075 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,076 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,076 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,076 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,077 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,077 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,077 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,079 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,079 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,079 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:28,958 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:28,959 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:28,959 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:11:29,245 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:11:29,245 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:11:29,245 >> Static Memory 0.00 GB, Max Memory 0.00 GB
09/22/2022 15:11:35 - WARNING - tools.init_tool -   Cannot load checkpoint file with error [Errno 2] No such file or directory: 'checkpoints/BERT-Adapter'
09/22/2022 15:11:35 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 16
shuffle: True
reader_num: 8
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
========
[eval]
batch_size: 16
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:11:35 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
Traceback (most recent call last):
  File "train.py", line 139, in <module>
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    for step, data in enumerate(dataset):
    main()      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__

  File "train.py", line 95, in main
main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):    
for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
        data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
        return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
        data.reraise()    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
            raise exception
data.reraise()raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers


  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    raise exception    main()
  File "train.py", line 95, in main

TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 99, in train
    for step, data in enumerate(dataset):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in process
    ctxs = [d["text"] for d in data]
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/formatter/mlm/VallinaFormatter.py", line 24, in <listcomp>
    ctxs = [d["text"] for d in data]
TypeError: string indices must be integers

/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin





/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from read config fromconfig/pre-train/adapter.config
 config/pre-train/adapter.configread config fromread config from
read config from  config/pre-train/adapter.configconfig/pre-train/adapter.config 

config/pre-train/adapter.config
read config from config/pre-train/adapter.config
NoneNone

None
None
NoneNone

None
None
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
formatter roberta-base
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
09/22/2022 15:13:25 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:13:56 - INFO - tools.init_tool -   Begin to initialize models...
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:13:59,689 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:13:59,689 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:13:59,689 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:01,358 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:01,358 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:01,358 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:02,515 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:02,515 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:02,515 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:03,386 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:03,386 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:03,387 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:03,582 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:03,582 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:03,582 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:05,181 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:05,181 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:05,181 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:05,399 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:05,399 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:05,399 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:14:13,116 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:14:13,116 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:14:13,116 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:14:18 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:14:18 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 16
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
========
[eval]
batch_size: 16
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:14:18 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:14:18 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    return forward_call(*input, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    main()    
return forward_call(*input, **kwargs)  File "train.py", line 95, in main

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    main()
  File "train.py", line 95, in main
        return forward_call(*input, **kwargs)train(parameters, config, gpu_list, do_test, args.local_rank)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 114, in train
    results = model(data, config, gpu_list, acc_result, "train")
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
                output = self.module(*inputs[0], **kwargs[0])output = self.module(*inputs[0], **kwargs[0])output = self.module(*inputs[0], **kwargs[0])output = self.module(*inputs[0], **kwargs[0])



  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    output = self.module(*inputs[0], **kwargs[0])    
output = self.module(*inputs[0], **kwargs[0])
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
output = self.module(*inputs[0], **kwargs[0])  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    
output = self.module(*inputs[0], **kwargs[0])
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    out = self.model(
        out = self.model(  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl

out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    out = self.model(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        return forward_call(*input, **kwargs)
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/model/Pretrain/VallinaPretrain.py", line 54, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
        out = self.model(return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 1095, in forward
            outputs = self.roberta(outputs = self.roberta(
outputs = self.roberta(
    
outputs = self.roberta(  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    
outputs = self.roberta(  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
outputs = self.roberta(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        outputs = self.roberta(outputs = self.roberta(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(    
encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 851, in forward
    encoder_outputs = self.encoder(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    layer_outputs = layer_module(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        layer_outputs = layer_module(layer_outputs = layer_module(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    layer_outputs = layer_module(    
layer_outputs = layer_module(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        encoder_outputs = self.encoder(encoder_outputs = self.encoder(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    layer_outputs = layer_module(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
    return forward_call(*input, **kwargs)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
        layer_output = apply_chunking_to_forward(layer_output = apply_chunking_to_forward(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
        layer_output = apply_chunking_to_forward(layer_output = apply_chunking_to_forward(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
    return forward_call(*input, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 526, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
        layer_outputs = layer_module(layer_outputs = layer_module(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 453, in forward
        layer_output = apply_chunking_to_forward(layer_output = apply_chunking_to_forward(

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2371, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)
          File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)return forward_fn(*input_tensors)return forward_fn(*input_tensors)


  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    intermediate_output = self.intermediate(attention_output)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_fn(*input_tensors)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 465, in feed_forward_chunk
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)    
intermediate_output = self.intermediate(attention_output)  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu

  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    hidden_states = self.intermediate_act_fn(hidden_states)
      File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
intermediate_output = self.intermediate(attention_output)
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 7; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return torch._C._nn.gelu(input)
RuntimeError    return torch._C._nn.gelu(input): 
CUDA out of memory. Tried to allocate 96.00 MiB (GPU 2; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
RuntimeError:     CUDA out of memory. Tried to allocate 96.00 MiB (GPU 4; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF    return torch._C._nn.gelu(input)return torch._C._nn.gelu(input)


RuntimeError: RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 96.00 MiB (GPU 3; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return forward_call(*input, **kwargs)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py", line 365, in forward
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 5; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1556, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 1; 10.76 GiB total capacity; 9.38 GiB already allocated; 43.12 MiB free; 9.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53274 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53275 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53276 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53277 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53279 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53280 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 53281 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 4 (pid: 53278) of binary: /home/xiaochaojun/env/miniconda3/bin/python3
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-09-22_15:14:35
  host      : 103server
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 53278)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from read config fromconfig/pre-train/adapter.config
 config/pre-train/adapter.config
NoneNone

None
None
NoneNone

None
None
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:15:36 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:15:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:16:03 - INFO - tools.init_tool -   Begin to initialize models...
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,256 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,256 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,256 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,259 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,260 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,260 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,268 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,268 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,268 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,583 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,583 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,583 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:14,862 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:14,862 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:14,862 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:16,074 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:16,074 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:16,074 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:18,862 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:18,863 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:18,863 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:16:19,199 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:16:19,200 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:16:19,200 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:16:24 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:16:24 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:16:24 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:16:24 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
0         train   1/102467     0:13/23906:25      1.770  tensor(0.7647, device='cuda:0'){"mlm": 1.7702429294586182, "mse": 0.0}
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:16:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   101/102467   0:49/838:11        1.473  tensor(0.7233, device='cuda:0'){"mlm": 1.472854727565652, "mse": 0.0}
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
formatter roberta-base
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.

formatter roberta-base
formatterformatter  roberta-baseroberta-base

09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:19:36 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:19:36 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:20:04 - INFO - tools.init_tool -   Begin to initialize models...
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:11,392 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:11,392 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:11,392 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:11,393 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:11,393 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:11,393 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:12,270 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:12,270 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:12,270 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:12,568 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:12,568 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:12,568 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:12,927 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:12,927 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:12,927 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:13,782 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:13,782 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:13,782 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:18,604 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:18,605 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:18,605 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:20:19,961 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:20:19,961 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:20:19,961 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:20:24 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:20:24 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
grad_accumulate: 2
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:20:24 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:20:24 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
0         train   1/102467     0:14/23973:33      1.728         {"mlm": 1.728100299835205, "mse": 0.0}  None
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:20:38 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   101/102467   0:49/837:49        1.471         {"mlm": 1.4712648244187383, "mse": 0.0}  None
0         train   201/102467   1:25/723:44        1.488         {"mlm": 1.4879416129482326, "mse": 0.0}  None
0         train   301/102467   2:01/685:56        1.493         {"mlm": 1.49250820051396, "mse": 0.0}  None
0         train   401/102467   2:37/666:43        1.501         {"mlm": 1.500505304841924, "mse": 0.0}  None
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin


/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

NoneNone

NoneNone

None
None
NoneNone

09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-baseformatter
 roberta-base
formatter roberta-base
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/22/2022 15:24:31 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/22/2022 15:25:01 - INFO - tools.init_tool -   Begin to initialize models...
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:08,647 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:08,647 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:08,647 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:08,651 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:08,651 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:08,651 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:08,651 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:08,652 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:08,652 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:09,065 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:09,065 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:09,065 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:10,173 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:10,173 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:10,173 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:12,721 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:12,721 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:12,721 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:13,659 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:13,659 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:13,660 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-22 15:25:13,860 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-22 15:25:13,860 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-22 15:25:13,860 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/22/2022 15:25:19 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/22/2022 15:25:19 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
grad_accumulate: 2
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
========
09/22/2022 15:25:19 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/22/2022 15:25:19 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/22/2022 15:25:32 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   100/102467   0:48/833:03        1.474         {"mlm": 1.4740522986650466, "mse": 0.0}  0.5573
0         train   200/102467   1:24/721:01        1.490         {"mlm": 1.4902419385313987, "mse": 0.0}  0.5464
0         train   300/102467   2:00/683:15        1.492         {"mlm": 1.4918665808439255, "mse": 0.0}  0.5164
0         train   400/102467   2:36/664:17        1.500         {"mlm": 1.5003701157867908, "mse": 0.0}  0.505
0         train   500/102467   3:12/652:41        1.497         {"mlm": 1.4966139410734176, "mse": 0.0}  0.5092
0         train   600/102467   3:47/644:44        1.495         {"mlm": 1.4946815793712933, "mse": 0.0}  0.4998
0         train   700/102467   4:23/639:02        1.489         {"mlm": 1.4892618253401348, "mse": 0.0}  0.495
0         train   800/102467   4:59/634:45        1.488         {"mlm": 1.4882808303087949, "mse": 0.0}  0.4915
0         train   900/102467   5:35/631:11        1.483         {"mlm": 1.4828980328639347, "mse": 0.0}  0.482
0         train   1000/102467  6:11/628:14        1.481         {"mlm": 1.4813558894991874, "mse": 0.0}  0.498
0         train   1100/102467  6:47/625:47        1.480         {"mlm": 1.4798838514089585, "mse": 0.0}  0.505
0         train   1200/102467  7:23/623:34        1.479         {"mlm": 1.478777339408795, "mse": 0.0}  0.5017
0         train   1300/102467  7:59/621:49        1.480         {"mlm": 1.4798490166205627, "mse": 0.0}  0.4951
0         train   1400/102467  8:35/620:09        1.478         {"mlm": 1.477761804163456, "mse": 0.0}  0.5024
0         train   1500/102467  9:11/618:42        1.476         {"mlm": 1.4762078045606613, "mse": 0.0}  0.4886
0         train   1600/102467  9:47/617:14        1.475         {"mlm": 1.4748854521289467, "mse": 0.0}  0.5048
0         train   1700/102467 10:23/616:00        1.475         {"mlm": 1.474520916202489, "mse": 0.0}  0.6301
0         train   1800/102467 10:59/614:48        1.473         {"mlm": 1.4730305129620764, "mse": 0.0}  0.494
0         train   1900/102467 11:35/613:47        1.472         {"mlm": 1.471604926617522, "mse": 0.0}  0.5237
0         train   2000/102467 12:11/612:47        1.471         {"mlm": 1.4712602557241916, "mse": 0.0}  0.4868
0         train   2100/102467 12:48/611:48        1.470         {"mlm": 1.4499794341096974, "mse": 0.0}  0.4899
0         train   2200/102467 13:24/610:51        1.470         {"mlm": 1.452539246585501, "mse": 0.0}  0.5048
0         train   2300/102467 14:00/609:55        1.470         {"mlm": 1.4576741355318688, "mse": 0.0}  0.4969
0         train   2400/102467 14:36/609:04        1.469         {"mlm": 1.4561040973603576, "mse": 0.0}  0.5525
0         train   2500/102467 15:12/608:11        1.468         {"mlm": 1.4551511199058655, "mse": 0.0}  0.4754
0         train   2600/102467 15:48/607:20        1.466         {"mlm": 1.4484001490627982, "mse": 0.0}  0.5064
0         train   2700/102467 16:24/606:30        1.465         {"mlm": 1.4469215601298941, "mse": 0.0}  0.4666
0         train   2800/102467 17:01/605:43        1.464         {"mlm": 1.4460571742475554, "mse": 0.0}  0.4517
0         train   2900/102467 17:37/605:01        1.463         {"mlm": 1.4429334470374433, "mse": 0.0}  0.4652
0         train   3000/102467 18:13/604:15        1.462         {"mlm": 1.4434500427336783, "mse": 0.0}  0.7303
0         train   3100/102467 18:49/603:31        1.462         {"mlm": 1.4449343807703805, "mse": 0.0}  0.5075
0         train   3200/102467 19:25/602:48        1.462         {"mlm": 1.4460735543257401, "mse": 0.0}  0.5602
0         train   3300/102467 20:02/602:04        1.461         {"mlm": 1.4440977411420644, "mse": 0.0}  0.4593
0         train   3400/102467 20:38/601:21        1.460         {"mlm": 1.4447737657401798, "mse": 0.0}  0.4726
0         train   3500/102467 21:14/600:43        1.460         {"mlm": 1.4444062738040035, "mse": 0.0}  0.5796
0         train   3600/102467 21:50/599:59        1.459         {"mlm": 1.4436605030704543, "mse": 0.0}  0.4497
0         train   3700/102467 22:27/599:17        1.459         {"mlm": 1.4434094078545012, "mse": 0.0}  0.4531
0         train   3800/102467 23:03/598:36        1.458         {"mlm": 1.4424468032050755, "mse": 0.0}  0.4909
0         train   3900/102467 23:39/597:55        1.458         {"mlm": 1.4439159507121206, "mse": 0.0}  0.43
0         train   4000/102467 24:15/597:13        1.458         {"mlm": 1.4439913727093363, "mse": 0.0}  0.4542
0         train   4100/102467 24:51/596:33        1.458         {"mlm": 1.4605077249663216, "mse": 0.0}  0.4588
0         train   4200/102467 25:28/595:53        1.457         {"mlm": 1.4423224673126682, "mse": 0.0}  0.4715
0         train   4300/102467 26:04/595:12        1.456         {"mlm": 1.4351306489650035, "mse": 0.0}  0.4597
0         train   4400/102467 26:40/594:34        1.456         {"mlm": 1.4342835937912142, "mse": 0.0}  0.4639
0         train   4500/102467 27:16/593:54        1.455         {"mlm": 1.433157334126622, "mse": 0.0}  0.4374
0         train   4600/102467 27:53/593:14        1.455         {"mlm": 1.4341992943183235, "mse": 0.0}  0.4629
0         train   4700/102467 28:29/592:36        1.454         {"mlm": 1.436011611390592, "mse": 0.0}  0.464
0         train   4800/102467 29:05/591:56        1.454         {"mlm": 1.4375047198214328, "mse": 0.0}  0.4417
0         train   4900/102467 29:41/591:18        1.454         {"mlm": 1.437394497853876, "mse": 0.0}  0.4716
0         train   5000/102467 30:17/590:38        1.454         {"mlm": 1.4376146355587878, "mse": 0.0}  0.4867
0         train   5100/102467 30:54/590:00        1.454         {"mlm": 1.4388890674096857, "mse": 0.0}  0.4286
0         train   5200/102467 31:30/589:22        1.453         {"mlm": 1.4390857067947993, "mse": 0.0}  0.4332
0         train   5300/102467 32:06/588:44        1.453         {"mlm": 1.4388145082077737, "mse": 0.0}  0.4617
0         train   5400/102467 32:43/588:05        1.453         {"mlm": 1.4388225605133096, "mse": 0.0}  0.4802
0         train   5500/102467 33:19/587:28        1.453         {"mlm": 1.4390266538064216, "mse": 0.0}  0.5629
0         train   5600/102467 33:55/586:50        1.452         {"mlm": 1.4387523926915753, "mse": 0.0}  0.4686
0         train   5700/102467 34:31/586:11        1.452         {"mlm": 1.439138399522633, "mse": 0.0}  0.4929
0         train   5800/102467 35:08/585:35        1.452         {"mlm": 1.4405069799722898, "mse": 0.0}  0.4671
0         train   5900/102467 35:44/584:58        1.452         {"mlm": 1.4397405920779869, "mse": 0.0}  0.4485
0         train   6000/102467 36:20/584:20        1.452         {"mlm": 1.4398357368505992, "mse": 0.0}  0.4811
0         train   6100/102467 36:56/583:42        1.451         {"mlm": 1.4145087993022092, "mse": 0.0}  0.4605
0         train   6200/102467 37:33/583:03        1.451         {"mlm": 1.4314469307812336, "mse": 0.0}  0.4498
0         train   6300/102467 38:09/582:27        1.451         {"mlm": 1.4295868747162097, "mse": 0.0}  0.4545
0         train   6400/102467 38:45/581:49        1.450         {"mlm": 1.4279280231341307, "mse": 0.0}  0.4714
0         train   6500/102467 39:21/581:11        1.451         {"mlm": 1.4353301028610475, "mse": 0.0}  0.4838
0         train   6600/102467 39:58/580:34        1.450         {"mlm": 1.4328683972159022, "mse": 0.0}  0.436
0         train   6700/102467 40:34/579:56        1.449         {"mlm": 1.428573836555098, "mse": 0.0}  0.4684
0         train   6800/102467 41:10/579:21        1.448         {"mlm": 1.4227293659483025, "mse": 0.0}  0.4441
0         train   6900/102467 41:47/578:43        1.448         {"mlm": 1.4222491256635192, "mse": 0.0}  0.5278
0         train   7000/102467 42:23/578:06        1.448         {"mlm": 1.4242600817139912, "mse": 0.0}  0.4678
0         train   7100/102467 42:59/577:29        1.447         {"mlm": 1.4232053792574455, "mse": 0.0}  0.4441
0         train   7200/102467 43:35/576:51        1.447         {"mlm": 1.4243093149405075, "mse": 0.0}  0.4764
0         train   7300/102467 44:12/576:15        1.447         {"mlm": 1.4232442519162927, "mse": 0.0}  0.4687
0         train   7400/102467 44:48/575:37        1.447         {"mlm": 1.4239836336035514, "mse": 0.0}  0.4712
0         train   7500/102467 45:24/575:01        1.446         {"mlm": 1.4220335766245065, "mse": 0.0}  0.4401
0         train   7600/102467 46:00/574:24        1.445         {"mlm": 1.4204617793961622, "mse": 0.0}  0.4538
0         train   7700/102467 46:37/573:46        1.445         {"mlm": 1.4200042478463617, "mse": 0.0}  0.4728
0         train   7800/102467 47:13/573:09        1.445         {"mlm": 1.4204094618243779, "mse": 0.0}  0.5188
0         train   7900/102467 47:49/572:31        1.444         {"mlm": 1.4204013509755393, "mse": 0.0}  0.4811
0         train   8000/102467 48:25/571:53        1.443         {"mlm": 1.4186086538856364, "mse": 0.0}  0.4533
0         train   8100/102467 49:02/571:16        1.443         {"mlm": 1.4217858004073303, "mse": 0.0}  0.4501
0         train   8200/102467 49:38/570:39        1.443         {"mlm": 1.4348319227598152, "mse": 0.0}  0.4693
0         train   8300/102467 50:14/570:02        1.443         {"mlm": 1.4423862361424677, "mse": 0.0}  0.4703
0         train   8400/102467 50:50/569:25        1.443         {"mlm": 1.4395986500412528, "mse": 0.0}  0.4825
0         train   8500/102467 51:27/568:48        1.443         {"mlm": 1.429281784041274, "mse": 0.0}  0.4668
0         train   8600/102467 52:03/568:11        1.443         {"mlm": 1.4322738552453533, "mse": 0.0}  0.4763
0         train   8700/102467 52:39/567:33        1.442         {"mlm": 1.431348700718633, "mse": 0.0}  0.4915
0         train   8800/102467 53:15/566:57        1.442         {"mlm": 1.4281548515486358, "mse": 0.0}  0.4628
0         train   8900/102467 53:52/566:19        1.442         {"mlm": 1.428122375493071, "mse": 0.0}  0.4677
0         train   9000/102467 54:28/565:42        1.442         {"mlm": 1.4293670387990982, "mse": 0.0}  0.499
0         train   9100/102467 55:04/565:04        1.442         {"mlm": 1.428195640064069, "mse": 0.0}  0.6129
0         train   9200/102467 55:40/564:27        1.441         {"mlm": 1.42745227588459, "mse": 0.0}  0.4496
0         train   9300/102467 56:17/563:50        1.441         {"mlm": 1.4263065657865854, "mse": 0.0}  0.4789
0         train   9400/102467 56:53/563:13        1.441         {"mlm": 1.4253967715197784, "mse": 0.0}  0.444
0         train   9500/102467 57:29/562:37        1.440         {"mlm": 1.4233882632006936, "mse": 0.0}  0.4637
0         train   9600/102467 58:05/562:01        1.440         {"mlm": 1.422558939994726, "mse": 0.0}  0.502
0         train   9700/102467 58:42/561:23        1.440         {"mlm": 1.4224568187206421, "mse": 0.0}  0.4795
0         train   9800/102467 59:18/560:48        1.440         {"mlm": 1.4229151600917889, "mse": 0.0}  0.4892
0         train   9900/102467 59:54/560:11        1.440         {"mlm": 1.4237580154452645, "mse": 0.0}  0.4711
0         train   10000/102467 60:30/559:33       1.439         {"mlm": 1.4234325475468186, "mse": 0.0}  0.4592

09/22/2022 16:25:50 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        0:36/479:29        1.566           None
0         valid   101/781      0:52/ 5:51         1.311           None
0         valid   201/781      1:07/ 3:14         1.323           None
0         valid   301/781      1:22/ 2:11         1.314           None
0         valid   401/781      1:38/ 1:32         1.313           None
0         valid   501/781      1:53/ 1:03         1.319           None
0         valid   601/781      2:08/ 0:38         1.316           None
0         valid   701/781      2:24/ 0:16         1.319           None
0         valid   781/781      2:47/ 0:00         1.314         {"mlm": 1.314020486555698, "mse": 0.0, "train": 0.0}  None
0         train   10100/102467 63:54/584:25       1.439         {"mlm": 1.3767614805698394, "mse": 0.0}  0.4857
0         train   10200/102467 64:30/583:29       1.439         {"mlm": 1.4018088710308074, "mse": 0.0}  0.498
0         train   10300/102467 65:06/582:33       1.439         {"mlm": 1.413335347175598, "mse": 0.0}  0.5721
0         train   10400/102467 65:42/581:37       1.438         {"mlm": 1.4062170577049256, "mse": 0.0}  0.5047
0         train   10500/102467 66:18/580:42       1.438         {"mlm": 1.4021923825740814, "mse": 0.0}  0.4763
0         train   10600/102467 66:53/579:47       1.437         {"mlm": 1.4031041344006856, "mse": 0.0}  0.4981
0         train   10700/102467 67:29/578:53       1.437         {"mlm": 1.4024774508816855, "mse": 0.0}  0.488
0         train   10800/102467 68:05/577:59       1.437         {"mlm": 1.4051273906230926, "mse": 0.0}  0.4803
0         train   10900/102467 68:41/577:05       1.437         {"mlm": 1.407434586683909, "mse": 0.0}  0.4879
0         train   11000/102467 69:17/576:13       1.436         {"mlm": 1.4063273681402206, "mse": 0.0}  0.516
0         train   11100/102467 69:53/575:21       1.436         {"mlm": 1.4049431361393494, "mse": 0.0}  0.5013
0         train   11200/102467 70:30/574:30       1.436         {"mlm": 1.4070098571976026, "mse": 0.0}  0.7143
0         train   11300/102467 71:06/573:40       1.436         {"mlm": 1.4071624355132764, "mse": 0.0}  0.6332
0         train   11400/102467 71:42/572:50       1.436         {"mlm": 1.4076740432637078, "mse": 0.0}  0.8773
0         train   11500/102467 72:18/571:59       1.435         {"mlm": 1.4080048172473907, "mse": 0.0}  0.5025
0         train   11600/102467 72:54/571:10       1.435         {"mlm": 1.4088416571170093, "mse": 0.0}  0.5049
0         train   11700/102467 73:31/570:21       1.435         {"mlm": 1.4073767280227998, "mse": 0.0}  0.5425
0         train   11800/102467 74:07/569:32       1.435         {"mlm": 1.407599762181441, "mse": 0.0}  0.51
0         train   11900/102467 74:43/568:43       1.434         {"mlm": 1.4060033723241405, "mse": 0.0}  0.4846
0         train   12000/102467 75:19/567:54       1.434         {"mlm": 1.405276476264, "mse": 0.0}  0.5009
0         train   12100/102467 75:56/567:06       1.433         {"mlm": 1.3817591305935022, "mse": 0.0}  0.4796
0         train   12200/102467 76:32/566:17       1.433         {"mlm": 1.3985696006659887, "mse": 0.0}  0.5551
0         train   12300/102467 77:08/565:29       1.433         {"mlm": 1.4026025414068164, "mse": 0.0}  0.5294
0         train   12400/102467 77:44/564:42       1.433         {"mlm": 1.4036805838869328, "mse": 0.0}  0.5251
0         train   12500/102467 78:21/563:55       1.433         {"mlm": 1.4076243770385315, "mse": 0.0}  0.4994
0         train   12600/102467 78:57/563:07       1.433         {"mlm": 1.4079022377679662, "mse": 0.0}  0.5281
0         train   12700/102467 79:33/562:19       1.432         {"mlm": 1.4039488048512536, "mse": 0.0}  0.5404
0         train   12800/102467 80:09/561:33       1.432         {"mlm": 1.4041412564481752, "mse": 0.0}  0.5242
0         train   12900/102467 80:45/560:46       1.432         {"mlm": 1.4052239645177185, "mse": 0.0}  0.5206
0         train   13000/102467 81:22/559:59       1.432         {"mlm": 1.4079890201638292, "mse": 0.0}  0.526
0         train   13100/102467 81:58/559:13       1.432         {"mlm": 1.4095909212587963, "mse": 0.0}  0.5115
0         train   13200/102467 82:34/558:27       1.432         {"mlm": 1.4100890264996297, "mse": 0.0}  0.4972
0         train   13300/102467 83:11/557:41       1.431         {"mlm": 1.410048965218436, "mse": 0.0}  0.514
0         train   13400/102467 83:47/556:54       1.431         {"mlm": 1.4100118432409683, "mse": 0.0}  0.5064
0         train   13500/102467 84:23/556:08       1.431         {"mlm": 1.410603472516885, "mse": 0.0}  0.4956
0         train   13600/102467 84:59/555:23       1.431         {"mlm": 1.4115597747727586, "mse": 0.0}  0.5224
0         train   13700/102467 85:36/554:38       1.431         {"mlm": 1.4126634705831473, "mse": 0.0}  0.544
0         train   13800/102467 86:12/553:52       1.431         {"mlm": 1.4126729824067223, "mse": 0.0}  0.5227
0         train   13900/102467 86:48/553:07       1.431         {"mlm": 1.4145153399327355, "mse": 0.0}  0.5285
0         train   14000/102467 87:24/552:23       1.431         {"mlm": 1.4140742090477116, "mse": 0.0}  0.5044
0         train   14100/102467 88:01/551:38       1.431         {"mlm": 1.4383572364340023, "mse": 0.0}  0.5279
0         train   14200/102467 88:37/550:54       1.431         {"mlm": 1.412949755336299, "mse": 0.0}  0.4977
0         train   14300/102467 89:13/550:09       1.431         {"mlm": 1.4156179694121316, "mse": 0.0}  0.5323
0         train   14400/102467 89:50/549:25       1.431         {"mlm": 1.4162198767889684, "mse": 0.0}  0.5291
0         train   14500/102467 90:26/548:41       1.431         {"mlm": 1.4180695935186134, "mse": 0.0}  0.5018
0         train   14600/102467 91:02/547:56       1.430         {"mlm": 1.4179201573631834, "mse": 0.0}  0.5085
0         train   14700/102467 91:39/547:13       1.430         {"mlm": 1.4165318636463844, "mse": 0.0}  0.5234
0         train   14800/102467 92:15/546:29       1.430         {"mlm": 1.4163108919945575, "mse": 0.0}  0.5494
0         train   14900/102467 92:51/545:45       1.430         {"mlm": 1.4149262042911122, "mse": 0.0}  0.57
0         train   15000/102467 93:28/545:01       1.430         {"mlm": 1.4119131483271987, "mse": 0.0}  0.5215
0         train   15100/102467 94:04/544:18       1.430         {"mlm": 1.4119718273480732, "mse": 0.0}  0.7878
0         train   15200/102467 94:40/543:34       1.429         {"mlm": 1.411793576407313, "mse": 0.0}  0.5245
0         train   15300/102467 95:16/542:50       1.430         {"mlm": 1.4140223170557449, "mse": 0.0}  0.5353
0         train   15400/102467 95:53/542:07       1.429         {"mlm": 1.4137012812286316, "mse": 0.0}  0.5562
0         train   15500/102467 96:29/541:23       1.429         {"mlm": 1.4133342122124417, "mse": 0.0}  0.6382
0         train   15600/102467 97:05/540:39       1.429         {"mlm": 1.4117626531028629, "mse": 0.0}  0.5442
0         train   15700/102467 97:41/539:56       1.429         {"mlm": 1.411366388675602, "mse": 0.0}  0.583
0         train   15800/102467 98:18/539:13       1.429         {"mlm": 1.4105073717796763, "mse": 0.0}  0.5707
0         train   15900/102467 98:54/538:29       1.429         {"mlm": 1.4112096161623775, "mse": 0.0}  0.6707
0         train   16000/102467 99:30/537:46       1.428         {"mlm": 1.4100497948216486, "mse": 0.0}  0.5442
0         train   16100/102467 100:06/537:03      1.428         {"mlm": 1.3894832146536444, "mse": 0.0}  0.4934
0         train   16200/102467 100:43/536:20      1.428         {"mlm": 1.37715131074644, "mse": 0.0}  0.5494
0         train   16300/102467 101:19/535:37      1.427         {"mlm": 1.380593736565073, "mse": 0.0}  0.5725
0         train   16400/102467 101:55/534:55      1.427         {"mlm": 1.3855003346724233, "mse": 0.0}  0.5826
0         train   16500/102467 102:32/534:13      1.427         {"mlm": 1.3857436406780297, "mse": 0.0}  0.523
0         train   16600/102467 103:08/533:30      1.427         {"mlm": 1.3936326281109848, "mse": 0.0}  0.5172
0         train   16700/102467 103:44/532:48      1.427         {"mlm": 1.3933045616964015, "mse": 0.0}  0.602
0         train   16800/102467 104:21/532:06      1.427         {"mlm": 1.3913198500834265, "mse": 0.0}  0.699
0         train   16900/102467 104:57/531:23      1.426         {"mlm": 1.3921311667797425, "mse": 0.0}  0.5714
0         train   17000/102467 105:33/530:41      1.426         {"mlm": 1.390633629879717, "mse": 0.0}  0.5806
0         train   17100/102467 106:09/529:58      1.426         {"mlm": 1.393649558153822, "mse": 0.0}  0.539
0         train   17200/102467 106:45/529:16      1.426         {"mlm": 1.3916455305608593, "mse": 0.0}  0.5602
0         train   17300/102467 107:22/528:34      1.426         {"mlm": 1.3906257102006017, "mse": 0.0}  0.5145
0         train   17400/102467 107:58/527:52      1.425         {"mlm": 1.3893321875000137, "mse": 0.0}  0.6013
0         train   17500/102467 108:34/527:11      1.425         {"mlm": 1.3903269259310118, "mse": 0.0}  0.5509
0         train   17600/102467 109:11/526:29      1.425         {"mlm": 1.390102902575441, "mse": 0.0}  0.5316
0         train   17700/102467 109:47/525:47      1.425         {"mlm": 1.3921453667586174, "mse": 0.0}  0.5692
0         train   17800/102467 110:23/525:06      1.425         {"mlm": 1.3934998242272625, "mse": 0.0}  0.5561
0         train   17900/102467 110:59/524:24      1.425         {"mlm": 1.3944321128023758, "mse": 0.0}  0.5894
0         train   18000/102467 111:36/523:42      1.425         {"mlm": 1.3943933697420177, "mse": 0.0}  0.5468
0         train   18100/102467 112:12/523:01      1.424         {"mlm": 1.3829528875648975, "mse": 0.0}  0.5528
0         train   18200/102467 112:48/522:19      1.424         {"mlm": 1.4040553934720097, "mse": 0.0}  0.5408
0         train   18300/102467 113:24/521:38      1.424         {"mlm": 1.3889629865014874, "mse": 0.0}  0.5646
0         train   18400/102467 114:01/520:56      1.424         {"mlm": 1.3884503349210278, "mse": 0.0}  0.514
0         train   18500/102467 114:37/520:15      1.424         {"mlm": 1.3932685610507765, "mse": 0.0}  0.5292
0         train   18600/102467 115:13/519:34      1.424         {"mlm": 1.3955760493174496, "mse": 0.0}  0.5732
0         train   18700/102467 115:50/518:53      1.424         {"mlm": 1.397194549869532, "mse": 0.0}  0.665
0         train   18800/102467 116:26/518:11      1.423         {"mlm": 1.398319864168239, "mse": 0.0}  0.5324
0         train   18900/102467 117:02/517:30      1.423         {"mlm": 1.398557897457587, "mse": 0.0}  0.5837
0         train   19000/102467 117:38/516:48      1.423         {"mlm": 1.397734338799155, "mse": 0.0}  0.542
0         train   19100/102467 118:15/516:08      1.423         {"mlm": 1.3973786777062138, "mse": 0.0}  0.5608
0         train   19200/102467 118:51/515:26      1.423         {"mlm": 1.3966036778329607, "mse": 0.0}  0.5139
0         train   19300/102467 119:27/514:45      1.423         {"mlm": 1.3966695603305175, "mse": 0.0}  0.6107
0         train   19400/102467 120:03/514:04      1.423         {"mlm": 1.3966454071523808, "mse": 0.0}  0.5746
0         train   19500/102467 120:39/513:23      1.422         {"mlm": 1.395938483350736, "mse": 0.0}  0.5736
0         train   19600/102467 121:16/512:43      1.422         {"mlm": 1.3981367429919112, "mse": 0.0}  0.5324
0         train   19700/102467 121:52/512:02      1.422         {"mlm": 1.3981551300593704, "mse": 0.0}  0.548
0         train   19800/102467 122:28/511:21      1.422         {"mlm": 1.3981653034753416, "mse": 0.0}  0.5575
0         train   19900/102467 123:04/510:40      1.422         {"mlm": 1.3976195762680552, "mse": 0.0}  0.5443
0         train   20000/102467 123:41/510:00      1.422         {"mlm": 1.3975923218146593, "mse": 0.0}  0.5609

09/22/2022 17:29:00 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
09/22/2022 17:29:47 - ERROR - tools.eval_tool -   There is no data given to the model in this epoch, check your data.
09/22/2022 17:29:47 - ERROR - tools.eval_tool -   There is no data given to the model in this epoch, check your data.
09/22/2022 17:29:47 - ERROR - tools.eval_tool -   There is no data given to the model in this epoch, check your data.
09/22/2022 17:29:47 - ERROR - tools.eval_tool -   There is no data given to the model in this epoch, check your data.
09/22/2022 17:29:47 - ERROR - tools.eval_tool -   There is no data given to the model in this epoch, check your data.
09/22/2022 17:29:47 - ERROR - tools.eval_tool -   There is no data given to the model in this epoch, check your data.
09/22/2022 17:29:47 - ERROR - tools.eval_tool -   There is no data given to the model in this epoch, check your data.
09/22/2022 17:29:47 - ERROR - tools.eval_tool -   There is no data given to the model in this epoch, check your data.
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 183, in train
    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/eval_tool.py", line 89, in valid
    raise NotImplementedError
NotImplementedError
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 183, in train
    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/eval_tool.py", line 89, in valid
    raise NotImplementedError
NotImplementedError
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 183, in train
    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/eval_tool.py", line 89, in valid
    raise NotImplementedError
NotImplementedError
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 183, in train
    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/eval_tool.py", line 89, in valid
    raise NotImplementedError
NotImplementedError
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 183, in train
    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/eval_tool.py", line 89, in valid
    raise NotImplementedError
NotImplementedError
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 183, in train
    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/eval_tool.py", line 89, in valid
Traceback (most recent call last):
    raise NotImplementedError  File "train.py", line 139, in <module>

NotImplementedError
    main()
  File "train.py", line 95, in main
Traceback (most recent call last):
  File "train.py", line 139, in <module>
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 183, in train
    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/eval_tool.py", line 89, in valid
    raise NotImplementedError
NotImplementedError
    main()
  File "train.py", line 95, in main
    train(parameters, config, gpu_list, do_test, args.local_rank)
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/train_tool.py", line 183, in train
    
  File "/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/tools/eval_tool.py", line 89, in valid
    raise NotImplementedError
NotImplementedError
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 36834) of binary: /home/xiaochaojun/env/miniconda3/bin/python3
Traceback (most recent call last):
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2022-09-22_17:29:54
  host      : 101server
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 36835)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2022-09-22_17:29:54
  host      : 101server
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 36836)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2022-09-22_17:29:54
  host      : 101server
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 36837)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2022-09-22_17:29:54
  host      : 101server
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 36838)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2022-09-22_17:29:54
  host      : 101server
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 36839)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2022-09-22_17:29:54
  host      : 101server
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 36840)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2022-09-22_17:29:54
  host      : 101server
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 36841)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-09-22_17:29:54
  host      : 101server
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 36834)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin




/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config

read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
NoneNone

None
None
None
None
NoneNone

09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/23/2022 08:03:20 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/23/2022 08:03:20 - INFO - __main__ -   CUDA available: True
09/23/2022 08:03:20 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
formatter roberta-base
09/23/2022 08:03:20 - INFO - __main__ -   CUDA available: True
09/23/2022 08:03:20 - INFO - __main__ -   CUDA available: True
09/23/2022 08:03:20 - INFO - __main__ -   CUDA available: True
09/23/2022 08:03:20 - INFO - __main__ -   CUDA available: True
formatter roberta-base
09/23/2022 08:03:20 - INFO - __main__ -   CUDA available: True
09/23/2022 08:03:20 - INFO - __main__ -   CUDA available: True
09/23/2022 08:03:20 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/23/2022 08:03:45 - INFO - tools.init_tool -   Begin to initialize models...
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-23 08:03:56,170 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-23 08:03:56,170 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-23 08:03:56,170 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-23 08:03:56,178 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-23 08:03:56,178 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-23 08:03:56,178 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-23 08:03:56,187 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-23 08:03:56,187 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-23 08:03:56,187 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-23 08:03:56,202 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-23 08:03:56,202 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-23 08:03:56,202 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-23 08:03:56,206 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-23 08:03:56,207 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-23 08:03:56,207 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-23 08:03:56,207 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-23 08:03:56,207 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-23 08:03:56,207 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
[INFO|(OpenDelta)basemodel:675]2022-09-23 08:03:56,209 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-23 08:03:56,209 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-23 08:03:56,209 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-23 08:03:56,209 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-23 08:03:56,209 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-23 08:03:56,209 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter []
09/23/2022 08:04:04 - WARNING - tools.init_tool -   Cannot load checkpoint file with error max() arg is an empty sequence
09/23/2022 08:04:04 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
grad_accumulate: 2
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
load_from_path: True
========
09/23/2022 08:04:04 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/23/2022 08:04:04 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
09/23/2022 08:04:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/23/2022 08:04:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/23/2022 08:04:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/23/2022 08:04:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/23/2022 08:04:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/23/2022 08:04:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/23/2022 08:04:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/23/2022 08:04:18 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   100/102467   0:49/840:37        1.482         {"mlm": 1.482006219625473, "mse": 0.0}  0.5306
0         train   200/102467   1:24/724:15        1.494         {"mlm": 1.4939774346351624, "mse": 0.0}  0.5094
0         train   300/102467   2:00/685:20        1.503         {"mlm": 1.5031593028704326, "mse": 0.0}  0.5175
0         train   400/102467   2:36/665:56        1.504         {"mlm": 1.503830224275589, "mse": 0.0}  0.5051
0         train   500/102467   3:12/654:12        1.497         {"mlm": 1.4966217522621155, "mse": 0.0}  0.5047
0         train   600/102467   3:48/646:22        1.489         {"mlm": 1.489461753865083, "mse": 0.0}  0.5139
0         train   700/102467   4:24/640:58        1.487         {"mlm": 1.4872812249830791, "mse": 0.0}  0.511
0         train   800/102467   5:00/636:34        1.489         {"mlm": 1.4892127487808466, "mse": 0.0}  0.5301
0         train   900/102467   5:36/632:56        1.485         {"mlm": 1.48528458668126, "mse": 0.0}  0.4825
0         train   1000/102467  6:12/630:06        1.483         {"mlm": 1.4831150681376457, "mse": 0.0}  0.5103
0         train   1100/102467  6:48/627:37        1.480         {"mlm": 1.4799851901422847, "mse": 0.0}  0.5023
0         train   1200/102467  7:24/625:29        1.478         {"mlm": 1.4775789395471415, "mse": 0.0}  0.5129
0         train   1300/102467  8:00/623:42        1.479         {"mlm": 1.4793601863659345, "mse": 0.0}  0.4794
0         train   1400/102467  8:37/622:06        1.477         {"mlm": 1.4772401269418853, "mse": 0.0}  0.485
0         train   1500/102467  9:13/620:44        1.477         {"mlm": 1.4765584569374721, "mse": 0.0}  0.5077
0         train   1600/102467  9:49/619:24        1.475         {"mlm": 1.474974574483931, "mse": 0.0}  0.4996
0         train   1700/102467 10:25/618:14        1.473         {"mlm": 1.4734244306297863, "mse": 0.0}  0.4993
0         train   1800/102467 11:02/617:10        1.473         {"mlm": 1.4732369351055887, "mse": 0.0}  0.4787
0         train   1900/102467 11:38/616:17        1.473         {"mlm": 1.4726052367373517, "mse": 0.0}  0.5039
0         train   2000/102467 12:15/615:21        1.472         {"mlm": 1.4723329032361507, "mse": 0.0}  0.4802
0         train   2100/102467 12:51/614:28        1.472         {"mlm": 1.4671289463235875, "mse": 0.0}  0.4737
0         train   2200/102467 13:27/613:38        1.470         {"mlm": 1.4493262426338005, "mse": 0.0}  0.4858
0         train   2300/102467 14:04/612:44        1.470         {"mlm": 1.45078950502402, "mse": 0.0}  0.4792
0         train   2400/102467 14:40/611:53        1.469         {"mlm": 1.4506313947209142, "mse": 0.0}  0.4784
0         train   2500/102467 15:16/611:04        1.469         {"mlm": 1.4553853992469803, "mse": 0.0}  0.4708
0         train   2600/102467 15:53/610:16        1.468         {"mlm": 1.452441265865638, "mse": 0.0}  0.4859
0         train   2700/102467 16:29/609:29        1.466         {"mlm": 1.447732624066234, "mse": 0.0}  0.4481
0         train   2800/102467 17:06/608:48        1.465         {"mlm": 1.4482911940062597, "mse": 0.0}  0.481
0         train   2900/102467 17:42/608:03        1.464         {"mlm": 1.4458266292451087, "mse": 0.0}  0.4688
0         train   3000/102467 18:19/607:22        1.463         {"mlm": 1.4445456533699303, "mse": 0.0}  0.4722
0         train   3100/102467 18:55/606:40        1.462         {"mlm": 1.443173324118103, "mse": 0.0}  0.4704
0         train   3200/102467 19:32/605:56        1.462         {"mlm": 1.443841747386541, "mse": 0.0}  0.4743
0         train   3300/102467 20:08/605:12        1.461         {"mlm": 1.4444497976971187, "mse": 0.0}  0.4656
0         train   3400/102467 20:44/604:30        1.460         {"mlm": 1.4427597951343691, "mse": 0.0}  0.4534
0         train   3500/102467 21:21/603:47        1.459         {"mlm": 1.4418659660321542, "mse": 0.0}  0.4681
0         train   3600/102467 21:57/603:03        1.459         {"mlm": 1.4412599626446307, "mse": 0.0}  0.4498
0         train   3700/102467 22:33/602:22        1.458         {"mlm": 1.4406513081991792, "mse": 0.0}  0.4478
0         train   3800/102467 23:10/601:40        1.457         {"mlm": 1.440307348817504, "mse": 0.0}  0.4784
0         train   3900/102467 23:46/600:59        1.457         {"mlm": 1.4412948094021967, "mse": 0.0}  0.4474
0         train   4000/102467 24:23/600:17        1.457         {"mlm": 1.4412696785542773, "mse": 0.0}  0.4606
0         train   4100/102467 24:59/599:37        1.456         {"mlm": 1.4404771972675712, "mse": 0.0}  0.7037
0         train   4200/102467 25:35/598:57        1.456         {"mlm": 1.4434222356237547, "mse": 0.0}  0.4759
0         train   4300/102467 26:12/598:16        1.456         {"mlm": 1.4376457707593906, "mse": 0.0}  0.5767
0         train   4400/102467 26:48/597:36        1.455         {"mlm": 1.4333690714896026, "mse": 0.0}  0.4468
0         train   4500/102467 27:25/596:56        1.454         {"mlm": 1.43481425264753, "mse": 0.0}  0.4566
0         train   4600/102467 28:01/596:16        1.454         {"mlm": 1.4356602445294626, "mse": 0.0}  0.4548
0         train   4700/102467 28:38/595:37        1.454         {"mlm": 1.4371954516046026, "mse": 0.0}  0.4542
0         train   4800/102467 29:14/594:59        1.454         {"mlm": 1.4379652837912242, "mse": 0.0}  0.4517
0         train   4900/102467 29:50/594:19        1.453         {"mlm": 1.4362588159092815, "mse": 0.0}  0.4904
0         train   5000/102467 30:27/593:40        1.453         {"mlm": 1.4375554244957849, "mse": 0.0}  0.4443
0         train   5100/102467 31:03/593:00        1.453         {"mlm": 1.4380885391938882, "mse": 0.0}  0.4743
0         train   5200/102467 31:40/592:21        1.453         {"mlm": 1.438758820195827, "mse": 0.0}  0.4488
0         train   5300/102467 32:16/591:42        1.453         {"mlm": 1.4399893713199485, "mse": 0.0}  0.4974
0         train   5400/102467 32:52/591:03        1.453         {"mlm": 1.440320527391543, "mse": 0.0}  0.4768
0         train   5500/102467 33:29/590:24        1.452         {"mlm": 1.439032283063247, "mse": 0.0}  0.4599
0         train   5600/102467 34:05/589:45        1.452         {"mlm": 1.4397146324714523, "mse": 0.0}  0.4701
0         train   5700/102467 34:42/589:09        1.452         {"mlm": 1.4393879296993057, "mse": 0.0}  0.4404
0         train   5800/102467 35:18/588:30        1.452         {"mlm": 1.44131139172728, "mse": 0.0}  0.4586
0         train   5900/102467 35:55/587:52        1.452         {"mlm": 1.4409041825850721, "mse": 0.0}  0.4456
0         train   6000/102467 36:31/587:15        1.451         {"mlm": 1.440660824825814, "mse": 0.0}  0.4563
0         train   6100/102467 37:07/586:36        1.451         {"mlm": 1.4161098980412041, "mse": 0.0}  0.4756
0         train   6200/102467 37:44/585:57        1.451         {"mlm": 1.4338775355803786, "mse": 0.0}  0.4826
0         train   6300/102467 38:20/585:19        1.451         {"mlm": 1.4333366253962019, "mse": 0.0}  0.5572
0         train   6400/102467 38:57/584:42        1.450         {"mlm": 1.435371423097942, "mse": 0.0}  0.446
0         train   6500/102467 39:33/584:02        1.450         {"mlm": 1.4377328824229403, "mse": 0.0}  0.4603
0         train   6600/102467 40:09/583:23        1.451         {"mlm": 1.4412353795577133, "mse": 0.0}  0.4298
0         train   6700/102467 40:46/582:45        1.450         {"mlm": 1.4385054899938137, "mse": 0.0}  0.441
0         train   6800/102467 41:22/582:06        1.449         {"mlm": 1.4338395612751376, "mse": 0.0}  0.4672
0         train   6900/102467 41:59/581:29        1.449         {"mlm": 1.4340907834312455, "mse": 0.0}  0.4606
0         train   7000/102467 42:35/580:51        1.449         {"mlm": 1.434664010224395, "mse": 0.0}  0.4636
0         train   7100/102467 43:11/580:14        1.449         {"mlm": 1.4346941142812466, "mse": 0.0}  0.4705
0         train   7200/102467 43:48/579:37        1.449         {"mlm": 1.435356851259469, "mse": 0.0}  0.4706
0         train   7300/102467 44:24/578:58        1.448         {"mlm": 1.4325596955802318, "mse": 0.0}  0.4751
0         train   7400/102467 45:01/578:20        1.448         {"mlm": 1.4331972153168024, "mse": 0.0}  0.459
0         train   7500/102467 45:37/577:41        1.448         {"mlm": 1.4331553691056225, "mse": 0.0}  0.4614
0         train   7600/102467 46:13/577:04        1.447         {"mlm": 1.4313537104844898, "mse": 0.0}  0.4874
0         train   7700/102467 46:50/576:27        1.447         {"mlm": 1.4310116073490946, "mse": 0.0}  0.4656
0         train   7800/102467 47:26/575:49        1.447         {"mlm": 1.4313710499088435, "mse": 0.0}  0.4752
0         train   7900/102467 48:03/575:12        1.447         {"mlm": 1.4319717328235484, "mse": 0.0}  0.4754
0         train   8000/102467 48:39/574:35        1.446         {"mlm": 1.4305111209511696, "mse": 0.0}  0.634
0         train   8100/102467 49:16/573:59        1.446         {"mlm": 1.4187650854388874, "mse": 0.0}  0.5209
0         train   8200/102467 49:52/573:23        1.446         {"mlm": 1.4291064812212575, "mse": 0.0}  0.459
0         train   8300/102467 50:29/572:47        1.446         {"mlm": 1.4321156828790098, "mse": 0.0}  0.494
0         train   8400/102467 51:05/572:10        1.445         {"mlm": 1.428570207020249, "mse": 0.0}  0.4639
0         train   8500/102467 51:42/571:34        1.445         {"mlm": 1.4223831514677694, "mse": 0.0}  0.4894
0         train   8600/102467 52:18/570:57        1.444         {"mlm": 1.4213889173213268, "mse": 0.0}  0.4657
0         train   8700/102467 52:55/570:21        1.444         {"mlm": 1.4224226007173801, "mse": 0.0}  0.4955
0         train   8800/102467 53:31/569:43        1.444         {"mlm": 1.4209267928372675, "mse": 0.0}  0.47
0         train   8900/102467 54:07/569:05        1.444         {"mlm": 1.4224225307947822, "mse": 0.0}  0.4801
0         train   9000/102467 54:44/568:29        1.444         {"mlm": 1.4246671852098411, "mse": 0.0}  0.4878
0         train   9100/102467 55:20/567:52        1.444         {"mlm": 1.4256310349833357, "mse": 0.0}  0.4889
0         train   9200/102467 55:57/567:15        1.443         {"mlm": 1.4248075105474147, "mse": 0.0}  0.7244
0         train   9300/102467 56:33/566:37        1.443         {"mlm": 1.4245755585531394, "mse": 0.0}  0.4429
0         train   9400/102467 57:10/565:59        1.443         {"mlm": 1.4243331360697404, "mse": 0.0}  0.4794
0         train   9500/102467 57:46/565:22        1.443         {"mlm": 1.423755349999762, "mse": 0.0}  0.4754
0         train   9600/102467 58:22/564:45        1.442         {"mlm": 1.423147776633277, "mse": 0.0}  0.4645
0         train   9700/102467 58:59/564:08        1.442         {"mlm": 1.4223751667184088, "mse": 0.0}  0.4898
0         train   9800/102467 59:35/563:30        1.442         {"mlm": 1.4242246355073223, "mse": 0.0}  0.4995
0         train   9900/102467 60:12/562:53        1.442         {"mlm": 1.4243892307082813, "mse": 0.0}  0.5002
0         train   10000/102467 60:48/562:16       1.442         {"mlm": 1.4244985343041066, "mse": 0.0}  0.4761

09/23/2022 09:04:53 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        0:44/579:16        1.277           None
0         valid   101/781      1:00/ 6:44         1.318           None
0         valid   201/781      1:15/ 3:38         1.337           None
0         valid   301/781      1:31/ 2:25         1.326           None
0         valid   401/781      1:46/ 1:40         1.318           None
0         valid   501/781      2:02/ 1:08         1.320           None
0         valid   601/781      2:17/ 0:41         1.319           None
0         valid   701/781      2:33/ 0:17         1.319           None
0         valid   781/781      2:53/ 0:00         1.313         {"mlm": 1.3125800256081945, "mse": 0.0, "train": 0.0}  None
0         train   10100/102467 64:18/588:04       1.441         {"mlm": 1.3851612401008606, "mse": 0.0}  0.4913
0         train   10200/102467 64:54/587:06       1.441         {"mlm": 1.3978017109632492, "mse": 0.0}  0.4866
0         train   10300/102467 65:30/586:08       1.441         {"mlm": 1.4122860952218375, "mse": 0.0}  0.4653
0         train   10400/102467 66:06/585:11       1.441         {"mlm": 1.411920428276062, "mse": 0.0}  0.5996
0         train   10500/102467 66:42/584:15       1.440         {"mlm": 1.4050399527549744, "mse": 0.0}  0.5008
0         train   10600/102467 67:18/583:20       1.440         {"mlm": 1.4046478007237118, "mse": 0.0}  0.5321
0         train   10700/102467 67:54/582:25       1.440         {"mlm": 1.4073806388037546, "mse": 0.0}  0.4926
0         train   10800/102467 68:30/581:30       1.439         {"mlm": 1.4095971240103244, "mse": 0.0}  0.501
0         train   10900/102467 69:07/580:38       1.439         {"mlm": 1.4100696162382762, "mse": 0.0}  0.7681
0         train   11000/102467 69:43/579:46       1.439         {"mlm": 1.4103035361766816, "mse": 0.0}  0.4751
0         train   11100/102467 70:19/578:53       1.439         {"mlm": 1.4084472822059284, "mse": 0.0}  0.4581
0         train   11200/102467 70:56/578:02       1.438         {"mlm": 1.4096357867121696, "mse": 0.0}  0.5783
0         train   11300/102467 71:32/577:11       1.438         {"mlm": 1.409426022538772, "mse": 0.0}  0.4997
0         train   11400/102467 72:08/576:21       1.438         {"mlm": 1.4093074191042354, "mse": 0.0}  0.4915
0         train   11500/102467 72:45/575:30       1.438         {"mlm": 1.409282542904218, "mse": 0.0}  0.5006
0         train   11600/102467 73:21/574:40       1.437         {"mlm": 1.408945632763207, "mse": 0.0}  0.4913
0         train   11700/102467 73:58/573:50       1.437         {"mlm": 1.4097571525503607, "mse": 0.0}  0.5112
0         train   11800/102467 74:34/573:01       1.437         {"mlm": 1.4082827398843236, "mse": 0.0}  0.5199
0         train   11900/102467 75:11/572:12       1.436         {"mlm": 1.4069473268797523, "mse": 0.0}  0.5062
0         train   12000/102467 75:47/571:23       1.436         {"mlm": 1.4069144153892994, "mse": 0.0}  0.5039
0         train   12100/102467 76:23/570:34       1.436         {"mlm": 1.4159706445655438, "mse": 0.0}  0.51
0         train   12200/102467 77:00/569:46       1.436         {"mlm": 1.41312077955984, "mse": 0.0}  0.487
0         train   12300/102467 77:36/568:57       1.435         {"mlm": 1.4042836032982255, "mse": 0.0}  0.5166
0         train   12400/102467 78:13/568:09       1.435         {"mlm": 1.4009811970823092, "mse": 0.0}  0.5225
0         train   12500/102467 78:49/567:20       1.435         {"mlm": 1.4063727645453565, "mse": 0.0}  0.4818
0         train   12600/102467 79:25/566:32       1.435         {"mlm": 1.4050962618475964, "mse": 0.0}  0.5601
0         train   12700/102467 80:02/565:45       1.434         {"mlm": 1.405699758741136, "mse": 0.0}  0.5198
0         train   12800/102467 80:38/564:58       1.434         {"mlm": 1.404994280526277, "mse": 0.0}  0.7204
0         train   12900/102467 81:15/564:11       1.434         {"mlm": 1.4057627309018432, "mse": 0.0}  0.5116
0         train   13000/102467 81:51/563:24       1.434         {"mlm": 1.4072482894968104, "mse": 0.0}  0.5364
0         train   13100/102467 82:28/562:37       1.434         {"mlm": 1.4090437070145403, "mse": 0.0}  0.5349
0         train   13200/102467 83:04/561:51       1.434         {"mlm": 1.4093180440981454, "mse": 0.0}  0.5128
0         train   13300/102467 83:41/561:04       1.434         {"mlm": 1.4107112962067172, "mse": 0.0}  0.5642
0         train   13400/102467 84:17/560:18       1.433         {"mlm": 1.4099050107131779, "mse": 0.0}  0.5496
0         train   13500/102467 84:54/559:31       1.433         {"mlm": 1.4099235574828537, "mse": 0.0}  0.509
0         train   13600/102467 85:30/558:45       1.433         {"mlm": 1.410392774836282, "mse": 0.0}  0.5255
0         train   13700/102467 86:07/557:59       1.433         {"mlm": 1.4132944732430825, "mse": 0.0}  0.5163
0         train   13800/102467 86:43/557:13       1.433         {"mlm": 1.4144468054034565, "mse": 0.0}  0.4847
0         train   13900/102467 87:20/556:28       1.433         {"mlm": 1.4149281312880735, "mse": 0.0}  0.5108
0         train   14000/102467 87:56/555:42       1.433         {"mlm": 1.415344419242025, "mse": 0.0}  0.5502
0         train   14100/102467 88:32/554:57       1.433         {"mlm": 1.4264939463868433, "mse": 0.0}  0.5099
0         train   14200/102467 89:09/554:12       1.433         {"mlm": 1.4092871865840872, "mse": 0.0}  0.5449
0         train   14300/102467 89:46/553:27       1.433         {"mlm": 1.4165821415465951, "mse": 0.0}  0.5051
0         train   14400/102467 90:22/552:41       1.433         {"mlm": 1.41326045540709, "mse": 0.0}  0.5203
0         train   14500/102467 90:58/551:56       1.432         {"mlm": 1.4097152665915738, "mse": 0.0}  0.5291
0         train   14600/102467 91:35/551:11       1.432         {"mlm": 1.4104402625441153, "mse": 0.0}  0.5162
0         train   14700/102467 92:11/550:26       1.432         {"mlm": 1.4119820178067446, "mse": 0.0}  0.4833
0         train   14800/102467 92:48/549:42       1.432         {"mlm": 1.409447936859346, "mse": 0.0}  0.5453
0         train   14900/102467 93:24/548:58       1.431         {"mlm": 1.405607841129027, "mse": 0.0}  0.5017
0         train   15000/102467 94:01/548:13       1.431         {"mlm": 1.4049237632679796, "mse": 0.0}  0.4847
0         train   15100/102467 94:37/547:30       1.431         {"mlm": 1.404962583263498, "mse": 0.0}  0.5179
0         train   15200/102467 95:14/546:46       1.431         {"mlm": 1.404250109882307, "mse": 0.0}  0.5395
0         train   15300/102467 95:50/546:02       1.431         {"mlm": 1.4058243680073779, "mse": 0.0}  0.5182
0         train   15400/102467 96:27/545:18       1.431         {"mlm": 1.4047473472501757, "mse": 0.0}  0.5603
0         train   15500/102467 97:03/544:34       1.430         {"mlm": 1.4040035870428555, "mse": 0.0}  0.5629
0         train   15600/102467 97:40/543:51       1.430         {"mlm": 1.4035589187703235, "mse": 0.0}  0.5534
0         train   15700/102467 98:16/543:07       1.430         {"mlm": 1.4043100978235754, "mse": 0.0}  0.5295
0         train   15800/102467 98:53/542:24       1.430         {"mlm": 1.4049630875184353, "mse": 0.0}  0.499
0         train   15900/102467 99:29/541:40       1.430         {"mlm": 1.4064615390825825, "mse": 0.0}  0.5083
0         train   16000/102467 100:05/540:56      1.430         {"mlm": 1.404212884806298, "mse": 0.0}  0.5238
0         train   16100/102467 100:51/541:04      1.429         {"mlm": 1.389010859518936, "mse": 0.0}  0.5081
0         train   16200/102467 101:28/540:19      1.429         {"mlm": 1.3836996083332198, "mse": 0.0}  0.5201
0         train   16300/102467 102:04/539:35      1.429         {"mlm": 1.3935242698650168, "mse": 0.0}  0.5194
0         train   16400/102467 102:40/538:51      1.429         {"mlm": 1.3876145767324817, "mse": 0.0}  0.5503
0         train   16500/102467 103:17/538:08      1.428         {"mlm": 1.3855765279629821, "mse": 0.0}  0.5309
0         train   16600/102467 103:53/537:24      1.428         {"mlm": 1.3900081299657199, "mse": 0.0}  0.5462
0         train   16700/102467 104:29/536:40      1.428         {"mlm": 1.38919550077816, "mse": 0.0}  0.5016
0         train   16800/102467 105:06/535:57      1.428         {"mlm": 1.3889343261419604, "mse": 0.0}  0.5372
0         train   16900/102467 105:42/535:14      1.427         {"mlm": 1.3884928774009713, "mse": 0.0}  0.5298
0         train   17000/102467 106:19/534:31      1.427         {"mlm": 1.3891438724045289, "mse": 0.0}  0.5219
0         train   17100/102467 106:55/533:48      1.427         {"mlm": 1.3902328031760731, "mse": 0.0}  0.5397
0         train   17200/102467 107:32/533:05      1.427         {"mlm": 1.3898912726488328, "mse": 0.0}  0.5329
0         train   17300/102467 108:08/532:22      1.426         {"mlm": 1.3891057765015107, "mse": 0.0}  0.5281
0         train   17400/102467 108:44/531:39      1.426         {"mlm": 1.3882509214415581, "mse": 0.0}  0.5344
0         train   17500/102467 109:21/530:57      1.426         {"mlm": 1.3889258328723206, "mse": 0.0}  0.5356
0         train   17600/102467 109:57/530:14      1.426         {"mlm": 1.3874373443811927, "mse": 0.0}  0.5785
0         train   17700/102467 110:34/529:32      1.426         {"mlm": 1.387746814179575, "mse": 0.0}  0.6885
0         train   17800/102467 111:10/528:49      1.426         {"mlm": 1.3898130676848528, "mse": 0.0}  0.5085
0         train   17900/102467 111:47/528:08      1.425         {"mlm": 1.3910697111972834, "mse": 0.0}  0.5676
0         train   18000/102467 112:23/527:26      1.425         {"mlm": 1.3915455853216756, "mse": 0.0}  0.5241
0         train   18100/102467 113:00/526:44      1.425         {"mlm": 1.3759518495450418, "mse": 0.0}  0.5284
0         train   18200/102467 113:36/526:01      1.425         {"mlm": 1.386338116258991, "mse": 0.0}  0.5655
0         train   18300/102467 114:12/525:18      1.425         {"mlm": 1.3892050480117668, "mse": 0.0}  0.5409
0         train   18400/102467 114:49/524:36      1.424         {"mlm": 1.3854755410040267, "mse": 0.0}  0.5648
0         train   18500/102467 115:25/523:54      1.424         {"mlm": 1.3910585427957196, "mse": 0.0}  0.5354
0         train   18600/102467 116:02/523:12      1.424         {"mlm": 1.3978409798153295, "mse": 0.0}  0.532
0         train   18700/102467 116:38/522:31      1.424         {"mlm": 1.4015608375099884, "mse": 0.0}  0.5394
0         train   18800/102467 117:15/521:49      1.424         {"mlm": 1.403648925336761, "mse": 0.0}  0.5235
0         train   18900/102467 117:51/521:07      1.424         {"mlm": 1.4067110711974757, "mse": 0.0}  0.8675
0         train   19000/102467 118:28/520:25      1.424         {"mlm": 1.4047572879666783, "mse": 0.0}  0.6897
0         train   19100/102467 119:04/519:44      1.424         {"mlm": 1.4046947839703874, "mse": 0.0}  0.5946
0         train   19200/102467 119:41/519:03      1.424         {"mlm": 1.4044292989781868, "mse": 0.0}  0.5623
0         train   19300/102467 120:17/518:21      1.424         {"mlm": 1.4048628525601492, "mse": 0.0}  0.5538
0         train   19400/102467 120:53/517:39      1.424         {"mlm": 1.4037527902249278, "mse": 0.0}  0.5727
0         train   19500/102467 121:30/516:58      1.424         {"mlm": 1.4037499313845354, "mse": 0.0}  0.5401
0         train   19600/102467 122:06/516:17      1.424         {"mlm": 1.404550281308946, "mse": 0.0}  0.5487
0         train   19700/102467 122:43/515:36      1.423         {"mlm": 1.4036194426974036, "mse": 0.0}  0.5396
0         train   19800/102467 123:19/514:55      1.423         {"mlm": 1.403207991927663, "mse": 0.0}  0.5424
0         train   19900/102467 123:56/514:14      1.423         {"mlm": 1.4032106143638554, "mse": 0.0}  0.553
0         train   20000/102467 124:32/513:33      1.423         {"mlm": 1.4025581126700422, "mse": 0.0}  0.5397

09/23/2022 10:08:37 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        0:59/779:58        1.565           None
0         valid   101/781      1:15/ 8:27         1.290           None
0         valid   201/781      1:30/ 4:22         1.306           None
0         valid   301/781      1:46/ 2:49         1.293           None
0         valid   401/781      2:01/ 1:55         1.291           None
0         valid   501/781      2:17/ 1:16         1.294           None
0         valid   601/781      2:32/ 0:45         1.293           None
0         valid   701/781      2:48/ 0:19         1.294           None
0         valid   781/781      3:03/ 0:00         1.289         {"mlm": 1.2888924455825865, "mse": 0.0, "train": 0.0}  None
0         train   20100/102467 128:12/525:22      1.423         {"mlm": 1.4000158673524856, "mse": 0.0}  0.538
0         train   20200/102467 128:48/524:34      1.423         {"mlm": 1.390490479171276, "mse": 0.0}  0.5879
0         train   20300/102467 129:24/523:47      1.423         {"mlm": 1.3920333999395371, "mse": 0.0}  0.6183
0         train   20400/102467 130:00/523:00      1.422         {"mlm": 1.3890891321003438, "mse": 0.0}  0.5459
0         train   20500/102467 130:36/522:13      1.422         {"mlm": 1.3840400198698044, "mse": 0.0}  0.5728
0         train   20600/102467 131:12/521:27      1.422         {"mlm": 1.388499711851279, "mse": 0.0}  0.5914
0         train   20700/102467 131:48/520:40      1.422         {"mlm": 1.3840867455516543, "mse": 0.0}  0.5611
0         train   20800/102467 132:24/519:53      1.422         {"mlm": 1.3840573323518037, "mse": 0.0}  0.5387
0         train   20900/102467 133:01/519:07      1.421         {"mlm": 1.3843089052703645, "mse": 0.0}  0.5492
0         train   21000/102467 133:37/518:22      1.421         {"mlm": 1.3847944930791856, "mse": 0.0}  0.5713
0         train   21100/102467 134:13/517:37      1.421         {"mlm": 1.384184345332059, "mse": 0.0}  0.588
0         train   21200/102467 134:49/516:51      1.421         {"mlm": 1.3834424250324566, "mse": 0.0}  0.5892
0         train   21300/102467 135:26/516:06      1.421         {"mlm": 1.3851854266570165, "mse": 0.0}  0.6264
0         train   21400/102467 136:02/515:21      1.420         {"mlm": 1.383825304550784, "mse": 0.0}  0.5502
0         train   21500/102467 136:39/514:37      1.420         {"mlm": 1.3842885398467382, "mse": 0.0}  0.5922
0         train   21600/102467 137:15/513:52      1.420         {"mlm": 1.3846662556752563, "mse": 0.0}  0.5281
0         train   21700/102467 137:51/513:08      1.420         {"mlm": 1.385843443274498, "mse": 0.0}  0.6056
0         train   21800/102467 138:28/512:23      1.420         {"mlm": 1.3854453522960346, "mse": 0.0}  0.5901
0         train   21900/102467 139:04/511:39      1.420         {"mlm": 1.384919529557228, "mse": 0.0}  0.5688
0         train   22000/102467 139:41/510:55      1.420         {"mlm": 1.3860493548214436, "mse": 0.0}  0.5282
0         train   22100/102467 140:17/510:11      1.420         {"mlm": 1.379430469840464, "mse": 0.0}  0.5562
0         train   22200/102467 140:54/509:27      1.419         {"mlm": 1.3703721105752877, "mse": 0.0}  0.5259
0         train   22300/102467 141:30/508:43      1.419         {"mlm": 1.3746180057924329, "mse": 0.0}  0.5307
0         train   22400/102467 142:07/507:59      1.419         {"mlm": 1.3792683565825747, "mse": 0.0}  0.552
0         train   22500/102467 142:43/507:15      1.419         {"mlm": 1.3753274740102535, "mse": 0.0}  0.516
0         train   22600/102467 143:19/506:31      1.419         {"mlm": 1.3783156355354742, "mse": 0.0}  0.5482
0         train   22700/102467 143:56/505:48      1.418         {"mlm": 1.3796017645766294, "mse": 0.0}  0.5259
0         train   22800/102467 144:32/505:04      1.418         {"mlm": 1.3792126876242616, "mse": 0.0}  0.593
0         train   22900/102467 145:09/504:21      1.418         {"mlm": 1.3792714208066132, "mse": 0.0}  0.5316
0         train   23000/102467 145:46/503:38      1.418         {"mlm": 1.377828060626029, "mse": 0.0}  0.56
0         train   23100/102467 146:22/502:54      1.418         {"mlm": 1.379356317622104, "mse": 0.0}  0.5403
0         train   23200/102467 146:58/502:11      1.418         {"mlm": 1.3799425895458663, "mse": 0.0}  0.5385
0         train   23300/102467 147:35/501:28      1.418         {"mlm": 1.3814234041176547, "mse": 0.0}  0.5617
0         train   23400/102467 148:11/500:44      1.417         {"mlm": 1.3805964169884, "mse": 0.0}  0.5244
0         train   23500/102467 148:48/500:01      1.417         {"mlm": 1.3809883337246727, "mse": 0.0}  0.5647
0         train   23600/102467 149:24/499:18      1.417         {"mlm": 1.3797726953230924, "mse": 0.0}  0.5233
0         train   23700/102467 150:01/498:35      1.417         {"mlm": 1.380379729056232, "mse": 0.0}  0.5499
0         train   23800/102467 150:37/497:52      1.417         {"mlm": 1.3805049735350765, "mse": 0.0}  0.5826
0         train   23900/102467 151:14/497:10      1.417         {"mlm": 1.3803380352374817, "mse": 0.0}  0.5614
0         train   24000/102467 151:50/496:27      1.416         {"mlm": 1.380274112550183, "mse": 0.0}  0.6228
0         train   24100/102467 152:27/495:44      1.416         {"mlm": 1.3526317975959, "mse": 0.0}  0.5575
0         train   24200/102467 153:03/495:01      1.416         {"mlm": 1.3747626124608396, "mse": 0.0}  0.5528
0         train   24300/102467 153:40/494:19      1.416         {"mlm": 1.3826067261647859, "mse": 0.0}  0.5332
0         train   24400/102467 154:16/493:36      1.416         {"mlm": 1.38192709411808, "mse": 0.0}  0.5428
0         train   24500/102467 154:53/492:54      1.416         {"mlm": 1.3802243273660362, "mse": 0.0}  0.6549
0         train   24600/102467 155:29/492:12      1.416         {"mlm": 1.3866973480452662, "mse": 0.0}  0.5603
0         train   24700/102467 156:06/491:29      1.416         {"mlm": 1.3897694749784333, "mse": 0.0}  0.5992
0         train   24800/102467 156:42/490:47      1.415         {"mlm": 1.3879236452710957, "mse": 0.0}  0.5428
0         train   24900/102467 157:19/490:04      1.415         {"mlm": 1.3855542047384852, "mse": 0.0}  0.5465
0         train   25000/102467 157:55/489:22      1.415         {"mlm": 1.3867275381613828, "mse": 0.0}  0.5361
0         train   25100/102467 158:32/488:40      1.415         {"mlm": 1.3863508453786046, "mse": 0.0}  2.6115
0         train   25200/102467 159:08/487:57      1.415         {"mlm": 1.3859605329462603, "mse": 0.0}  0.5575
0         train   25300/102467 159:45/487:15      1.415         {"mlm": 1.3867762185667623, "mse": 0.0}  0.5425
0         train   25400/102467 160:21/486:33      1.415         {"mlm": 1.3892536969570302, "mse": 0.0}  0.6597
0         train   25500/102467 160:58/485:51      1.415         {"mlm": 1.3897013908155134, "mse": 0.0}  0.5417
0         train   25600/102467 161:34/485:10      1.415         {"mlm": 1.3895635752937523, "mse": 0.0}  0.5386
0         train   25700/102467 162:11/484:28      1.415         {"mlm": 1.388921997712835, "mse": 0.0}  0.5439
0         train   25800/102467 162:47/483:46      1.414         {"mlm": 1.389114605273235, "mse": 0.0}  0.568
0         train   25900/102467 163:24/483:04      1.414         {"mlm": 1.3905945589846378, "mse": 0.0}  0.566
0         train   26000/102467 164:00/482:22      1.414         {"mlm": 1.3891357524139625, "mse": 0.0}  0.7899
0         train   26100/102467 164:37/481:40      1.414         {"mlm": 1.3865564938673038, "mse": 0.0}  0.6038
0         train   26200/102467 165:13/480:58      1.414         {"mlm": 1.3811010321989883, "mse": 0.0}  0.5271
0         train   26300/102467 165:50/480:17      1.414         {"mlm": 1.3850226356124236, "mse": 0.0}  1.0469
0         train   26400/102467 166:26/479:35      1.414         {"mlm": 1.3919374376760623, "mse": 0.0}  0.5663
0         train   26500/102467 167:03/478:54      1.414         {"mlm": 1.390934885747476, "mse": 0.0}  0.5239
0         train   26600/102467 167:39/478:12      1.414         {"mlm": 1.3939084119133813, "mse": 0.0}  0.5267
0         train   26700/102467 168:16/477:30      1.414         {"mlm": 1.3968287849186825, "mse": 0.0}  0.5745
0         train   26800/102467 168:53/476:49      1.414         {"mlm": 1.3937901136567032, "mse": 0.0}  0.5372
0         train   26900/102467 169:29/476:07      1.414         {"mlm": 1.3922839116890693, "mse": 0.0}  0.5448
0         train   27000/102467 170:05/475:26      1.413         {"mlm": 1.3898093014088653, "mse": 0.0}  0.5887
0         train   27100/102467 170:42/474:45      1.413         {"mlm": 1.3912428720712444, "mse": 0.0}  0.5382
0         train   27200/102467 171:19/474:03      1.413         {"mlm": 1.3911275028086945, "mse": 0.0}  0.5817
0         train   27300/102467 171:55/473:22      1.413         {"mlm": 1.38897585133543, "mse": 0.0}  0.5162
0         train   27400/102467 172:31/472:40      1.413         {"mlm": 1.3883279770207388, "mse": 0.0}  0.5361
0         train   27500/102467 173:08/471:59      1.413         {"mlm": 1.3897085009054415, "mse": 0.0}  0.5498
0         train   27600/102467 173:44/471:18      1.413         {"mlm": 1.3901358857032426, "mse": 0.0}  1.2075
0         train   27700/102467 174:21/470:37      1.413         {"mlm": 1.3900545324096274, "mse": 0.0}  0.5449
0         train   27800/102467 174:57/469:55      1.413         {"mlm": 1.3899384823254102, "mse": 0.0}  0.531
0         train   27900/102467 175:34/469:14      1.413         {"mlm": 1.3885628005109716, "mse": 0.0}  0.5248
0         train   28000/102467 176:10/468:33      1.412         {"mlm": 1.3884299379858782, "mse": 0.0}  0.5317
0         train   28100/102467 176:47/467:52      1.412         {"mlm": 1.3570605559895437, "mse": 0.0}  1.1401
0         train   28200/102467 177:23/467:11      1.412         {"mlm": 1.3620395052189729, "mse": 0.0}  0.5423
0         train   28300/102467 178:00/466:30      1.412         {"mlm": 1.3728948303976574, "mse": 0.0}  1.7572
0         train   28400/102467 178:36/465:49      1.412         {"mlm": 1.378004098480398, "mse": 0.0}  0.6283
0         train   28500/102467 179:13/465:08      1.412         {"mlm": 1.3859470884405798, "mse": 0.0}  0.5867
0         train   28600/102467 179:49/464:27      1.412         {"mlm": 1.3925178350018175, "mse": 0.0}  0.5291
0         train   28700/102467 180:26/463:46      1.412         {"mlm": 1.3860597007576076, "mse": 0.0}  0.5309
0         train   28800/102467 181:02/463:05      1.412         {"mlm": 1.3864743138977031, "mse": 0.0}  0.6335
0         train   28900/102467 181:39/462:24      1.412         {"mlm": 1.384039664907115, "mse": 0.0}  0.5462
0         train   29000/102467 182:15/461:43      1.411         {"mlm": 1.385242137444546, "mse": 0.0}  0.52
0         train   29100/102467 182:52/461:03      1.411         {"mlm": 1.3848238212131236, "mse": 0.0}  0.6196
0         train   29200/102467 183:28/460:22      1.411         {"mlm": 1.383031159689195, "mse": 0.0}  0.5241
0         train   29300/102467 184:05/459:41      1.411         {"mlm": 1.3826278912156453, "mse": 0.0}  0.5132
0         train   29400/102467 184:41/459:00      1.411         {"mlm": 1.382501457174733, "mse": 0.0}  0.5398
0         train   29500/102467 185:18/458:20      1.411         {"mlm": 1.3828695794159078, "mse": 0.0}  0.5233
0         train   29600/102467 185:54/457:39      1.411         {"mlm": 1.3848165806224173, "mse": 0.0}  0.6135
0         train   29700/102467 186:31/456:59      1.411         {"mlm": 1.3849153262123746, "mse": 0.0}  0.5368
0         train   29800/102467 187:07/456:18      1.411         {"mlm": 1.3840901164674015, "mse": 0.0}  0.8415
0         train   29900/102467 187:44/455:38      1.411         {"mlm": 1.3829887764614845, "mse": 0.0}  0.5471
0         train   30000/102467 188:20/454:57      1.411         {"mlm": 1.3842059546458219, "mse": 0.0}  0.7628

09/23/2022 11:12:24 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        0:36/478:43        1.197           None
0         valid   101/781      0:52/ 5:51         1.269           None
0         valid   201/781      1:07/ 3:15         1.292           None
0         valid   301/781      1:23/ 2:12         1.293           None
0         valid   401/781      1:38/ 1:33         1.288           None
0         valid   501/781      1:54/ 1:04         1.293           None
0         valid   601/781      2:10/ 0:38         1.290           None
0         valid   701/781      2:25/ 0:16         1.293           None
0         valid   781/781      2:40/ 0:00         1.289         {"mlm": 1.2888924455825865, "mse": 0.0, "train": 0.0}  None
0         train   30100/102467 191:37/460:42      1.410         {"mlm": 1.3992653620243072, "mse": 0.0}  0.5283
0         train   30200/102467 192:13/459:58      1.410         {"mlm": 1.3950517570972443, "mse": 0.0}  0.5178
0         train   30300/102467 192:49/459:15      1.410         {"mlm": 1.390825763543447, "mse": 0.0}  0.5003
0         train   30400/102467 193:25/458:32      1.410         {"mlm": 1.390906040519476, "mse": 0.0}  0.5347
0         train   30500/102467 194:01/457:49      1.410         {"mlm": 1.387408287167549, "mse": 0.0}  0.5229
0         train   30600/102467 194:37/457:06      1.410         {"mlm": 1.3855047873655955, "mse": 0.0}  0.5093
0         train   30700/102467 195:14/456:23      1.410         {"mlm": 1.3880355654444014, "mse": 0.0}  0.5224
0         train   30800/102467 195:50/455:41      1.410         {"mlm": 1.3880433781445027, "mse": 0.0}  0.5649
0         train   30900/102467 196:26/454:59      1.410         {"mlm": 1.3890908243921067, "mse": 0.0}  0.5209
0         train   31000/102467 197:03/454:16      1.410         {"mlm": 1.3875278494358063, "mse": 0.0}  0.5135
0         train   31100/102467 197:39/453:34      1.410         {"mlm": 1.3867721449245105, "mse": 0.0}  0.5241
0         train   31200/102467 198:15/452:52      1.410         {"mlm": 1.3861245269576707, "mse": 0.0}  0.5399
0         train   31300/102467 198:52/452:10      1.409         {"mlm": 1.3856417166728239, "mse": 0.0}  0.5039
0         train   31400/102467 199:28/451:28      1.409         {"mlm": 1.3857313899057253, "mse": 0.0}  0.4935
0         train   31500/102467 200:05/450:46      1.409         {"mlm": 1.384516363898913, "mse": 0.0}  0.526
0         train   31600/102467 200:41/450:05      1.409         {"mlm": 1.3849821615591644, "mse": 0.0}  0.5032
0         train   31700/102467 201:18/449:23      1.409         {"mlm": 1.384242146891706, "mse": 0.0}  0.5155
0         train   31800/102467 201:54/448:41      1.409         {"mlm": 1.383994456364049, "mse": 0.0}  0.5299
0         train   31900/102467 202:30/447:59      1.409         {"mlm": 1.3831065758592205, "mse": 0.0}  0.4832
0         train   32000/102467 203:07/447:17      1.409         {"mlm": 1.3815142832398415, "mse": 0.0}  0.5241
0         train   32100/102467 203:43/446:36      1.409         {"mlm": 1.4101369043793341, "mse": 0.0}  0.5085
0         train   32200/102467 204:20/445:54      1.409         {"mlm": 1.38913212918756, "mse": 0.0}  0.4873
0         train   32300/102467 204:56/445:12      1.408         {"mlm": 1.3852984675994287, "mse": 0.0}  0.4914
0         train   32400/102467 205:33/444:30      1.408         {"mlm": 1.3850361307462056, "mse": 0.0}  0.4811
0         train   32500/102467 206:09/443:49      1.408         {"mlm": 1.3861706224615444, "mse": 0.0}  0.5257
0         train   32600/102467 206:45/443:08      1.408         {"mlm": 1.3853184903205336, "mse": 0.0}  0.48
0         train   32700/102467 207:22/442:26      1.408         {"mlm": 1.3861463067357631, "mse": 0.0}  0.4644
0         train   32800/102467 207:58/441:45      1.408         {"mlm": 1.385119506653319, "mse": 0.0}  0.5051
0         train   32900/102467 208:35/441:03      1.408         {"mlm": 1.3845451866028968, "mse": 0.0}  0.4974
0         train   33000/102467 209:11/440:22      1.408         {"mlm": 1.385072619588048, "mse": 0.0}  0.5138
0         train   33100/102467 209:48/439:41      1.408         {"mlm": 1.3851498591021258, "mse": 0.0}  0.4812
0         train   33200/102467 210:24/438:59      1.408         {"mlm": 1.3832039490056296, "mse": 0.0}  0.5
0         train   33300/102467 211:01/438:18      1.408         {"mlm": 1.380305508176027, "mse": 0.0}  0.4925
0         train   33400/102467 211:37/437:37      1.408         {"mlm": 1.3803075373470997, "mse": 0.0}  0.4838
0         train   33500/102467 212:14/436:56      1.407         {"mlm": 1.3808531494360117, "mse": 0.0}  0.4813
0         train   33600/102467 212:50/436:14      1.407         {"mlm": 1.378773483468116, "mse": 0.0}  0.4981
0         train   33700/102467 213:27/435:33      1.407         {"mlm": 1.378769541327009, "mse": 0.0}  0.4689
0         train   33800/102467 214:03/434:52      1.407         {"mlm": 1.3794322588696886, "mse": 0.0}  0.4867
0         train   33900/102467 214:40/434:11      1.407         {"mlm": 1.381073543779344, "mse": 0.0}  0.8549
0         train   34000/102467 215:16/433:30      1.407         {"mlm": 1.380952021937301, "mse": 0.0}  0.4833
0         train   34100/102467 215:53/432:49      1.407         {"mlm": 1.3711252017897002, "mse": 0.0}  0.5194
0         train   34200/102467 216:29/432:08      1.407         {"mlm": 1.3756471047497758, "mse": 0.0}  0.4448
0         train   34300/102467 217:06/431:27      1.407         {"mlm": 1.3839684392781866, "mse": 0.0}  0.4661
0         train   34400/102467 217:42/430:46      1.407         {"mlm": 1.3892101356432067, "mse": 0.0}  0.486
0         train   34500/102467 218:19/430:05      1.407         {"mlm": 1.3928502192458954, "mse": 0.0}  0.5152
0         train   34600/102467 218:55/429:24      1.407         {"mlm": 1.3868667046760637, "mse": 0.0}  0.5319
0         train   34700/102467 219:31/428:43      1.407         {"mlm": 1.3816644212919524, "mse": 0.0}  0.4956
0         train   34800/102467 220:08/428:02      1.406         {"mlm": 1.3778905647440363, "mse": 0.0}  0.5538
0         train   34900/102467 220:44/427:21      1.406         {"mlm": 1.3771022138059272, "mse": 0.0}  0.4863
0         train   35000/102467 221:21/426:40      1.406         {"mlm": 1.377917577663739, "mse": 0.0}  0.4775
0         train   35100/102467 221:57/426:00      1.406         {"mlm": 1.3796191458167926, "mse": 0.0}  0.5115
0         train   35200/102467 222:33/425:19      1.406         {"mlm": 1.3794917283153694, "mse": 0.0}  0.5479
0         train   35300/102467 223:10/424:38      1.406         {"mlm": 1.3794851368307517, "mse": 0.0}  0.7068
0         train   35400/102467 223:46/423:57      1.406         {"mlm": 1.3774187698725808, "mse": 0.0}  0.499
0         train   35500/102467 224:23/423:17      1.406         {"mlm": 1.379221635284984, "mse": 0.0}  0.6816
0         train   35600/102467 224:59/422:36      1.406         {"mlm": 1.3780393935413624, "mse": 0.0}  0.4589
0         train   35700/102467 225:36/421:55      1.406         {"mlm": 1.3779756506143666, "mse": 0.0}  0.4858
0         train   35800/102467 226:12/421:15      1.406         {"mlm": 1.3794624337696526, "mse": 0.0}  0.4724
0         train   35900/102467 226:49/420:34      1.406         {"mlm": 1.3798371384781958, "mse": 0.0}  0.5707
0         train   36000/102467 227:25/419:53      1.406         {"mlm": 1.3811985089912548, "mse": 0.0}  0.4865
0         train   36100/102467 228:02/419:13      1.406         {"mlm": 1.3797271448312347, "mse": 0.0}  0.7213
0         train   36200/102467 228:38/418:32      1.405         {"mlm": 1.369729940661319, "mse": 0.0}  0.4785
0         train   36300/102467 229:15/417:52      1.405         {"mlm": 1.3781069673673072, "mse": 0.0}  0.4745
0         train   36400/102467 229:51/417:11      1.405         {"mlm": 1.381447900752877, "mse": 0.0}  0.4701
0         train   36500/102467 230:27/416:31      1.405         {"mlm": 1.374876862080764, "mse": 0.0}  0.569
0         train   36600/102467 231:04/415:51      1.405         {"mlm": 1.3777503906382589, "mse": 0.0}  0.4732
0         train   36700/102467 231:40/415:10      1.405         {"mlm": 1.3796374287290587, "mse": 0.0}  0.5487
0         train   36800/102467 232:17/414:30      1.405         {"mlm": 1.3794962003957971, "mse": 0.0}  0.4826
0         train   36900/102467 232:53/413:49      1.405         {"mlm": 1.3751176844871165, "mse": 0.0}  0.4878
0         train   37000/102467 233:30/413:09      1.405         {"mlm": 1.3742699843710857, "mse": 0.0}  0.4907
0         train   37100/102467 234:06/412:29      1.405         {"mlm": 1.3742348032314993, "mse": 0.0}  0.4884
0         train   37200/102467 234:43/411:48      1.405         {"mlm": 1.3760761455286514, "mse": 0.0}  0.5399
0         train   37300/102467 235:19/411:08      1.405         {"mlm": 1.3744907045970995, "mse": 0.0}  0.4531
0         train   37400/102467 235:56/410:28      1.405         {"mlm": 1.3760333732357175, "mse": 0.0}  0.5372
0         train   37500/102467 236:32/409:47      1.404         {"mlm": 1.3754453604509613, "mse": 0.0}  0.6236
0         train   37600/102467 237:08/409:07      1.404         {"mlm": 1.3768455854190762, "mse": 0.0}  0.4527
0         train   37700/102467 237:45/408:27      1.404         {"mlm": 1.3763126714059586, "mse": 0.0}  0.4418
0         train   37800/102467 238:21/407:46      1.404         {"mlm": 1.376470798708428, "mse": 0.0}  0.4285
0         train   37900/102467 238:58/407:06      1.404         {"mlm": 1.3777576405937948, "mse": 0.0}  0.4694
0         train   38000/102467 239:34/406:26      1.404         {"mlm": 1.3765363574445875, "mse": 0.0}  0.7982
0         train   38100/102467 240:11/405:46      1.404         {"mlm": 1.3659660716851552, "mse": 0.0}  0.465
0         train   38200/102467 240:47/405:06      1.404         {"mlm": 1.3694588998142554, "mse": 0.0}  0.4764
0         train   38300/102467 241:24/404:26      1.404         {"mlm": 1.3723094580141273, "mse": 0.0}  10.0784
0         train   38400/102467 242:00/403:46      1.404         {"mlm": 1.3652456270323858, "mse": 0.0}  0.5461
0         train   38500/102467 242:37/403:06      1.404         {"mlm": 1.3659409938320037, "mse": 0.0}  0.5398
0         train   38600/102467 243:13/402:26      1.404         {"mlm": 1.3664744449341857, "mse": 0.0}  0.7725
0         train   38700/102467 243:50/401:46      1.403         {"mlm": 1.370121725759972, "mse": 0.0}  0.649
0         train   38800/102467 244:26/401:06      1.403         {"mlm": 1.3691897188598787, "mse": 0.0}  0.4878
0         train   38900/102467 245:03/400:26      1.403         {"mlm": 1.3687244289155518, "mse": 0.0}  0.4448
0         train   39000/102467 245:39/399:46      1.403         {"mlm": 1.3713625541413166, "mse": 0.0}  0.5358
0         train   39100/102467 246:16/399:06      1.403         {"mlm": 1.3709208051653674, "mse": 0.0}  0.4358
0         train   39200/102467 246:52/398:27      1.403         {"mlm": 1.3717577577035962, "mse": 0.0}  0.4473
0         train   39300/102467 247:29/397:47      1.403         {"mlm": 1.3726259556909401, "mse": 0.0}  0.4517
0         train   39400/102467 248:05/397:07      1.403         {"mlm": 1.371735132825067, "mse": 0.0}  0.4949
0         train   39500/102467 248:42/396:27      1.403         {"mlm": 1.3700801170366332, "mse": 0.0}  0.5607
0         train   39600/102467 249:18/395:48      1.403         {"mlm": 1.3712629611256129, "mse": 0.0}  0.4265
0         train   39700/102467 249:55/395:08      1.403         {"mlm": 1.37085237898776, "mse": 0.0}  0.4792
0         train   39800/102467 250:31/394:28      1.403         {"mlm": 1.3716486839780828, "mse": 0.0}  0.528
0         train   39900/102467 251:08/393:48      1.403         {"mlm": 1.3710395263720163, "mse": 0.0}  0.4436
0         train   40000/102467 251:44/393:08      1.402         {"mlm": 1.3709373858744252, "mse": 0.0}  0.4406

09/23/2022 12:15:49 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        0:33/436:37        1.332           None
0         valid   101/781      0:49/ 5:31         1.278           None
0         valid   201/781      1:04/ 3:06         1.290           None
0         valid   301/781      1:20/ 2:08         1.275           None
0         valid   401/781      1:35/ 1:30         1.272           None
0         valid   501/781      1:51/ 1:02         1.274           None
0         valid   601/781      2:07/ 0:38         1.270           None
0         valid   701/781      2:22/ 0:16         1.273           None
0         valid   781/781      2:37/ 0:00         1.270         {"mlm": 1.2695262483994878, "mse": 0.0, "train": 0.0}  None
0         train   40100/102467 254:58/396:34      1.402         {"mlm": 1.3355668097734452, "mse": 0.0}  0.4238
0         train   40200/102467 255:34/395:52      1.402         {"mlm": 1.371124733388424, "mse": 0.0}  0.8224
0         train   40300/102467 256:10/395:11      1.402         {"mlm": 1.3794455448786418, "mse": 0.0}  0.4688
0         train   40400/102467 256:47/394:30      1.402         {"mlm": 1.3788603211939334, "mse": 0.0}  0.4901
0         train   40500/102467 257:23/393:48      1.402         {"mlm": 1.3742057884931564, "mse": 0.0}  1.027
0         train   40600/102467 257:59/393:07      1.402         {"mlm": 1.3737101537982623, "mse": 0.0}  0.4882
0         train   40700/102467 258:35/392:26      1.402         {"mlm": 1.372892753311566, "mse": 0.0}  0.6122
0         train   40800/102467 259:12/391:46      1.402         {"mlm": 1.3707558158785105, "mse": 0.0}  0.4286
0         train   40900/102467 259:48/391:05      1.402         {"mlm": 1.369901647898886, "mse": 0.0}  1.7549
0         train   41000/102467 260:24/390:24      1.402         {"mlm": 1.3684154691696166, "mse": 0.0}  0.5085
0         train   41100/102467 261:01/389:43      1.402         {"mlm": 1.369539184028452, "mse": 0.0}  0.8384
0         train   41200/102467 261:37/389:03      1.402         {"mlm": 1.3704059873024623, "mse": 0.0}  0.5607
0         train   41300/102467 262:13/388:22      1.401         {"mlm": 1.3715970807809097, "mse": 0.0}  0.4288
0         train   41400/102467 262:50/387:41      1.401         {"mlm": 1.3708102736728531, "mse": 0.0}  0.4551
0         train   41500/102467 263:26/387:01      1.401         {"mlm": 1.371796440243721, "mse": 0.0}  0.6033
0         train   41600/102467 264:03/386:21      1.401         {"mlm": 1.3732933666929603, "mse": 0.0}  0.6408
0         train   41700/102467 264:39/385:40      1.401         {"mlm": 1.3733025062084199, "mse": 0.0}  0.4519
0         train   41800/102467 265:16/384:59      1.401         {"mlm": 1.3735520180066427, "mse": 0.0}  0.4371
0         train   41900/102467 265:52/384:19      1.401         {"mlm": 1.373450932879197, "mse": 0.0}  0.5117
0         train   42000/102467 266:29/383:39      1.401         {"mlm": 1.3744235529005528, "mse": 0.0}  2.5938
0         train   42100/102467 267:05/382:58      1.401         {"mlm": 1.3629162275429927, "mse": 0.0}  0.422
0         train   42200/102467 267:41/382:18      1.401         {"mlm": 1.3750368752072204, "mse": 0.0}  0.4745
0         train   42300/102467 268:18/381:38      1.401         {"mlm": 1.3793838881329947, "mse": 0.0}  0.4317
0         train   42400/102467 268:54/380:57      1.401         {"mlm": 1.3824690471316938, "mse": 0.0}  0.4225
0         train   42500/102467 269:31/380:17      1.401         {"mlm": 1.3744201064109802, "mse": 0.0}  0.4339
0         train   42600/102467 270:07/379:37      1.401         {"mlm": 1.372864454238363, "mse": 0.0}  4.3191
0         train   42700/102467 270:44/378:57      1.401         {"mlm": 1.3781728842568841, "mse": 0.0}  0.4664
0         train   42800/102467 271:20/378:17      1.401         {"mlm": 1.3795779005756068, "mse": 0.0}  0.4151
0         train   42900/102467 271:57/377:36      1.401         {"mlm": 1.3800138762185519, "mse": 0.0}  0.5531
0         train   43000/102467 272:33/376:56      1.401         {"mlm": 1.3798916533783272, "mse": 0.0}  0.4355
0         train   43100/102467 273:10/376:16      1.401         {"mlm": 1.3788569488777043, "mse": 0.0}  0.7584
0         train   43200/102467 273:46/375:36      1.400         {"mlm": 1.3778791882476775, "mse": 0.0}  0.43
0         train   43300/102467 274:23/374:55      1.400         {"mlm": 1.3783359499598027, "mse": 0.0}  0.3921
0         train   43400/102467 274:59/374:15      1.400         {"mlm": 1.3767075563772309, "mse": 0.0}  0.3778
0         train   43500/102467 275:36/373:35      1.400         {"mlm": 1.375697508782367, "mse": 0.0}  0.4214
0         train   43600/102467 276:12/372:55      1.400         {"mlm": 1.3762740374282423, "mse": 0.0}  0.4699
0         train   43700/102467 276:49/372:15      1.400         {"mlm": 1.3750030124026091, "mse": 0.0}  0.4759
0         train   43800/102467 277:25/371:35      1.400         {"mlm": 1.375132279372202, "mse": 0.0}  0.4105
0         train   43900/102467 278:02/370:55      1.400         {"mlm": 1.3737043443637624, "mse": 0.0}  0.4551
0         train   44000/102467 278:38/370:15      1.400         {"mlm": 1.3735286691535884, "mse": 0.0}  0.401
0         train   44100/102467 279:15/369:35      1.400         {"mlm": 1.3729169539042883, "mse": 0.0}  0.428
0         train   44200/102467 279:51/368:55      1.400         {"mlm": 1.3583704567316808, "mse": 0.0}  0.9001
0         train   44300/102467 280:28/368:15      1.400         {"mlm": 1.3605791962386777, "mse": 0.0}  0.4184
0         train   44400/102467 281:04/367:35      1.400         {"mlm": 1.363258607723006, "mse": 0.0}  0.4677
0         train   44500/102467 281:41/366:55      1.399         {"mlm": 1.3600937817470138, "mse": 0.0}  0.404
0         train   44600/102467 282:17/366:16      1.399         {"mlm": 1.3620691720059883, "mse": 0.0}  0.3972
0         train   44700/102467 282:54/365:36      1.399         {"mlm": 1.3614824644815615, "mse": 0.0}  0.7528
0         train   44800/102467 283:30/364:56      1.399         {"mlm": 1.3642427364088838, "mse": 0.0}  0.5986
0         train   44900/102467 284:07/364:16      1.399         {"mlm": 1.3646084347652698, "mse": 0.0}  0.4545
0         train   45000/102467 284:43/363:36      1.399         {"mlm": 1.3645455514739653, "mse": 0.0}  0.6169
0         train   45100/102467 285:19/362:56      1.399         {"mlm": 1.3652966144632122, "mse": 0.0}  0.4149
0         train   45200/102467 285:56/362:16      1.399         {"mlm": 1.3641092690681178, "mse": 0.0}  0.6286
0         train   45300/102467 286:32/361:36      1.399         {"mlm": 1.3647969158937456, "mse": 0.0}  0.4135
0         train   45400/102467 287:09/360:57      1.399         {"mlm": 1.3651416874324134, "mse": 0.0}  0.3975
0         train   45500/102467 287:45/360:17      1.399         {"mlm": 1.364200287930001, "mse": 0.0}  0.389
0         train   45600/102467 288:22/359:37      1.399         {"mlm": 1.3636725675850845, "mse": 0.0}  0.4033
0         train   45700/102467 288:58/358:57      1.399         {"mlm": 1.364242337863493, "mse": 0.0}  0.7857
0         train   45800/102467 289:35/358:18      1.398         {"mlm": 1.3636699697133299, "mse": 0.0}  0.3841
0         train   45900/102467 290:11/357:38      1.398         {"mlm": 1.3635273122624174, "mse": 0.0}  0.4136
0         train   46000/102467 290:48/356:58      1.398         {"mlm": 1.3627797851452719, "mse": 0.0}  0.4091
0         train   46100/102467 291:24/356:18      1.398         {"mlm": 1.3521070812166351, "mse": 0.0}  0.373
0         train   46200/102467 292:01/355:39      1.398         {"mlm": 1.3612665022690285, "mse": 0.0}  0.4053
0         train   46300/102467 292:37/354:59      1.398         {"mlm": 1.3540478585545062, "mse": 0.0}  0.3722
0         train   46400/102467 293:14/354:19      1.398         {"mlm": 1.3522274758413397, "mse": 0.0}  0.3971
0         train   46500/102467 293:50/353:40      1.398         {"mlm": 1.3494125441526263, "mse": 0.0}  0.3673
0         train   46600/102467 294:27/353:00      1.398         {"mlm": 1.3506518834000656, "mse": 0.0}  0.3972
0         train   46700/102467 295:03/352:20      1.398         {"mlm": 1.3520762537268005, "mse": 0.0}  0.3892
0         train   46800/102467 295:40/351:41      1.397         {"mlm": 1.3520447993966538, "mse": 0.0}  0.4028
0         train   46900/102467 296:16/351:01      1.397         {"mlm": 1.3494986530929105, "mse": 0.0}  0.3718
0         train   47000/102467 296:53/350:22      1.397         {"mlm": 1.3477235234967444, "mse": 0.0}  0.4329
0         train   47100/102467 297:29/349:42      1.397         {"mlm": 1.346595241410578, "mse": 0.0}  0.3773
0         train   47200/102467 298:06/349:03      1.397         {"mlm": 1.3436346472853307, "mse": 0.0}  0.3889
0         train   47300/102467 298:42/348:23      1.397         {"mlm": 1.3433312185930122, "mse": 0.0}  0.4169
0         train   47400/102467 299:19/347:44      1.397         {"mlm": 1.344008150831153, "mse": 0.0}  0.4083
0         train   47500/102467 299:56/347:05      1.397         {"mlm": 1.3438951284708622, "mse": 0.0}  1.2392
0         train   47600/102467 300:32/346:25      1.396         {"mlm": 1.3433830537643743, "mse": 0.0}  0.427
0         train   47700/102467 301:09/345:46      1.396         {"mlm": 1.3442862630241676, "mse": 0.0}  0.3683
0         train   47800/102467 301:45/345:06      1.396         {"mlm": 1.344917388909647, "mse": 0.0}  0.3923
0         train   47900/102467 302:22/344:27      1.396         {"mlm": 1.3474615280038755, "mse": 0.0}  0.4664
0         train   48000/102467 302:58/343:47      1.396         {"mlm": 1.3480008247200703, "mse": 0.0}  0.618
0         train   48100/102467 303:35/343:08      1.396         {"mlm": 1.3902144792179267, "mse": 0.0}  0.6626
0         train   48200/102467 304:11/342:28      1.396         {"mlm": 1.3649048793072602, "mse": 0.0}  0.7665
0         train   48300/102467 304:48/341:49      1.396         {"mlm": 1.3627282995227221, "mse": 0.0}  0.5393
0         train   48400/102467 305:24/341:10      1.396         {"mlm": 1.3606443599318012, "mse": 0.0}  0.4918
0         train   48500/102467 306:01/340:30      1.396         {"mlm": 1.3569334502662382, "mse": 0.0}  0.3961
0         train   48600/102467 306:37/339:51      1.396         {"mlm": 1.357684809589546, "mse": 0.0}  0.4349
0         train   48700/102467 307:14/339:12      1.396         {"mlm": 1.3579757508190198, "mse": 0.0}  0.3775
0         train   48800/102467 307:50/338:32      1.395         {"mlm": 1.3545595790872622, "mse": 0.0}  0.5044
0         train   48900/102467 308:27/337:53      1.395         {"mlm": 1.3557244586492223, "mse": 0.0}  0.6973
0         train   49000/102467 309:03/337:14      1.395         {"mlm": 1.3553979788798405, "mse": 0.0}  0.577
0         train   49100/102467 309:40/336:34      1.395         {"mlm": 1.353935620917456, "mse": 0.0}  0.3788
0         train   49200/102467 310:16/335:55      1.395         {"mlm": 1.3543018401665832, "mse": 0.0}  0.3623
0         train   49300/102467 310:53/335:16      1.395         {"mlm": 1.3517035724664177, "mse": 0.0}  0.5337
0         train   49400/102467 311:29/334:36      1.395         {"mlm": 1.3505821062364005, "mse": 0.0}  0.3637
0         train   49500/102467 312:05/333:57      1.395         {"mlm": 1.3505971212995882, "mse": 0.0}  0.4393
0         train   49600/102467 312:42/333:18      1.395         {"mlm": 1.3498119915786542, "mse": 0.0}  0.372
0         train   49700/102467 313:18/332:38      1.395         {"mlm": 1.3506217553930462, "mse": 0.0}  0.3495
0         train   49800/102467 313:55/331:59      1.394         {"mlm": 1.3505587697626489, "mse": 0.0}  1.5542
0         train   49900/102467 314:31/331:20      1.394         {"mlm": 1.3515496985507414, "mse": 0.0}  0.4259
0         train   50000/102467 315:08/330:41      1.394         {"mlm": 1.3507205474830581, "mse": 0.0}  0.4066

09/23/2022 13:19:12 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        0:36/475:00        1.049           None
0         valid   101/781      0:51/ 5:49         1.262           None
0         valid   201/781      1:07/ 3:14         1.267           None
0         valid   301/781      1:23/ 2:12         1.263           None
0         valid   401/781      1:38/ 1:33         1.259           None
0         valid   501/781      1:54/ 1:03         1.262           None
0         valid   601/781      2:09/ 0:38         1.262           None
0         valid   701/781      2:25/ 0:16         1.267           None
0         valid   781/781      2:39/ 0:00         1.255         {"mlm": 1.2548015364916774, "mse": 0.0, "train": 0.0}  None
0         train   50100/102467 318:24/332:49      1.394         {"mlm": 1.3421681690216065, "mse": 0.0}  0.3951
0         train   50200/102467 319:00/332:09      1.394         {"mlm": 1.371114013195038, "mse": 0.0}  0.4502
0         train   50300/102467 319:37/331:28      1.394         {"mlm": 1.3766119869550069, "mse": 0.0}  0.3738
0         train   50400/102467 320:13/330:48      1.394         {"mlm": 1.3705418321490288, "mse": 0.0}  0.3686
0         train   50500/102467 320:49/330:08      1.394         {"mlm": 1.3707186841964722, "mse": 0.0}  0.3489
0         train   50600/102467 321:25/329:28      1.394         {"mlm": 1.3684267010291418, "mse": 0.0}  0.4451
0         train   50700/102467 322:01/328:48      1.394         {"mlm": 1.3655959861619131, "mse": 0.0}  0.351
0         train   50800/102467 322:38/328:08      1.394         {"mlm": 1.3651289384067058, "mse": 0.0}  0.413
0         train   50900/102467 323:14/327:28      1.394         {"mlm": 1.36330985016293, "mse": 0.0}  0.4069
0         train   51000/102467 323:50/326:48      1.394         {"mlm": 1.3615708029270173, "mse": 0.0}  0.5122
0         train   51100/102467 324:27/326:08      1.394         {"mlm": 1.36152917498892, "mse": 0.0}  0.412
0         train   51200/102467 325:03/325:29      1.394         {"mlm": 1.3638066792984804, "mse": 0.0}  0.6893
0         train   51300/102467 325:39/324:49      1.394         {"mlm": 1.3629045840410086, "mse": 0.0}  1.5887
0         train   51400/102467 326:16/324:09      1.393         {"mlm": 1.3619720513905798, "mse": 0.0}  1.2863
0         train   51500/102467 326:52/323:29      1.393         {"mlm": 1.3601056131919225, "mse": 0.0}  1.6939
0         train   51600/102467 327:29/322:50      1.393         {"mlm": 1.3591560085490346, "mse": 0.0}  0.5256
0         train   51700/102467 328:05/322:10      1.393         {"mlm": 1.3605239266858382, "mse": 0.0}  0.9663
0         train   51800/102467 328:42/321:30      1.393         {"mlm": 1.36094910585218, "mse": 0.0}  0.3825
0         train   51900/102467 329:18/320:51      1.393         {"mlm": 1.3618677550554275, "mse": 0.0}  1.3578
0         train   52000/102467 329:55/320:11      1.393         {"mlm": 1.3614263460636138, "mse": 0.0}  0.392
0         train   52100/102467 330:31/319:32      1.393         {"mlm": 1.3876902226245764, "mse": 0.0}  1.1243
0         train   52200/102467 331:08/318:52      1.393         {"mlm": 1.3795700315854058, "mse": 0.0}  0.7984
0         train   52300/102467 331:44/318:12      1.393         {"mlm": 1.3710772769905653, "mse": 0.0}  0.6641
0         train   52400/102467 332:21/317:33      1.393         {"mlm": 1.370314924041729, "mse": 0.0}  0.3442
0         train   52500/102467 332:57/316:53      1.393         {"mlm": 1.362152968953272, "mse": 0.0}  0.4373
0         train   52600/102467 333:34/316:14      1.393         {"mlm": 1.3598547262421037, "mse": 0.0}  0.384
0         train   52700/102467 334:10/315:34      1.393         {"mlm": 1.3592581211753159, "mse": 0.0}  0.3606
0         train   52800/102467 334:47/314:55      1.393         {"mlm": 1.3566227406076854, "mse": 0.0}  0.3653
0         train   52900/102467 335:23/314:15      1.392         {"mlm": 1.3594921930082913, "mse": 0.0}  0.433
0         train   53000/102467 336:00/313:36      1.392         {"mlm": 1.3612046139137641, "mse": 0.0}  0.3189
0         train   53100/102467 336:36/312:56      1.392         {"mlm": 1.361125412423357, "mse": 0.0}  0.4165
0         train   53200/102467 337:13/312:17      1.392         {"mlm": 1.3600959880735797, "mse": 0.0}  0.4756
0         train   53300/102467 337:49/311:37      1.392         {"mlm": 1.3577829569555229, "mse": 0.0}  1.4736
0         train   53400/102467 338:26/310:58      1.392         {"mlm": 1.3579227041993676, "mse": 0.0}  0.4016
0         train   53500/102467 339:02/310:19      1.392         {"mlm": 1.3571518198341588, "mse": 0.0}  0.8031
0         train   53600/102467 339:39/309:39      1.392         {"mlm": 1.3574393362160992, "mse": 0.0}  0.6452
0         train   53700/102467 340:15/309:00      1.392         {"mlm": 1.3589506143678842, "mse": 0.0}  4.5475
0         train   53800/102467 340:52/308:20      1.392         {"mlm": 1.3586530308050206, "mse": 0.0}  0.3557
0         train   53900/102467 341:28/307:41      1.392         {"mlm": 1.3580581121786197, "mse": 0.0}  0.3494
0         train   54000/102467 342:05/307:02      1.392         {"mlm": 1.3566026383784486, "mse": 0.0}  0.3492
0         train   54100/102467 342:41/306:22      1.392         {"mlm": 1.326369000332696, "mse": 0.0}  1.3625
0         train   54200/102467 343:18/305:43      1.391         {"mlm": 1.3311972082263293, "mse": 0.0}  0.3335
0         train   54300/102467 343:54/305:04      1.391         {"mlm": 1.3348851857969426, "mse": 0.0}  0.8751
0         train   54400/102467 344:31/304:25      1.391         {"mlm": 1.3365522693449527, "mse": 0.0}  5.6701
0         train   54500/102467 345:07/303:45      1.391         {"mlm": 1.3354267679065106, "mse": 0.0}  0.3397
0         train   54600/102467 345:44/303:06      1.391         {"mlm": 1.3377254963120488, "mse": 0.0}  0.5486
0         train   54700/102467 346:20/302:27      1.391         {"mlm": 1.343928818033213, "mse": 0.0}  0.6078
0         train   54800/102467 346:57/301:47      1.391         {"mlm": 1.3441088842718225, "mse": 0.0}  0.8299
0         train   54900/102467 347:34/301:08      1.391         {"mlm": 1.3474735522721553, "mse": 0.0}  0.4057
0         train   55000/102467 348:10/300:29      1.391         {"mlm": 1.3586722935488325, "mse": 0.0}  0.3793
0         train   55100/102467 348:46/299:49      1.391         {"mlm": 1.3617179697220008, "mse": 0.0}  0.5071
0         train   55200/102467 349:23/299:10      1.391         {"mlm": 1.3763508779278184, "mse": 0.0}  3.0295
0         train   55300/102467 349:59/298:31      1.391         {"mlm": 1.378880972946737, "mse": 0.0}  1.6278
0         train   55400/102467 350:36/297:52      1.391         {"mlm": 1.377711499112529, "mse": 0.0}  0.5669
0         train   55500/102467 351:12/297:12      1.391         {"mlm": 1.3755220791526408, "mse": 0.0}  0.4698
0         train   55600/102467 351:49/296:33      1.391         {"mlm": 1.3731122953870867, "mse": 0.0}  0.4622
0         train   55700/102467 352:25/295:54      1.391         {"mlm": 1.3722986879772798, "mse": 0.0}  1.2641
0         train   55800/102467 353:02/295:15      1.391         {"mlm": 1.371750644867094, "mse": 0.0}  0.529
0         train   55900/102467 353:38/294:36      1.391         {"mlm": 1.3715227948162152, "mse": 0.0}  0.4463
0         train   56000/102467 354:15/293:56      1.391         {"mlm": 1.3721825614407495, "mse": 0.0}  1.7407
0         train   56100/102467 354:51/293:17      1.391         {"mlm": 1.3591627005449276, "mse": 0.0}  0.4512
0         train   56200/102467 355:28/292:38      1.391         {"mlm": 1.3735214686635786, "mse": 0.0}  0.4251
0         train   56300/102467 356:04/291:59      1.391         {"mlm": 1.368857650243072, "mse": 0.0}  0.3903
0         train   56400/102467 356:41/291:20      1.391         {"mlm": 1.3663141736455768, "mse": 0.0}  0.3142
0         train   56500/102467 357:17/290:41      1.391         {"mlm": 1.3720908265718272, "mse": 0.0}  177.3103
0         train   56600/102467 357:54/290:02      1.391         {"mlm": 1.3823520013036041, "mse": 0.0}  0.3447
0         train   56700/102467 358:30/289:22      1.391         {"mlm": 1.3816528398987211, "mse": 0.0}  0.9583
0         train   56800/102467 359:07/288:43      1.391         {"mlm": 1.383558201670198, "mse": 0.0}  5.0113
0         train   56900/102467 359:43/288:04      1.391         {"mlm": 1.3858299475316884, "mse": 0.0}  2.4427
0         train   57000/102467 360:20/287:25      1.391         {"mlm": 1.3855460614952424, "mse": 0.0}  0.8296
0         train   57100/102467 360:56/286:46      1.392         {"mlm": 1.4183894118180358, "mse": 0.0}  15.3986
0         train   57200/102467 361:32/286:07      1.392         {"mlm": 1.4292114344157074, "mse": 0.0}  1.7022
0         train   57300/102467 362:09/285:28      1.392         {"mlm": 1.42693732289415, "mse": 0.0}  6.6744
0         train   57400/102467 362:45/284:49      1.392         {"mlm": 1.4213966548058845, "mse": 0.0}  3.0565
0         train   57500/102467 363:22/284:10      1.392         {"mlm": 1.4160510873348615, "mse": 0.0}  32.818
0         train   57600/102467 363:58/283:31      1.392         {"mlm": 1.4127191193191873, "mse": 0.0}  3.5948
0         train   57700/102467 364:35/282:52      1.392         {"mlm": 1.4099778195444668, "mse": 0.0}  0.5767
0         train   57800/102467 365:11/282:13      1.392         {"mlm": 1.4085081670107813, "mse": 0.0}  0.5259
0         train   57900/102467 365:48/281:34      1.391         {"mlm": 1.405371497258049, "mse": 0.0}  1.9101
0         train   58000/102467 366:24/280:55      1.391         {"mlm": 1.402063393956969, "mse": 0.0}  0.5721
0         train   58100/102467 367:01/280:16      1.391         {"mlm": 1.3923243333896, "mse": 0.0}  2.7523
0         train   58200/102467 367:37/279:37      1.391         {"mlm": 1.3658107367097114, "mse": 0.0}  1.0873
0         train   58300/102467 368:14/278:58      1.391         {"mlm": 1.3737693932410833, "mse": 0.0}  0.3287
0         train   58400/102467 368:50/278:19      1.391         {"mlm": 1.3740533635471806, "mse": 0.0}  0.3627
0         train   58500/102467 369:27/277:40      1.391         {"mlm": 1.37157747798389, "mse": 0.0}  0.3951
0         train   58600/102467 370:03/277:01      1.391         {"mlm": 1.369941434904233, "mse": 0.0}  0.4807
0         train   58700/102467 370:40/276:22      1.391         {"mlm": 1.3678874277520454, "mse": 0.0}  0.838
0         train   58800/102467 371:16/275:43      1.391         {"mlm": 1.3689552734245607, "mse": 0.0}  0.4357
0         train   58900/102467 371:52/275:04      1.391         {"mlm": 1.3619802080627, "mse": 0.0}  0.3367
0         train   59000/102467 372:29/274:25      1.391         {"mlm": 1.3627790525615455, "mse": 0.0}  0.3204
0         train   59100/102467 373:05/273:46      1.391         {"mlm": 1.3617241439701866, "mse": 0.0}  0.3129
0         train   59200/102467 373:42/273:07      1.391         {"mlm": 1.358576020369163, "mse": 0.0}  0.4318
0         train   59300/102467 374:18/272:28      1.391         {"mlm": 1.3620577216332341, "mse": 0.0}  14.7279
0         train   59400/102467 374:55/271:49      1.391         {"mlm": 1.372244926484403, "mse": 0.0}  0.3419
0         train   59500/102467 375:31/271:10      1.391         {"mlm": 1.3723805630430181, "mse": 0.0}  0.3098
0         train   59600/102467 376:08/270:32      1.391         {"mlm": 1.3715332347274125, "mse": 0.0}  0.4595
0         train   59700/102467 376:44/269:53      1.391         {"mlm": 1.3724047265033115, "mse": 0.0}  0.4356
0         train   59800/102467 377:21/269:14      1.391         {"mlm": 1.3715846577373008, "mse": 0.0}  0.634
0         train   59900/102467 377:57/268:35      1.391         {"mlm": 1.371244137391511, "mse": 0.0}  1.072
0         train   60000/102467 378:34/267:56      1.391         {"mlm": 1.3823031324124289, "mse": 0.0}  7.8715

09/23/2022 14:22:38 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        1:08/885:14        1.256           None
0         valid   101/781      1:23/ 9:23         1.273           None
0         valid   201/781      1:39/ 4:46         1.281           None
0         valid   301/781      1:54/ 3:02         1.271           None
0         valid   401/781      2:10/ 2:03         1.265           None
0         valid   501/781      2:25/ 1:21         1.264           None
0         valid   601/781      2:41/ 0:48         1.265           None
0         valid   701/781      2:56/ 0:20         1.268           None
0         valid   781/781      3:11/ 0:00         1.268         {"mlm": 1.267605633802817, "mse": 0.0, "train": 0.0}  None
0         train   60100/102467 382:22/269:32      1.391         {"mlm": 1.3722680217027665, "mse": 0.0}  1.245
0         train   60200/102467 382:58/268:53      1.391         {"mlm": 1.3698229470849037, "mse": 0.0}  1.9679
0         train   60300/102467 383:34/268:13      1.391         {"mlm": 1.3590314340591432, "mse": 0.0}  14.9981
0         train   60400/102467 384:10/267:33      1.391         {"mlm": 1.381867516040802, "mse": 0.0}  12.8566
0         train   60500/102467 384:46/266:54      1.391         {"mlm": 1.3962058342695236, "mse": 0.0}  0.3155
0         train   60600/102467 385:22/266:14      1.391         {"mlm": 1.3915623467167217, "mse": 0.0}  0.7661
0         train   60700/102467 385:58/265:35      1.391         {"mlm": 1.3860451487132481, "mse": 0.0}  1.4082
0         train   60800/102467 386:34/264:55      1.391         {"mlm": 1.3822703211009502, "mse": 0.0}  1.069
0         train   60900/102467 387:11/264:16      1.391         {"mlm": 1.3771654667456945, "mse": 0.0}  0.7226
0         train   61000/102467 387:47/263:36      1.391         {"mlm": 1.37572496342659, "mse": 0.0}  0.4542
0         train   61100/102467 388:23/262:57      1.391         {"mlm": 1.375653435804627, "mse": 0.0}  0.455
0         train   61200/102467 388:59/262:17      1.391         {"mlm": 1.3751611723502477, "mse": 0.0}  1.0666
0         train   61300/102467 389:36/261:38      1.391         {"mlm": 1.3714515880437999, "mse": 0.0}  0.2793
0         train   61400/102467 390:12/260:59      1.391         {"mlm": 1.3709639060923031, "mse": 0.0}  1.0029
0         train   61500/102467 390:48/260:19      1.391         {"mlm": 1.369347833275795, "mse": 0.0}  0.3952
0         train   61600/102467 391:25/259:40      1.391         {"mlm": 1.3685508046671748, "mse": 0.0}  0.3228
0         train   61700/102467 392:01/259:01      1.390         {"mlm": 1.365859577129869, "mse": 0.0}  0.3565
0         train   61800/102467 392:37/258:21      1.390         {"mlm": 1.3664907699823379, "mse": 0.0}  0.6811
0         train   61900/102467 393:14/257:42      1.390         {"mlm": 1.3656999889173005, "mse": 0.0}  1.5309
0         train   62000/102467 393:50/257:03      1.390         {"mlm": 1.3663070621192455, "mse": 0.0}  0.8494
0         train   62100/102467 394:27/256:24      1.390         {"mlm": 1.3490078569662691, "mse": 0.0}  0.3253
0         train   62200/102467 395:03/255:45      1.390         {"mlm": 1.3459208038584072, "mse": 0.0}  2.4491
0         train   62300/102467 395:40/255:06      1.390         {"mlm": 1.3461805540183716, "mse": 0.0}  0.3155
0         train   62400/102467 396:16/254:27      1.390         {"mlm": 1.3459140652403199, "mse": 0.0}  0.4438
0         train   62500/102467 396:53/253:47      1.390         {"mlm": 1.3445599071965189, "mse": 0.0}  3.5048
0         train   62600/102467 397:29/253:08      1.390         {"mlm": 1.3703819413814002, "mse": 0.0}  0.317
0         train   62700/102467 398:06/252:29      1.390         {"mlm": 1.3663212850540662, "mse": 0.0}  1.3054
0         train   62800/102467 398:42/251:50      1.390         {"mlm": 1.363792866580328, "mse": 0.0}  0.3268
0         train   62900/102467 399:19/251:11      1.390         {"mlm": 1.3640996797198846, "mse": 0.0}  0.3226
0         train   63000/102467 399:55/250:32      1.390         {"mlm": 1.3627921233067404, "mse": 0.0}  0.2778
0         train   63100/102467 400:32/249:53      1.390         {"mlm": 1.3599124595834733, "mse": 0.0}  0.2876
0         train   63200/102467 401:09/249:14      1.390         {"mlm": 1.359836194592779, "mse": 0.0}  1.5307
0         train   63300/102467 401:45/248:35      1.390         {"mlm": 1.3575682797920163, "mse": 0.0}  0.3554
0         train   63400/102467 402:22/247:56      1.390         {"mlm": 1.3699642477758107, "mse": 0.0}  128.5163
0         train   63500/102467 402:58/247:17      1.390         {"mlm": 1.3751025444829839, "mse": 0.0}  0.7095
0         train   63600/102467 403:35/246:38      1.390         {"mlm": 1.3725636007638182, "mse": 0.0}  0.3064
0         train   63700/102467 404:11/245:59      1.390         {"mlm": 1.3694302597910324, "mse": 0.0}  0.3007
0         train   63800/102467 404:48/245:20      1.390         {"mlm": 1.3680245133490614, "mse": 0.0}  0.3657
0         train   63900/102467 405:24/244:41      1.390         {"mlm": 1.3648750603732591, "mse": 0.0}  0.3489
0         train   64000/102467 406:00/244:02      1.389         {"mlm": 1.363508998214632, "mse": 0.0}  0.3753
0         train   64100/102467 406:37/243:23      1.390         {"mlm": 1.6456082463264465, "mse": 0.0}  5.7668
0         train   64200/102467 407:13/242:44      1.390         {"mlm": 1.6947375120538655, "mse": 0.0}  0.4343
0         train   64300/102467 407:50/242:05      1.391         {"mlm": 1.7409851575057778, "mse": 0.0}  3.9897
0         train   64400/102467 408:26/241:26      1.391         {"mlm": 1.6529615042197645, "mse": 0.0}  0.8983
0         train   64500/102467 409:03/240:47      1.391         {"mlm": 1.5900392704699413, "mse": 0.0}  0.6025
0         train   64600/102467 409:39/240:08      1.391         {"mlm": 1.5531029940448875, "mse": 0.0}  0.374
0         train   64700/102467 410:16/239:29      1.391         {"mlm": 1.5302204490561881, "mse": 0.0}  0.6671
0         train   64800/102467 410:52/238:50      1.391         {"mlm": 1.50602002542718, "mse": 0.0}  0.3518
0         train   64900/102467 411:29/238:11      1.391         {"mlm": 1.4902874436436888, "mse": 0.0}  0.2978
0         train   65000/102467 412:05/237:32      1.391         {"mlm": 1.4796004439881425, "mse": 0.0}  0.3398
0         train   65100/102467 412:42/236:53      1.391         {"mlm": 1.4685695611605445, "mse": 0.0}  0.7224
0         train   65200/102467 413:18/236:14      1.391         {"mlm": 1.455997230960849, "mse": 0.0}  0.3542
0         train   65300/102467 413:55/235:35      1.391         {"mlm": 1.446930902718763, "mse": 0.0}  0.4597
0         train   65400/102467 414:31/234:56      1.391         {"mlm": 1.439214064839572, "mse": 0.0}  0.385
0         train   65500/102467 415:07/234:17      1.390         {"mlm": 1.431650938234915, "mse": 0.0}  0.3843
0         train   65600/102467 415:44/233:38      1.390         {"mlm": 1.4247459789017711, "mse": 0.0}  0.3413
0         train   65700/102467 416:20/232:59      1.390         {"mlm": 1.4204674399994288, "mse": 0.0}  0.289
0         train   65800/102467 416:57/232:20      1.390         {"mlm": 1.4174990247897763, "mse": 0.0}  0.5496
0         train   65900/102467 417:33/231:42      1.390         {"mlm": 1.4115367147392417, "mse": 0.0}  0.4563
0         train   66000/102467 418:10/231:03      1.390         {"mlm": 1.4074712151342683, "mse": 0.0}  0.4115
0         train   66100/102467 418:46/230:24      1.390         {"mlm": 1.3444448236337643, "mse": 0.0}  0.3238
0         train   66200/102467 419:23/229:45      1.390         {"mlm": 1.3381392402092211, "mse": 0.0}  0.34
0         train   66300/102467 419:59/229:06      1.390         {"mlm": 1.348345783222404, "mse": 0.0}  3.7682
0         train   66400/102467 420:36/228:27      1.390         {"mlm": 1.3505592632954306, "mse": 0.0}  0.3465
0         train   66500/102467 421:12/227:48      1.390         {"mlm": 1.351838955337133, "mse": 0.0}  0.4077
0         train   66600/102467 421:49/227:10      1.390         {"mlm": 1.3524769944960948, "mse": 0.0}  0.6882
0         train   66700/102467 422:25/226:31      1.390         {"mlm": 1.3521982362143792, "mse": 0.0}  0.2883
0         train   66800/102467 423:02/225:52      1.390         {"mlm": 1.3494577007730453, "mse": 0.0}  0.3073
0         train   66900/102467 423:38/225:13      1.389         {"mlm": 1.3515582721900514, "mse": 0.0}  2.887
0         train   67000/102467 424:15/224:34      1.389         {"mlm": 1.3489539493529226, "mse": 0.0}  3.7148
0         train   67100/102467 424:51/223:56      1.389         {"mlm": 1.353284252102851, "mse": 0.0}  0.639
0         train   67200/102467 425:28/223:17      1.389         {"mlm": 1.3535161140369394, "mse": 0.0}  0.555
0         train   67300/102467 426:04/222:38      1.389         {"mlm": 1.3534519997301153, "mse": 0.0}  0.4559
0         train   67400/102467 426:40/221:59      1.389         {"mlm": 1.350868373319944, "mse": 0.0}  1.4473
0         train   67500/102467 427:17/221:20      1.389         {"mlm": 1.3507034041201502, "mse": 0.0}  0.3138
0         train   67600/102467 427:53/220:42      1.389         {"mlm": 1.350687866257218, "mse": 0.0}  0.3044
0         train   67700/102467 428:30/220:03      1.389         {"mlm": 1.34941883956817, "mse": 0.0}  0.3789
0         train   67800/102467 429:06/219:24      1.389         {"mlm": 1.3489262684557792, "mse": 0.0}  4.1965
0         train   67900/102467 429:43/218:45      1.389         {"mlm": 1.3476796455928761, "mse": 0.0}  0.2912
0         train   68000/102467 430:19/218:07      1.389         {"mlm": 1.3457783158266012, "mse": 0.0}  1.7631
0         train   68100/102467 430:56/217:28      1.389         {"mlm": 1.3754233097036679, "mse": 0.0}  0.4853
0         train   68200/102467 431:32/216:49      1.389         {"mlm": 1.3767004204647881, "mse": 0.0}  0.3878
0         train   68300/102467 432:09/216:11      1.389         {"mlm": 1.3517381428061306, "mse": 0.0}  0.4427
0         train   68400/102467 432:45/215:32      1.389         {"mlm": 1.3577841276472264, "mse": 0.0}  0.436
0         train   68500/102467 433:22/214:53      1.389         {"mlm": 1.364324258940835, "mse": 0.0}  0.3646
0         train   68600/102467 433:58/214:15      1.393         {"mlm": 1.8292825854064634, "mse": 0.0}  68.2718
0         train   68700/102467 434:35/213:36      1.402         {"mlm": 2.685597552650276, "mse": 0.0}  0.2782
0         train   68800/102467 435:11/212:57      1.411         {"mlm": 3.3200652006283478, "mse": 0.0}  0.1288
0         train   68900/102467 435:48/212:18      1.420         {"mlm": 3.810863921684878, "mse": 0.0}  1.599
0         train   69000/102467 436:24/211:40      1.429         {"mlm": 4.203783688775028, "mse": 0.0}  0.1251
0         train   69100/102467 437:00/211:01      1.438         {"mlm": 4.526082276427833, "mse": 0.0}  0.1135
0         train   69200/102467 437:37/210:22      1.448         {"mlm": 4.793088218440181, "mse": 0.0}  0.2323
0         train   69300/102467 438:13/209:44      1.457         {"mlm": 5.018958611988727, "mse": 0.0}  34.6977
0         train   69400/102467 438:50/209:05      1.466         {"mlm": 5.213786270010437, "mse": 0.0}  0.1261
0         train   69500/102467 439:26/208:26      1.475         {"mlm": 5.382251222503377, "mse": 0.0}  0.1045
0         train   69600/102467 440:02/207:48      1.484         {"mlm": 5.5312329324564535, "mse": 0.0}  3.4533
0         train   69700/102467 440:38/207:09      1.493         {"mlm": 5.660683950163284, "mse": 0.0}  0.1072
0         train   69800/102467 441:15/206:30      1.502         {"mlm": 5.774341428200227, "mse": 0.0}  0.1255
0         train   69900/102467 441:51/205:51      1.510         {"mlm": 5.877609314546303, "mse": 0.0}  0.1022
0         train   70000/102467 442:27/205:13      1.519         {"mlm": 5.970282686736159, "mse": 0.0}  0.1102

09/23/2022 15:26:32 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        0:25/336:15        7.779           None
0         valid   101/781      0:41/ 4:37         7.709           None
0         valid   201/781      0:56/ 2:43         7.718           None
0         valid   301/781      1:11/ 1:54         7.720           None
0         valid   401/781      1:27/ 1:22         7.723           None
0         valid   501/781      1:42/ 0:57         7.723           None
0         valid   601/781      1:57/ 0:35         7.723           None
0         valid   701/781      2:13/ 0:15         7.724           None
0         valid   781/781      2:28/ 0:00         7.723         {"mlm": 7.723271446862996, "mse": 0.0, "train": 0.0}  None
0         train   70100/102467 445:32/205:42      1.528         {"mlm": 7.726502766609192, "mse": 0.0}  0.1163
0         train   70200/102467 446:08/205:03      1.537         {"mlm": 7.739069612026214, "mse": 0.0}  0.0805
0         train   70300/102467 446:44/204:24      1.546         {"mlm": 7.730875447591146, "mse": 0.0}  0.1054
0         train   70400/102467 447:20/203:45      1.555         {"mlm": 7.7325913834571836, "mse": 0.0}  0.0876
0         train   70500/102467 447:56/203:06      1.563         {"mlm": 7.730670443534851, "mse": 0.0}  0.0921
0         train   70600/102467 448:32/202:27      1.572         {"mlm": 7.7289489928881325, "mse": 0.0}  0.0893
0         train   70700/102467 449:08/201:48      1.581         {"mlm": 7.730015475409371, "mse": 0.0}  0.0848
0         train   70800/102467 449:44/201:09      1.589         {"mlm": 7.724853564500808, "mse": 0.0}  0.0842
0         train   70900/102467 450:20/200:30      1.598         {"mlm": 7.722302876048618, "mse": 0.0}  0.0913
0         train   71000/102467 450:56/199:51      1.607         {"mlm": 7.720217043876648, "mse": 0.0}  0.079
0         train   71100/102467 451:33/199:12      1.615         {"mlm": 7.720765289393339, "mse": 0.0}  0.0761
0         train   71200/102467 452:09/198:33      1.624         {"mlm": 7.720609407424927, "mse": 0.0}  0.0822
0         train   71300/102467 452:45/197:54      1.632         {"mlm": 7.722749624619117, "mse": 0.0}  0.0897
0         train   71400/102467 453:21/197:15      1.641         {"mlm": 7.723732214995793, "mse": 0.0}  0.0929
0         train   71500/102467 453:58/196:37      1.650         {"mlm": 7.724772699991862, "mse": 0.0}  0.0854
0         train   71600/102467 454:34/195:58      1.658         {"mlm": 7.724496597349644, "mse": 0.0}  0.0753
0         train   71700/102467 455:10/195:19      1.666         {"mlm": 7.724591243687798, "mse": 0.0}  0.0825
0         train   71800/102467 455:47/194:40      1.675         {"mlm": 7.723985481792026, "mse": 0.0}  0.0937
0         train   71900/102467 456:23/194:01      1.683         {"mlm": 7.724063125660545, "mse": 0.0}  0.0822
0         train   72000/102467 456:59/193:22      1.692         {"mlm": 7.723884369134903, "mse": 0.0}  0.0826
0         train   72100/102467 457:36/192:43      1.700         {"mlm": 7.705745841517593, "mse": 0.0}  0.0864
0         train   72200/102467 458:12/192:05      1.708         {"mlm": 7.7035093019955125, "mse": 0.0}  0.0729
0         train   72300/102467 458:48/191:26      1.717         {"mlm": 7.7158731074636195, "mse": 0.0}  0.0774
0         train   72400/102467 459:25/190:47      1.725         {"mlm": 7.717224344573822, "mse": 0.0}  0.0955
0         train   72500/102467 460:01/190:08      1.733         {"mlm": 7.720885737386639, "mse": 0.0}  0.0695
0         train   72600/102467 460:37/189:29      1.742         {"mlm": 7.723802690712956, "mse": 0.0}  0.0795
0         train   72700/102467 461:14/188:51      1.750         {"mlm": 7.7244624163800895, "mse": 0.0}  0.069
0         train   72800/102467 461:50/188:12      1.758         {"mlm": 7.724492073655875, "mse": 0.0}  0.0732
0         train   72900/102467 462:26/187:33      1.766         {"mlm": 7.726040508643671, "mse": 0.0}  0.068
0         train   73000/102467 463:03/186:54      1.774         {"mlm": 7.72490946881406, "mse": 0.0}  0.085
0         train   73100/102467 463:39/186:16      1.782         {"mlm": 7.726064280231396, "mse": 0.0}  0.1031
0         train   73200/102467 464:15/185:37      1.791         {"mlm": 7.725236613915501, "mse": 0.0}  0.0706
0         train   73300/102467 464:52/184:58      1.799         {"mlm": 7.725161225727836, "mse": 0.0}  0.0852
0         train   73400/102467 465:28/184:19      1.807         {"mlm": 7.725532263496077, "mse": 0.0}  0.0711
0         train   73500/102467 466:04/183:41      1.815         {"mlm": 7.725503898287233, "mse": 0.0}  0.0646
0         train   73600/102467 466:40/183:02      1.823         {"mlm": 7.724691865442693, "mse": 0.0}  0.0716
0         train   73700/102467 467:17/182:23      1.831         {"mlm": 7.723188781120835, "mse": 0.0}  0.0698
0         train   73800/102467 467:53/181:44      1.839         {"mlm": 7.722555825549938, "mse": 0.0}  0.0781
0         train   73900/102467 468:29/181:06      1.847         {"mlm": 7.721764720948888, "mse": 0.0}  0.0696
0         train   74000/102467 469:06/180:27      1.855         {"mlm": 7.720848306051906, "mse": 0.0}  0.0657
0         train   74100/102467 469:42/179:48      1.863         {"mlm": 7.717640940023928, "mse": 0.0}  0.062
0         train   74200/102467 470:18/179:10      1.870         {"mlm": 7.72940791014469, "mse": 0.0}  0.071
0         train   74300/102467 470:55/178:31      1.878         {"mlm": 7.73390074544305, "mse": 0.0}  0.0655
0         train   74400/102467 471:31/177:52      1.886         {"mlm": 7.7296518057435, "mse": 0.0}  0.0678
0         train   74500/102467 472:07/177:14      1.894         {"mlm": 7.728776851332332, "mse": 0.0}  0.0681
0         train   74600/102467 472:44/176:35      1.902         {"mlm": 7.731642737436454, "mse": 0.0}  0.066
0         train   74700/102467 473:20/175:56      1.910         {"mlm": 7.729983144639898, "mse": 0.0}  0.06
0         train   74800/102467 473:56/175:18      1.917         {"mlm": 7.727403416071918, "mse": 0.0}  0.0636
0         train   74900/102467 474:32/174:39      1.925         {"mlm": 7.726936012174611, "mse": 0.0}  0.0709
0         train   75000/102467 475:09/174:00      1.933         {"mlm": 7.727992813190621, "mse": 0.0}  0.0651
0         train   75100/102467 475:45/173:22      1.941         {"mlm": 7.72813956290212, "mse": 0.0}  0.0724
0         train   75200/102467 476:22/172:43      1.948         {"mlm": 7.727436766202541, "mse": 0.0}  0.0661
0         train   75300/102467 476:58/172:05      1.956         {"mlm": 7.728068894340739, "mse": 0.0}  0.0646
0         train   75400/102467 477:34/171:26      1.964         {"mlm": 7.726951953167568, "mse": 0.0}  0.0672
0         train   75500/102467 478:10/170:47      1.971         {"mlm": 7.726968890993554, "mse": 0.0}  0.0754
0         train   75600/102467 478:47/170:09      1.979         {"mlm": 7.725722097485176, "mse": 0.0}  0.0729
0         train   75700/102467 479:23/169:30      1.987         {"mlm": 7.726688359174066, "mse": 0.0}  0.0658
0         train   75800/102467 479:59/168:52      1.994         {"mlm": 7.726183862389129, "mse": 0.0}  0.077
0         train   75900/102467 480:36/168:13      2.002         {"mlm": 7.726819070046267, "mse": 0.0}  0.0698
0         train   76000/102467 481:12/167:34      2.009         {"mlm": 7.7278903926815, "mse": 0.0}  0.0637
0         train   76100/102467 481:48/166:56      2.017         {"mlm": 7.728951655712324, "mse": 0.0}  0.0638
0         train   76200/102467 482:24/166:17      2.024         {"mlm": 7.7200270158990385, "mse": 0.0}  0.0653
0         train   76300/102467 483:01/165:39      2.032         {"mlm": 7.722618040412363, "mse": 0.0}  0.0667
0         train   76400/102467 483:37/165:00      2.039         {"mlm": 7.7244625295739935, "mse": 0.0}  0.0619
0         train   76500/102467 484:13/164:21      2.047         {"mlm": 7.72226685729305, "mse": 0.0}  0.0669
0         train   76600/102467 484:49/163:43      2.054         {"mlm": 7.723036968927687, "mse": 0.0}  0.0671
0         train   76700/102467 485:25/163:04      2.061         {"mlm": 7.7231300899937985, "mse": 0.0}  0.0672
0         train   76800/102467 486:02/162:26      2.069         {"mlm": 7.723090295660002, "mse": 0.0}  0.0709
0         train   76900/102467 486:38/161:47      2.076         {"mlm": 7.722206702599158, "mse": 0.0}  0.0657
0         train   77000/102467 487:14/161:09      2.083         {"mlm": 7.720358745265031, "mse": 0.0}  0.0597
0         train   77100/102467 487:51/160:30      2.091         {"mlm": 7.722264788426806, "mse": 0.0}  0.0615
0         train   77200/102467 488:27/159:52      2.098         {"mlm": 7.72216496591082, "mse": 0.0}  0.0637
0         train   77300/102467 489:03/159:13      2.105         {"mlm": 7.723024814241008, "mse": 0.0}  0.069
0         train   77400/102467 489:39/158:35      2.113         {"mlm": 7.7219732826576974, "mse": 0.0}  0.061
0         train   77500/102467 490:16/157:56      2.120         {"mlm": 7.72244593996801, "mse": 0.0}  0.0573
0         train   77600/102467 490:52/157:18      2.127         {"mlm": 7.72204911387855, "mse": 0.0}  0.063
0         train   77700/102467 491:28/156:39      2.134         {"mlm": 7.721043111017752, "mse": 0.0}  0.0697
0         train   77800/102467 492:05/156:01      2.141         {"mlm": 7.719905188034558, "mse": 0.0}  0.0614
0         train   77900/102467 492:41/155:22      2.148         {"mlm": 7.718754101754743, "mse": 0.0}  0.0695
0         train   78000/102467 493:17/154:44      2.156         {"mlm": 7.718390369749571, "mse": 0.0}  0.0558
0         train   78100/102467 493:54/154:05      2.163         {"mlm": 7.724601715803146, "mse": 0.0}  0.0646
0         train   78200/102467 494:30/153:27      2.170         {"mlm": 7.726921463499264, "mse": 0.0}  0.0613
0         train   78300/102467 495:06/152:48      2.177         {"mlm": 7.72726571559906, "mse": 0.0}  0.0577
0         train   78400/102467 495:43/152:10      2.184         {"mlm": 7.725817517800764, "mse": 0.0}  0.0565
0         train   78500/102467 496:19/151:32      2.191         {"mlm": 7.726640610925613, "mse": 0.0}  0.0657
0         train   78600/102467 496:55/150:53      2.198         {"mlm": 7.727061583691795, "mse": 0.0}  0.0555
0         train   78700/102467 497:32/150:15      2.205         {"mlm": 7.727833884885942, "mse": 0.0}  0.0616
0         train   78800/102467 498:08/149:36      2.212         {"mlm": 7.727318211416503, "mse": 0.0}  0.0504
0         train   78900/102467 498:44/148:58      2.219         {"mlm": 7.7263812726097445, "mse": 0.0}  0.06
0         train   79000/102467 499:21/148:20      2.226         {"mlm": 7.724088917774369, "mse": 0.0}  0.0657
0         train   79100/102467 499:57/147:41      2.233         {"mlm": 7.724330755915955, "mse": 0.0}  0.0816
0         train   79200/102467 500:33/147:03      2.240         {"mlm": 7.723644735821114, "mse": 0.0}  0.0946
0         train   79300/102467 501:10/146:24      2.247         {"mlm": 7.722799181570242, "mse": 0.0}  0.0624
0         train   79400/102467 501:46/145:46      2.254         {"mlm": 7.7219356213053185, "mse": 0.0}  0.0648
0         train   79500/102467 502:22/145:08      2.261         {"mlm": 7.721431490253, "mse": 0.0}  0.0564
0         train   79600/102467 502:58/144:29      2.267         {"mlm": 7.721583713564956, "mse": 0.0}  0.0589
0         train   79700/102467 503:35/143:51      2.274         {"mlm": 7.720796542628756, "mse": 0.0}  0.0756
0         train   79800/102467 504:11/143:12      2.281         {"mlm": 7.721162521228493, "mse": 0.0}  0.0588
0         train   79900/102467 504:47/142:34      2.288         {"mlm": 7.720815117349101, "mse": 0.0}  0.0625
0         train   80000/102467 505:23/141:56      2.295         {"mlm": 7.720470605250112, "mse": 0.0}  0.0689

09/23/2022 16:29:28 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        0:46/609:24        7.649           None
0         valid   101/781      1:02/ 6:57         7.699           None
0         valid   201/781      1:17/ 3:43         7.717           None
0         valid   301/781      1:32/ 2:27         7.715           None
0         valid   401/781      1:47/ 1:42         7.723           None
0         valid   501/781      2:02/ 1:08         7.719           None
0         valid   601/781      2:18/ 0:41         7.720           None
0         valid   701/781      2:33/ 0:17         7.722           None
0         valid   781/781      2:48/ 0:00         7.721         {"mlm": 7.720870678617158, "mse": 0.0, "train": 0.0}  None
0         train   80100/102467 508:47/142:04      2.301         {"mlm": 7.716258735656738, "mse": 0.0}  0.0778
0         train   80200/102467 509:23/141:25      2.308         {"mlm": 7.714138894081116, "mse": 0.0}  0.0766
0         train   80300/102467 509:59/140:47      2.315         {"mlm": 7.7151264524459835, "mse": 0.0}  0.0602
0         train   80400/102467 510:35/140:08      2.322         {"mlm": 7.719587022066117, "mse": 0.0}  0.0571
0         train   80500/102467 511:11/139:29      2.328         {"mlm": 7.717536590576172, "mse": 0.0}  0.0542
0         train   80600/102467 511:47/138:51      2.335         {"mlm": 7.721356165409088, "mse": 0.0}  0.0577
0         train   80700/102467 512:23/138:12      2.342         {"mlm": 7.721138170787266, "mse": 0.0}  0.0614
0         train   80800/102467 512:59/137:33      2.348         {"mlm": 7.723726567029953, "mse": 0.0}  0.0702
0         train   80900/102467 513:35/136:55      2.355         {"mlm": 7.723261547088623, "mse": 0.0}  0.0579
0         train   81000/102467 514:11/136:16      2.362         {"mlm": 7.722942210674286, "mse": 0.0}  0.056
0         train   81100/102467 514:47/135:37      2.368         {"mlm": 7.723086510138078, "mse": 0.0}  0.0651
0         train   81200/102467 515:24/134:59      2.375         {"mlm": 7.722162938912709, "mse": 0.0}  0.05
0         train   81300/102467 516:00/134:20      2.381         {"mlm": 7.722047769839947, "mse": 0.0}  0.054
0         train   81400/102467 516:36/133:42      2.388         {"mlm": 7.721124052320208, "mse": 0.0}  0.0672
0         train   81500/102467 517:12/133:03      2.395         {"mlm": 7.721992532730103, "mse": 0.0}  0.0638
0         train   81600/102467 517:48/132:25      2.401         {"mlm": 7.722483010590077, "mse": 0.0}  0.0628
0         train   81700/102467 518:25/131:46      2.408         {"mlm": 7.721762379197513, "mse": 0.0}  0.063
0         train   81800/102467 519:01/131:07      2.414         {"mlm": 7.72177579190996, "mse": 0.0}  0.0596
0         train   81900/102467 519:37/130:29      2.421         {"mlm": 7.721961135111357, "mse": 0.0}  0.0532
0         train   82000/102467 520:14/129:50      2.427         {"mlm": 7.722074783086777, "mse": 0.0}  0.0584
0         train   82100/102467 520:50/129:12      2.434         {"mlm": 7.7293707915026735, "mse": 0.0}  0.0567
0         train   82200/102467 521:26/128:33      2.440         {"mlm": 7.733437207475979, "mse": 0.0}  0.054
0         train   82300/102467 522:03/127:55      2.446         {"mlm": 7.727936977526815, "mse": 0.0}  0.0591
0         train   82400/102467 522:39/127:17      2.453         {"mlm": 7.71958829765033, "mse": 0.0}  0.0513
0         train   82500/102467 523:15/126:38      2.459         {"mlm": 7.716588814416247, "mse": 0.0}  0.0541
0         train   82600/102467 523:52/126:00      2.466         {"mlm": 7.715549707014692, "mse": 0.0}  0.0734
0         train   82700/102467 524:28/125:21      2.472         {"mlm": 7.717143974249626, "mse": 0.0}  0.0551
0         train   82800/102467 525:04/124:43      2.478         {"mlm": 7.718066139722497, "mse": 0.0}  0.051
0         train   82900/102467 525:41/124:04      2.485         {"mlm": 7.7159503941010845, "mse": 0.0}  0.0702
0         train   83000/102467 526:17/123:26      2.491         {"mlm": 7.716005828406837, "mse": 0.0}  0.059
0         train   83100/102467 526:53/122:47      2.497         {"mlm": 7.717168292964557, "mse": 0.0}  0.0585
0         train   83200/102467 527:29/122:09      2.503         {"mlm": 7.71766862280673, "mse": 0.0}  0.0562
0         train   83300/102467 528:06/121:30      2.510         {"mlm": 7.715777158553642, "mse": 0.0}  0.0549
0         train   83400/102467 528:42/120:52      2.516         {"mlm": 7.717187086627516, "mse": 0.0}  0.0553
0         train   83500/102467 529:18/120:13      2.522         {"mlm": 7.717360693427067, "mse": 0.0}  0.0629
0         train   83600/102467 529:54/119:35      2.528         {"mlm": 7.71637486665975, "mse": 0.0}  0.0534
0         train   83700/102467 530:31/118:57      2.535         {"mlm": 7.715757083724427, "mse": 0.0}  0.0602
0         train   83800/102467 531:07/118:18      2.541         {"mlm": 7.715363095375218, "mse": 0.0}  0.0634
0         train   83900/102467 531:43/117:40      2.547         {"mlm": 7.715450064140348, "mse": 0.0}  0.0597
0         train   84000/102467 532:20/117:01      2.553         {"mlm": 7.7146165176532815, "mse": 0.0}  0.0638
0         train   84100/102467 532:56/116:23      2.559         {"mlm": 7.726837800473583, "mse": 0.0}  0.0512
0         train   84200/102467 533:32/115:45      2.565         {"mlm": 7.718829718503085, "mse": 0.0}  0.0519
0         train   84300/102467 534:09/115:06      2.571         {"mlm": 7.7214322122151415, "mse": 0.0}  0.0557
0         train   84400/102467 534:45/114:28      2.577         {"mlm": 7.7204109544131025, "mse": 0.0}  0.051
0         train   84500/102467 535:22/113:50      2.584         {"mlm": 7.721190593328821, "mse": 0.0}  0.0517
0         train   84600/102467 535:58/113:11      2.590         {"mlm": 7.719104101825319, "mse": 0.0}  0.0504
0         train   84700/102467 536:34/112:33      2.596         {"mlm": 7.724440838341043, "mse": 0.0}  0.0594
0         train   84800/102467 537:10/111:54      2.602         {"mlm": 7.727337429696755, "mse": 0.0}  0.0516
0         train   84900/102467 537:47/111:16      2.608         {"mlm": 7.726234333552338, "mse": 0.0}  0.0531
0         train   85000/102467 538:23/110:38      2.614         {"mlm": 7.724125644248091, "mse": 0.0}  0.0532
0         train   85100/102467 538:59/109:59      2.620         {"mlm": 7.723586174091138, "mse": 0.0}  0.0586
0         train   85200/102467 539:36/109:21      2.626         {"mlm": 7.72277919637142, "mse": 0.0}  0.0548
0         train   85300/102467 540:12/108:43      2.632         {"mlm": 7.724220530094094, "mse": 0.0}  0.0607
0         train   85400/102467 540:48/108:04      2.638         {"mlm": 7.723563182336919, "mse": 0.0}  0.0583
0         train   85500/102467 541:25/107:26      2.644         {"mlm": 7.723979636728366, "mse": 0.0}  0.0666
0         train   85600/102467 542:01/106:48      2.650         {"mlm": 7.723922477048986, "mse": 0.0}  0.0567
0         train   85700/102467 542:37/106:09      2.656         {"mlm": 7.723560429574182, "mse": 0.0}  0.0723
0         train   85800/102467 543:14/105:31      2.661         {"mlm": 7.722122386777494, "mse": 0.0}  0.0634
0         train   85900/102467 543:50/104:53      2.667         {"mlm": 7.7225811649800855, "mse": 0.0}  0.0567
0         train   86000/102467 544:26/104:14      2.673         {"mlm": 7.723105434898858, "mse": 0.0}  0.0733
0         train   86100/102467 545:03/103:36      2.679         {"mlm": 7.733527188448562, "mse": 0.0}  0.0498
0         train   86200/102467 545:39/102:58      2.685         {"mlm": 7.716938488374507, "mse": 0.0}  0.0577
0         train   86300/102467 546:15/102:20      2.691         {"mlm": 7.723994755985761, "mse": 0.0}  0.0546
0         train   86400/102467 546:52/101:41      2.697         {"mlm": 7.7302212763192975, "mse": 0.0}  0.0518
0         train   86500/102467 547:28/101:03      2.702         {"mlm": 7.7276423068593445, "mse": 0.0}  0.051
0         train   86600/102467 548:04/100:25      2.708         {"mlm": 7.7271570983643905, "mse": 0.0}  0.0616
0         train   86700/102467 548:40/99:46       2.714         {"mlm": 7.723959952208028, "mse": 0.0}  0.0552
0         train   86800/102467 549:16/99:08       2.720         {"mlm": 7.72220246914489, "mse": 0.0}  0.0632
0         train   86900/102467 549:53/98:30       2.726         {"mlm": 7.7228444874486, "mse": 0.0}  0.0531
0         train   87000/102467 550:29/97:52       2.731         {"mlm": 7.720719502945482, "mse": 0.0}  0.0517
0         train   87100/102467 551:05/97:13       2.737         {"mlm": 7.7242488426410185, "mse": 0.0}  0.0503
0         train   87200/102467 551:42/96:35       2.743         {"mlm": 7.722692519501038, "mse": 0.0}  0.0512
0         train   87300/102467 552:18/95:57       2.748         {"mlm": 7.7235654479462825, "mse": 0.0}  0.0643
0         train   87400/102467 552:54/95:19       2.754         {"mlm": 7.72243942801408, "mse": 0.0}  0.0527
0         train   87500/102467 553:30/94:40       2.760         {"mlm": 7.721971652630415, "mse": 0.0}  0.0479
0         train   87600/102467 554:07/94:02       2.765         {"mlm": 7.7213173494834635, "mse": 0.0}  0.0507
0         train   87700/102467 554:43/93:24       2.771         {"mlm": 7.721216739874836, "mse": 0.0}  0.0509
0         train   87800/102467 555:19/92:46       2.777         {"mlm": 7.721381080236844, "mse": 0.0}  0.049
0         train   87900/102467 555:55/92:07       2.782         {"mlm": 7.721825243487883, "mse": 0.0}  0.0537
0         train   88000/102467 556:32/91:29       2.788         {"mlm": 7.7214278525331945, "mse": 0.0}  0.0528
0         train   88100/102467 557:08/90:51       2.794         {"mlm": 7.724269847075145, "mse": 0.0}  0.0522
0         train   88200/102467 557:44/90:13       2.799         {"mlm": 7.725212170153248, "mse": 0.0}  0.0553
0         train   88300/102467 558:21/89:34       2.805         {"mlm": 7.724186742627943, "mse": 0.0}  0.0513
0         train   88400/102467 558:57/88:56       2.810         {"mlm": 7.7220618050507825, "mse": 0.0}  0.0573
0         train   88500/102467 559:33/88:18       2.816         {"mlm": 7.722183665921611, "mse": 0.0}  0.0462
0         train   88600/102467 560:09/87:40       2.821         {"mlm": 7.723944984026403, "mse": 0.0}  0.0572
0         train   88700/102467 560:46/87:02       2.827         {"mlm": 7.727788509308607, "mse": 0.0}  0.0525
0         train   88800/102467 561:22/86:24       2.832         {"mlm": 7.729764460918292, "mse": 0.0}  0.0782
0         train   88900/102467 561:58/85:45       2.838         {"mlm": 7.727682422314372, "mse": 0.0}  0.0543
0         train   89000/102467 562:35/85:07       2.843         {"mlm": 7.7267059509055205, "mse": 0.0}  0.0514
0         train   89100/102467 563:11/84:29       2.849         {"mlm": 7.726987715185124, "mse": 0.0}  0.0518
0         train   89200/102467 563:47/83:51       2.854         {"mlm": 7.72510854097513, "mse": 0.0}  0.0538
0         train   89300/102467 564:24/83:13       2.860         {"mlm": 7.725011815627416, "mse": 0.0}  0.0459
0         train   89400/102467 565:00/82:35       2.865         {"mlm": 7.7241461731301335, "mse": 0.0}  0.0551
0         train   89500/102467 565:36/81:56       2.871         {"mlm": 7.72400705317125, "mse": 0.0}  0.0451
0         train   89600/102467 566:13/81:18       2.876         {"mlm": 7.723991282004163, "mse": 0.0}  0.0514
0         train   89700/102467 566:49/80:40       2.881         {"mlm": 7.723350192859487, "mse": 0.0}  0.0513
0         train   89800/102467 567:25/80:02       2.887         {"mlm": 7.723668026499334, "mse": 0.0}  0.0592
0         train   89900/102467 568:01/79:24       2.892         {"mlm": 7.722566281692891, "mse": 0.0}  0.0646
0         train   90000/102467 568:38/78:46       2.898         {"mlm": 7.722250141457231, "mse": 0.0}  0.0468

09/23/2022 17:32:42 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        0:46/609:15        7.598           None
0         valid   101/781      1:02/ 6:58         7.714           None
0         valid   201/781      1:17/ 3:43         7.724           None
0         valid   301/781      1:32/ 2:27         7.726           None
0         valid   401/781      1:47/ 1:42         7.728           None
0         valid   501/781      2:03/ 1:08         7.724           None
0         valid   601/781      2:18/ 0:41         7.724           None
0         valid   701/781      2:33/ 0:17         7.724           None
0         valid   781/781      2:48/ 0:00         7.722         {"mlm": 7.7216709346991035, "mse": 0.0, "train": 0.0}  None
0         train   90100/102467 572:02/78:31       2.903         {"mlm": 7.732250866889953, "mse": 0.0}  0.0457
0         train   90200/102467 572:38/77:52       2.908         {"mlm": 7.7339762425422665, "mse": 0.0}  0.0509
0         train   90300/102467 573:14/77:14       2.914         {"mlm": 7.726671800613404, "mse": 0.0}  0.0489
0         train   90400/102467 573:49/76:35       2.919         {"mlm": 7.72519118309021, "mse": 0.0}  0.0508
0         train   90500/102467 574:25/75:57       2.924         {"mlm": 7.725704106330872, "mse": 0.0}  0.0464
0         train   90600/102467 575:01/75:19       2.930         {"mlm": 7.720276056130727, "mse": 0.0}  0.0548
0         train   90700/102467 575:37/74:40       2.935         {"mlm": 7.719453765324184, "mse": 0.0}  0.048
0         train   90800/102467 576:13/74:02       2.940         {"mlm": 7.719589913487434, "mse": 0.0}  0.0535
0         train   90900/102467 576:50/73:24       2.945         {"mlm": 7.7179703701867, "mse": 0.0}  0.0445
0         train   91000/102467 577:26/72:45       2.951         {"mlm": 7.718244822502136, "mse": 0.0}  0.0464
0         train   91100/102467 578:02/72:07       2.956         {"mlm": 7.718478811870922, "mse": 0.0}  0.0533
0         train   91200/102467 578:38/71:29       2.961         {"mlm": 7.719779051144918, "mse": 0.0}  0.0432
0         train   91300/102467 579:14/70:50       2.966         {"mlm": 7.7193837389579185, "mse": 0.0}  0.0661
0         train   91400/102467 579:50/70:12       2.971         {"mlm": 7.720839465686253, "mse": 0.0}  0.0438
0         train   91500/102467 580:27/69:34       2.977         {"mlm": 7.722150835355123, "mse": 0.0}  0.0621
0         train   91600/102467 581:03/68:56       2.982         {"mlm": 7.720697414278984, "mse": 0.0}  0.0565
0         train   91700/102467 581:39/68:17       2.987         {"mlm": 7.718988993308123, "mse": 0.0}  0.0512
0         train   91800/102467 582:15/67:39       2.992         {"mlm": 7.719185130066342, "mse": 0.0}  0.0481
0         train   91900/102467 582:52/67:01       2.997         {"mlm": 7.721557954738015, "mse": 0.0}  0.0467
0         train   92000/102467 583:28/66:22       3.002         {"mlm": 7.7221551368236545, "mse": 0.0}  0.0586
0         train   92100/102467 584:04/65:44       3.008         {"mlm": 7.722951181007154, "mse": 0.0}  0.0546
0         train   92200/102467 584:41/65:06       3.013         {"mlm": 7.726094420830808, "mse": 0.0}  0.0482
0         train   92300/102467 585:17/64:28       3.018         {"mlm": 7.724969085642327, "mse": 0.0}  0.0428
0         train   92400/102467 585:53/63:50       3.023         {"mlm": 7.716383769099874, "mse": 0.0}  0.0454
0         train   92500/102467 586:30/63:11       3.028         {"mlm": 7.718718207670835, "mse": 0.0}  0.0455
0         train   92600/102467 587:06/62:33       3.033         {"mlm": 7.716111425167332, "mse": 0.0}  0.0471
0         train   92700/102467 587:42/61:55       3.038         {"mlm": 7.717787710552734, "mse": 0.0}  0.0544
0         train   92800/102467 588:18/61:17       3.043         {"mlm": 7.719231550028089, "mse": 0.0}  0.0495
0         train   92900/102467 588:55/60:38       3.048         {"mlm": 7.721573528909312, "mse": 0.0}  0.0503
0         train   93000/102467 589:31/60:00       3.053         {"mlm": 7.723382946965215, "mse": 0.0}  0.0509
0         train   93100/102467 590:07/59:22       3.058         {"mlm": 7.723581571813276, "mse": 0.0}  0.0491
0         train   93200/102467 590:43/58:44       3.063         {"mlm": 7.71962303315927, "mse": 0.0}  0.0521
0         train   93300/102467 591:20/58:06       3.068         {"mlm": 7.717126423804186, "mse": 0.0}  0.0518
0         train   93400/102467 591:56/57:27       3.073         {"mlm": 7.718044316453368, "mse": 0.0}  0.0496
0         train   93500/102467 592:32/56:49       3.078         {"mlm": 7.71752966142162, "mse": 0.0}  0.0645
0         train   93600/102467 593:08/56:11       3.083         {"mlm": 7.718390825019918, "mse": 0.0}  0.0441
0         train   93700/102467 593:45/55:33       3.088         {"mlm": 7.719545008225467, "mse": 0.0}  0.0494
0         train   93800/102467 594:21/54:55       3.093         {"mlm": 7.719596405304956, "mse": 0.0}  0.0564
0         train   93900/102467 594:57/54:16       3.098         {"mlm": 7.719093392057253, "mse": 0.0}  0.0446
0         train   94000/102467 595:34/53:38       3.103         {"mlm": 7.719613245810909, "mse": 0.0}  0.0476
0         train   94100/102467 596:10/53:00       3.108         {"mlm": 7.721756200401151, "mse": 0.0}  0.0448
0         train   94200/102467 596:46/52:22       3.113         {"mlm": 7.723868574758972, "mse": 0.0}  0.05
0         train   94300/102467 597:22/51:44       3.118         {"mlm": 7.717059669878659, "mse": 0.0}  0.0471
0         train   94400/102467 597:58/51:06       3.122         {"mlm": 7.719895612055333, "mse": 0.0}  0.0512
0         train   94500/102467 598:35/50:27       3.127         {"mlm": 7.719123926507422, "mse": 0.0}  0.0523
0         train   94600/102467 599:11/49:49       3.132         {"mlm": 7.721305010709475, "mse": 0.0}  0.0535
0         train   94700/102467 599:47/49:11       3.137         {"mlm": 7.721207409670154, "mse": 0.0}  0.0518
0         train   94800/102467 600:23/48:33       3.142         {"mlm": 7.720636799221947, "mse": 0.0}  0.0475
0         train   94900/102467 601:00/47:55       3.147         {"mlm": 7.72086816057065, "mse": 0.0}  0.0513
0         train   95000/102467 601:36/47:17       3.151         {"mlm": 7.719061295828504, "mse": 0.0}  0.0472
0         train   95100/102467 602:13/46:39       3.156         {"mlm": 7.7181136938175, "mse": 0.0}  0.0457
0         train   95200/102467 602:49/46:00       3.161         {"mlm": 7.717986141102939, "mse": 0.0}  0.0554
0         train   95300/102467 603:25/45:22       3.166         {"mlm": 7.715991157596394, "mse": 0.0}  0.0473
0         train   95400/102467 604:01/44:44       3.171         {"mlm": 7.716369491107815, "mse": 0.0}  0.0404
0         train   95500/102467 604:38/44:06       3.175         {"mlm": 7.717222674348166, "mse": 0.0}  0.0453
0         train   95600/102467 605:14/43:28       3.180         {"mlm": 7.717290766994108, "mse": 0.0}  0.0526
0         train   95700/102467 605:50/42:50       3.185         {"mlm": 7.7166370387914185, "mse": 0.0}  0.0416
0         train   95800/102467 606:26/42:12       3.190         {"mlm": 7.7170928839979505, "mse": 0.0}  0.0648
0         train   95900/102467 607:03/41:34       3.194         {"mlm": 7.718090302574873, "mse": 0.0}  0.0523
0         train   96000/102467 607:39/40:56       3.199         {"mlm": 7.7183396792387935, "mse": 0.0}  0.0471
0         train   96100/102467 608:15/40:17       3.204         {"mlm": 7.739220466810403, "mse": 0.0}  0.0494
0         train   96200/102467 608:52/39:39       3.208         {"mlm": 7.72836323801031, "mse": 0.0}  0.0518
0         train   96300/102467 609:28/39:01       3.213         {"mlm": 7.724184331669149, "mse": 0.0}  0.0518
0         train   96400/102467 610:04/38:23       3.218         {"mlm": 7.721357280721592, "mse": 0.0}  0.0422
0         train   96500/102467 610:40/37:45       3.222         {"mlm": 7.718089711018491, "mse": 0.0}  0.0462
0         train   96600/102467 611:17/37:07       3.227         {"mlm": 7.715267889064361, "mse": 0.0}  0.0457
0         train   96700/102467 611:53/36:29       3.232         {"mlm": 7.714669012099804, "mse": 0.0}  0.0493
0         train   96800/102467 612:29/35:51       3.236         {"mlm": 7.714186715660909, "mse": 0.0}  0.0463
0         train   96900/102467 613:05/35:13       3.241         {"mlm": 7.714753335933621, "mse": 0.0}  0.0427
0         train   97000/102467 613:42/34:35       3.246         {"mlm": 7.713824820733716, "mse": 0.0}  0.0533
0         train   97100/102467 614:18/33:57       3.250         {"mlm": 7.713330568784785, "mse": 0.0}  0.0483
0         train   97200/102467 614:54/33:19       3.255         {"mlm": 7.714280377852488, "mse": 0.0}  0.0429
0         train   97300/102467 615:30/32:41       3.259         {"mlm": 7.715452090537631, "mse": 0.0}  0.0519
0         train   97400/102467 616:06/32:03       3.264         {"mlm": 7.715400159913639, "mse": 0.0}  0.0518
0         train   97500/102467 616:43/31:25       3.268         {"mlm": 7.714543077893152, "mse": 0.0}  0.047
0         train   97600/102467 617:19/30:47       3.273         {"mlm": 7.714122325237946, "mse": 0.0}  0.0463
0         train   97700/102467 617:55/30:08       3.278         {"mlm": 7.714055535086619, "mse": 0.0}  0.0561
0         train   97800/102467 618:31/29:30       3.282         {"mlm": 7.714708856297123, "mse": 0.0}  0.0495
0         train   97900/102467 619:07/28:52       3.287         {"mlm": 7.715087243865902, "mse": 0.0}  0.0527
0         train   98000/102467 619:44/28:14       3.291         {"mlm": 7.714872032627799, "mse": 0.0}  0.0476
0         train   98100/102467 620:20/27:36       3.296         {"mlm": 7.740908518433571, "mse": 0.0}  0.0492
0         train   98200/102467 620:56/26:58       3.300         {"mlm": 7.72220944141855, "mse": 0.0}  0.0478
0         train   98300/102467 621:33/26:20       3.305         {"mlm": 7.721243473323616, "mse": 0.0}  0.0503
0         train   98400/102467 622:09/25:42       3.309         {"mlm": 7.72312273521616, "mse": 0.0}  0.0588
0         train   98500/102467 622:45/25:04       3.314         {"mlm": 7.719516807986844, "mse": 0.0}  0.0412
0         train   98600/102467 623:22/24:26       3.318         {"mlm": 7.718853533668006, "mse": 0.0}  0.0423
0         train   98700/102467 623:58/23:48       3.323         {"mlm": 7.718997436008234, "mse": 0.0}  0.0409
0         train   98800/102467 624:34/23:10       3.327         {"mlm": 7.71748242725679, "mse": 0.0}  0.0567
0         train   98900/102467 625:10/22:32       3.331         {"mlm": 7.715074426361492, "mse": 0.0}  0.0461
0         train   99000/102467 625:47/21:54       3.336         {"mlm": 7.717823183680156, "mse": 0.0}  0.0548
0         train   99100/102467 626:23/21:16       3.340         {"mlm": 7.718910480937819, "mse": 0.0}  0.0469
0         train   99200/102467 626:59/20:38       3.345         {"mlm": 7.718110101278809, "mse": 0.0}  0.0456
0         train   99300/102467 627:36/20:00       3.349         {"mlm": 7.716874997189016, "mse": 0.0}  0.0488
0         train   99400/102467 628:12/19:23       3.354         {"mlm": 7.717017624644632, "mse": 0.0}  0.0498
0         train   99500/102467 628:48/18:45       3.358         {"mlm": 7.717153868573234, "mse": 0.0}  0.0458
0         train   99600/102467 629:25/18:07       3.362         {"mlm": 7.717236549035649, "mse": 0.0}  0.0488
0         train   99700/102467 630:01/17:29       3.367         {"mlm": 7.716984930465806, "mse": 0.0}  0.0435
0         train   99800/102467 630:37/16:51       3.371         {"mlm": 7.717167607925517, "mse": 0.0}  0.0536
0         train   99900/102467 631:13/16:13       3.375         {"mlm": 7.716966200729965, "mse": 0.0}  0.0428
0         train   100000/102467 631:50/15:35      3.380         {"mlm": 7.7179017630751, "mse": 0.0}  0.0435

09/23/2022 18:35:54 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0.pkl
0         valid   1/781        0:44/581:28        7.691           None
0         valid   101/781      1:00/ 6:44         7.707           None
0         valid   201/781      1:15/ 3:37         7.708           None
0         valid   301/781      1:30/ 2:24         7.710           None
0         valid   401/781      1:45/ 1:40         7.716           None
0         valid   501/781      2:01/ 1:07         7.715           None
0         valid   601/781      2:16/ 0:40         7.716           None
0         valid   701/781      2:31/ 0:17         7.717           None
0         valid   781/781      2:46/ 0:00         7.719         {"mlm": 7.719110115236876, "mse": 0.0, "train": 0.0}  None
0         train   100100/102467 635:12/15:01      3.384         {"mlm": 7.720750150680542, "mse": 0.0}  0.0524
0         train   100200/102467 635:48/14:23      3.388         {"mlm": 7.718059976100921, "mse": 0.0}  0.0495
0         train   100300/102467 636:24/13:44      3.393         {"mlm": 7.713513000806173, "mse": 0.0}  0.0406
0         train   100400/102467 636:59/13:06      3.397         {"mlm": 7.708906008005142, "mse": 0.0}  0.0462
0         train   100500/102467 637:35/12:28      3.401         {"mlm": 7.714183953285217, "mse": 0.0}  0.0481
0         train   100600/102467 638:11/11:50      3.406         {"mlm": 7.715649167696635, "mse": 0.0}  0.0432
0         train   100700/102467 638:47/11:12      3.410         {"mlm": 7.712168139048985, "mse": 0.0}  0.0464
0         train   100800/102467 639:23/10:34      3.414         {"mlm": 7.714654623270035, "mse": 0.0}  0.0416
0         train   100900/102467 640:00/ 9:56      3.418         {"mlm": 7.715737815433078, "mse": 0.0}  0.0436
0         train   101000/102467 640:36/ 9:18      3.423         {"mlm": 7.71819585609436, "mse": 0.0}  0.0453
0         train   101100/102467 641:12/ 8:40      3.427         {"mlm": 7.717695354981856, "mse": 0.0}  0.0404
0         train   101200/102467 641:48/ 8:02      3.431         {"mlm": 7.717337819735209, "mse": 0.0}  0.053
0         train   101300/102467 642:24/ 7:24      3.435         {"mlm": 7.717219368861271, "mse": 0.0}  0.0678
0         train   101400/102467 643:00/ 6:45      3.440         {"mlm": 7.717259770802089, "mse": 0.0}  0.0446
0         train   101500/102467 643:36/ 6:07      3.444         {"mlm": 7.716483562787374, "mse": 0.0}  0.0433
0         train   101600/102467 644:13/ 5:29      3.448         {"mlm": 7.716460517644882, "mse": 0.0}  0.0532
0         train   101700/102467 644:49/ 4:51      3.452         {"mlm": 7.715791896090788, "mse": 0.0}  0.041
0         train   101800/102467 645:25/ 4:13      3.456         {"mlm": 7.716394125885434, "mse": 0.0}  0.0487
0         train   101900/102467 646:01/ 3:35      3.461         {"mlm": 7.7168815366845385, "mse": 0.0}  0.0476
0         train   102000/102467 646:38/ 2:57      3.465         {"mlm": 7.717601733446121, "mse": 0.0}  0.0597
0         train   102100/102467 647:14/ 2:19      3.469         {"mlm": 7.739023651739563, "mse": 0.0}  0.0393
0         train   102200/102467 647:50/ 1:41      3.473         {"mlm": 7.738991814043055, "mse": 0.0}  0.0457
0         train   102300/102467 648:26/ 1:03      3.477         {"mlm": 7.737323753012463, "mse": 0.0}  0.0415
0         train   102400/102467 649:02/ 0:25      3.481         {"mlm": 7.734701842592473, "mse": 0.0}  0.0539
True False
skip validation
1         train   100/102467   0:50/868:43        7.718         {"mlm": 7.717727065086365, "mse": 0.0}  0.0511
1         train   200/102467   1:26/740:08        7.718         {"mlm": 7.717610943317413, "mse": 0.0}  0.0443
1         train   300/102467   2:02/697:32        7.720         {"mlm": 7.720360851287841, "mse": 0.0}  0.0436
1         train   400/102467   2:38/675:56        7.724         {"mlm": 7.7241584122180935, "mse": 0.0}  0.0589
1         train   500/102467   3:15/663:06        7.723         {"mlm": 7.722954455375671, "mse": 0.0}  0.0435
1         train   600/102467   3:51/654:05        7.720         {"mlm": 7.7203835709889725, "mse": 0.0}  0.0427
1         train   700/102467   4:27/647:54        7.721         {"mlm": 7.721279237610953, "mse": 0.0}  0.0456
1         train   800/102467   5:03/642:49        7.720         {"mlm": 7.720365685224533, "mse": 0.0}  0.0503
1         train   900/102467   5:39/638:51        7.721         {"mlm": 7.720567341380649, "mse": 0.0}  0.0386
1         train   1000/102467  6:15/635:37        7.719         {"mlm": 7.718616082668304, "mse": 0.0}  0.0389
1         train   1100/102467  6:51/632:44        7.717         {"mlm": 7.717138435623863, "mse": 0.0}  0.0429
1         train   1200/102467  7:28/630:26        7.717         {"mlm": 7.717336802879969, "mse": 0.0}  0.0444
1         train   1300/102467  8:04/628:27        7.720         {"mlm": 7.7203754237981945, "mse": 0.0}  0.0529
1         train   1400/102467  8:40/626:37        7.720         {"mlm": 7.720426685810089, "mse": 0.0}  0.0413
1         train   1500/102467  9:17/624:57        7.720         {"mlm": 7.71995936425527, "mse": 0.0}  0.0589
1         train   1600/102467  9:53/623:24        7.718         {"mlm": 7.718217224180698, "mse": 0.0}  0.0415
1         train   1700/102467 10:29/621:59        7.719         {"mlm": 7.71878447448506, "mse": 0.0}  0.0474
1         train   1800/102467 11:05/620:41        7.720         {"mlm": 7.719642884996202, "mse": 0.0}  0.049
1         train   1900/102467 11:42/619:30        7.720         {"mlm": 7.719949849028336, "mse": 0.0}  0.0431
1         train   2000/102467 12:18/618:16        7.720         {"mlm": 7.720088146686554, "mse": 0.0}  0.0463
1         train   2100/102467 12:54/617:11        7.720         {"mlm": 7.723305143491186, "mse": 0.0}  0.0543
1         train   2200/102467 13:31/616:09        7.720         {"mlm": 7.71710749487182, "mse": 0.0}  0.0423
1         train   2300/102467 14:07/615:03        7.720         {"mlm": 7.723154249797298, "mse": 0.0}  0.049
1         train   2400/102467 14:43/614:03        7.720         {"mlm": 7.720883891696022, "mse": 0.0}  0.0424
1         train   2500/102467 15:19/613:05        7.721         {"mlm": 7.722612725947807, "mse": 0.0}  0.0515
1         train   2600/102467 15:56/612:13        7.720         {"mlm": 7.720755415488165, "mse": 0.0}  0.0437
1         train   2700/102467 16:32/611:21        7.720         {"mlm": 7.719805109654374, "mse": 0.0}  0.0452
1         train   2800/102467 17:09/610:28        7.720         {"mlm": 7.719046928109752, "mse": 0.0}  0.0457
1         train   2900/102467 17:45/609:38        7.719         {"mlm": 7.717464148401551, "mse": 0.0}  0.0396
1         train   3000/102467 18:21/608:48        7.719         {"mlm": 7.718079840934074, "mse": 0.0}  0.0422
1         train   3100/102467 18:58/607:57        7.719         {"mlm": 7.717547907408418, "mse": 0.0}  0.0453
1         train   3200/102467 19:34/607:07        7.719         {"mlm": 7.718310848885124, "mse": 0.0}  0.0463
1         train   3300/102467 20:10/606:19        7.720         {"mlm": 7.7199049407101485, "mse": 0.0}  0.0465
1         train   3400/102467 20:46/605:32        7.719         {"mlm": 7.718111923714039, "mse": 0.0}  0.0459
1         train   3500/102467 21:23/604:43        7.719         {"mlm": 7.717746170303518, "mse": 0.0}  0.0405
1         train   3600/102467 21:59/603:53        7.718         {"mlm": 7.7159177557687, "mse": 0.0}  0.0454
1         train   3700/102467 22:35/603:05        7.718         {"mlm": 7.716121612119983, "mse": 0.0}  0.0387
1         train   3800/102467 23:11/602:20        7.718         {"mlm": 7.716569279219589, "mse": 0.0}  0.0525
1         train   3900/102467 23:48/601:36        7.718         {"mlm": 7.715707111760401, "mse": 0.0}  0.0397
1         train   4000/102467 24:24/600:50        7.718         {"mlm": 7.716128140345044, "mse": 0.0}  0.0421
1         train   4100/102467 25:00/600:03        7.718         {"mlm": 7.706808425942246, "mse": 0.0}  0.0391
1         train   4200/102467 25:36/599:19        7.718         {"mlm": 7.724194478506994, "mse": 0.0}  0.0461
1         train   4300/102467 26:13/598:34        7.718         {"mlm": 7.719337672995241, "mse": 0.0}  0.0546
1         train   4400/102467 26:49/597:52        7.718         {"mlm": 7.71860690931579, "mse": 0.0}  0.0416
1         train   4500/102467 27:25/597:07        7.718         {"mlm": 7.720976550895047, "mse": 0.0}  0.0452
1         train   4600/102467 28:01/596:21        7.718         {"mlm": 7.718041236584003, "mse": 0.0}  0.0423
1         train   4700/102467 28:38/595:38        7.718         {"mlm": 7.719611344842993, "mse": 0.0}  0.0453
1         train   4800/102467 29:14/594:55        7.718         {"mlm": 7.715839133824322, "mse": 0.0}  0.04
1         train   4900/102467 29:50/594:13        7.718         {"mlm": 7.716232970987503, "mse": 0.0}  0.0454
1         train   5000/102467 30:26/593:31        7.718         {"mlm": 7.717264879681543, "mse": 0.0}  0.0492
1         train   5100/102467 31:03/592:49        7.718         {"mlm": 7.717321310755551, "mse": 0.0}  0.046
1         train   5200/102467 31:39/592:08        7.718         {"mlm": 7.718269787567087, "mse": 0.0}  0.0404
1         train   5300/102467 32:15/591:26        7.718         {"mlm": 7.71860233330396, "mse": 0.0}  0.0405
1         train   5400/102467 32:51/590:46        7.718         {"mlm": 7.717438756822687, "mse": 0.0}  0.0443
1         train   5500/102467 33:28/590:05        7.718         {"mlm": 7.719084812579391, "mse": 0.0}  0.0401
1         train   5600/102467 34:04/589:26        7.718         {"mlm": 7.718276405215114, "mse": 0.0}  0.043
1         train   5700/102467 34:40/588:47        7.718         {"mlm": 7.719331564976273, "mse": 0.0}  0.0451
1         train   5800/102467 35:17/588:07        7.718         {"mlm": 7.718214215638242, "mse": 0.0}  0.045
1         train   5900/102467 35:53/587:25        7.718         {"mlm": 7.718644689835286, "mse": 0.0}  0.045
1         train   6000/102467 36:29/586:46        7.718         {"mlm": 7.719270853905587, "mse": 0.0}  0.0401
1         train   6100/102467 37:06/586:06        7.719         {"mlm": 7.723526310674923, "mse": 0.0}  0.0503
1         train   6200/102467 37:42/585:27        7.719         {"mlm": 7.722525100417549, "mse": 0.0}  0.048
1         train   6300/102467 38:18/584:47        7.718         {"mlm": 7.716396169630365, "mse": 0.0}  0.0473
1         train   6400/102467 38:54/584:07        7.718         {"mlm": 7.715936037395102, "mse": 0.0}  0.0447
1         train   6500/102467 39:31/583:27        7.718         {"mlm": 7.717177851579079, "mse": 0.0}  0.0418
1         train   6600/102467 40:07/582:48        7.718         {"mlm": 7.716184133660653, "mse": 0.0}  0.0423
1         train   6700/102467 40:43/582:10        7.718         {"mlm": 7.716354231923348, "mse": 0.0}  0.0415
1         train   6800/102467 41:20/581:31        7.718         {"mlm": 7.7159417366592615, "mse": 0.0}  0.0438
1         train   6900/102467 41:56/580:51        7.718         {"mlm": 7.714199760951592, "mse": 0.0}  0.0341
1         train   7000/102467 42:32/580:11        7.718         {"mlm": 7.713244912617185, "mse": 0.0}  0.0428
1         train   7100/102467 43:08/579:31        7.718         {"mlm": 7.712815120856981, "mse": 0.0}  0.0506
1         train   7200/102467 43:45/578:52        7.718         {"mlm": 7.713086240572439, "mse": 0.0}  0.0482
1         train   7300/102467 44:21/578:12        7.718         {"mlm": 7.713744000645905, "mse": 0.0}  0.0462
1         train   7400/102467 44:57/577:33        7.718         {"mlm": 7.714267215305512, "mse": 0.0}  0.0423
1         train   7500/102467 45:33/576:55        7.718         {"mlm": 7.715660646269141, "mse": 0.0}  0.0411
1         train   7600/102467 46:09/576:15        7.718         {"mlm": 7.715032240414366, "mse": 0.0}  0.0559
1         train   7700/102467 46:46/575:37        7.718         {"mlm": 7.714554468041388, "mse": 0.0}  0.0441
1         train   7800/102467 47:22/574:59        7.718         {"mlm": 7.714726808406806, "mse": 0.0}  0.0479
1         train   7900/102467 47:58/574:21        7.718         {"mlm": 7.714604437068442, "mse": 0.0}  0.0405
1         train   8000/102467 48:35/573:42        7.718         {"mlm": 7.715440680638038, "mse": 0.0}  0.0477
1         train   8100/102467 49:11/573:04        7.718         {"mlm": 7.712997143467267, "mse": 0.0}  0.0525
1         train   8200/102467 49:47/572:25        7.717         {"mlm": 7.703205293538619, "mse": 0.0}  0.0437
1         train   8300/102467 50:23/571:46        7.717         {"mlm": 7.7116169446223495, "mse": 0.0}  0.0427
1         train   8400/102467 51:00/571:08        7.718         {"mlm": 7.7172324970515085, "mse": 0.0}  0.0478
1         train   8500/102467 51:36/570:29        7.718         {"mlm": 7.723014270105669, "mse": 0.0}  0.0501
1         train   8600/102467 52:12/569:50        7.718         {"mlm": 7.718097805176805, "mse": 0.0}  0.0413
1         train   8700/102467 52:48/569:11        7.718         {"mlm": 7.717886026563315, "mse": 0.0}  0.0477
1         train   8800/102467 53:24/568:32        7.718         {"mlm": 7.719585164707509, "mse": 0.0}  0.0635
1         train   8900/102467 54:01/567:53        7.718         {"mlm": 7.717848132231405, "mse": 0.0}  0.0373
1         train   9000/102467 54:37/567:14        7.718         {"mlm": 7.717742576656572, "mse": 0.0}  0.0453
1         train   9100/102467 55:13/566:35        7.718         {"mlm": 7.717615089277281, "mse": 0.0}  0.0416
1         train   9200/102467 55:49/565:57        7.718         {"mlm": 7.717030783959456, "mse": 0.0}  0.0413
1         train   9300/102467 56:25/565:19        7.718         {"mlm": 7.717723531855477, "mse": 0.0}  0.0411
1         train   9400/102467 57:02/564:42        7.718         {"mlm": 7.717739850197275, "mse": 0.0}  0.0371
1         train   9500/102467 57:38/564:03        7.718         {"mlm": 7.71756684142638, "mse": 0.0}  0.0467
1         train   9600/102467 58:14/563:27        7.718         {"mlm": 7.718241567599743, "mse": 0.0}  0.0424
1         train   9700/102467 58:51/562:49        7.718         {"mlm": 7.718910678658846, "mse": 0.0}  0.0496
1         train   9800/102467 59:27/562:12        7.718         {"mlm": 7.719424448724844, "mse": 0.0}  0.0433
1         train   9900/102467 60:03/561:33        7.718         {"mlm": 7.7184097993222975, "mse": 0.0}  0.0438
1         train   10000/102467 60:39/560:56       7.718         {"mlm": 7.718118242366997, "mse": 0.0}  0.042

09/23/2022 19:54:12 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1.pkl
1         valid   1/781        0:38/496:47        7.766           None
1         valid   101/781      0:53/ 5:59         7.716           None
1         valid   201/781      1:08/ 3:18         7.717           None
1         valid   301/781      1:24/ 2:14         7.721           None
1         valid   401/781      1:39/ 1:34         7.728           None
1         valid   501/781      1:54/ 1:04         7.725           None
1         valid   601/781      2:09/ 0:38         7.726           None
1         valid   701/781      2:25/ 0:16         7.724           None
1         valid   781/781      2:39/ 0:00         7.720         {"mlm": 7.719430217669654, "mse": 0.0, "train": 0.0}  None
1         train   10100/102467 63:56/584:41       7.718         {"mlm": 7.731506805419922, "mse": 0.0}  0.0429
1         train   10200/102467 64:31/583:44       7.718         {"mlm": 7.728757236003876, "mse": 0.0}  0.0529
1         train   10300/102467 65:07/582:47       7.718         {"mlm": 7.71982434908549, "mse": 0.0}  0.038
1         train   10400/102467 65:43/581:52       7.718         {"mlm": 7.723016840219498, "mse": 0.0}  0.0458
1         train   10500/102467 66:19/580:57       7.718         {"mlm": 7.720658243179321, "mse": 0.0}  0.0379
1         train   10600/102467 66:55/580:02       7.718         {"mlm": 7.719213593800863, "mse": 0.0}  0.0374
1         train   10700/102467 67:31/579:09       7.718         {"mlm": 7.720195123127529, "mse": 0.0}  0.0384
1         train   10800/102467 68:07/578:16       7.718         {"mlm": 7.721046171784401, "mse": 0.0}  0.0385
1         train   10900/102467 68:43/577:23       7.718         {"mlm": 7.71878972477383, "mse": 0.0}  0.0388
1         train   11000/102467 69:20/576:31       7.718         {"mlm": 7.7167370376586915, "mse": 0.0}  0.039
1         train   11100/102467 69:56/575:39       7.717         {"mlm": 7.714255466027693, "mse": 0.0}  0.0409
1         train   11200/102467 70:32/574:49       7.717         {"mlm": 7.715042319695155, "mse": 0.0}  0.0447
1         train   11300/102467 71:08/573:59       7.717         {"mlm": 7.714994480426495, "mse": 0.0}  0.0353
1         train   11400/102467 71:44/573:08       7.718         {"mlm": 7.716992394924164, "mse": 0.0}  0.0367
1         train   11500/102467 72:21/572:18       7.718         {"mlm": 7.715783132870992, "mse": 0.0}  0.0464
1         train   11600/102467 72:57/571:28       7.718         {"mlm": 7.715939035117626, "mse": 0.0}  0.0445
1         train   11700/102467 73:33/570:38       7.718         {"mlm": 7.716557876923505, "mse": 0.0}  0.039
1         train   11800/102467 74:09/569:49       7.718         {"mlm": 7.7172169078720945, "mse": 0.0}  0.0408
1         train   11900/102467 74:45/569:00       7.718         {"mlm": 7.717635173797607, "mse": 0.0}  0.0501
1         train   12000/102467 75:22/568:11       7.718         {"mlm": 7.717903775453568, "mse": 0.0}  0.0389
1         train   12100/102467 75:58/567:23       7.718         {"mlm": 7.702093504896068, "mse": 0.0}  0.0379
1         train   12200/102467 76:34/566:35       7.718         {"mlm": 7.712344035431368, "mse": 0.0}  0.0397
1         train   12300/102467 77:10/565:47       7.718         {"mlm": 7.716476571201082, "mse": 0.0}  0.0423
1         train   12400/102467 77:47/564:59       7.718         {"mlm": 7.720589636561267, "mse": 0.0}  0.0472
1         train   12500/102467 78:23/564:11       7.718         {"mlm": 7.723770825800772, "mse": 0.0}  0.0389
1         train   12600/102467 78:59/563:24       7.718         {"mlm": 7.721851715858472, "mse": 0.0}  0.0337
1         train   12700/102467 79:35/562:37       7.718         {"mlm": 7.720889419615695, "mse": 0.0}  0.0385
1         train   12800/102467 80:12/561:50       7.718         {"mlm": 7.71622961304513, "mse": 0.0}  0.0398
1         train   12900/102467 80:48/561:04       7.718         {"mlm": 7.71442949891223, "mse": 0.0}  0.0456
1         train   13000/102467 81:24/560:18       7.718         {"mlm": 7.714083895430312, "mse": 0.0}  0.0409
1         train   13100/102467 82:01/559:32       7.718         {"mlm": 7.715749863823291, "mse": 0.0}  0.0385
1         train   13200/102467 82:37/558:46       7.717         {"mlm": 7.7133390428226525, "mse": 0.0}  0.0414
1         train   13300/102467 83:13/557:59       7.717         {"mlm": 7.713179730377902, "mse": 0.0}  0.0392
1         train   13400/102467 83:50/557:14       7.717         {"mlm": 7.714448921334497, "mse": 0.0}  0.0379
1         train   13500/102467 84:26/556:28       7.718         {"mlm": 7.716533101980809, "mse": 0.0}  0.0419
1         train   13600/102467 85:02/555:42       7.718         {"mlm": 7.7170076817553666, "mse": 0.0}  0.0411
1         train   13700/102467 85:39/554:57       7.718         {"mlm": 7.717948003121164, "mse": 0.0}  0.0416
1         train   13800/102467 86:15/554:12       7.718         {"mlm": 7.7168000935845535, "mse": 0.0}  0.0446
1         train   13900/102467 86:51/553:27       7.718         {"mlm": 7.717699479278355, "mse": 0.0}  0.0404
1         train   14000/102467 87:28/552:42       7.718         {"mlm": 7.717740059376001, "mse": 0.0}  0.0411
1         train   14100/102467 88:04/551:56       7.718         {"mlm": 7.718414710492504, "mse": 0.0}  0.0443
1         train   14200/102467 88:40/551:11       7.718         {"mlm": 7.7154840411561905, "mse": 0.0}  0.0399
1         train   14300/102467 89:16/550:26       7.718         {"mlm": 7.718326544601645, "mse": 0.0}  0.047
1         train   14400/102467 89:53/549:42       7.718         {"mlm": 7.714586329819569, "mse": 0.0}  0.0419
1         train   14500/102467 90:29/548:58       7.718         {"mlm": 7.715714827120065, "mse": 0.0}  0.0453
1         train   14600/102467 91:05/548:14       7.718         {"mlm": 7.716042939635822, "mse": 0.0}  0.0447
1         train   14700/102467 91:41/547:29       7.718         {"mlm": 7.717530046288127, "mse": 0.0}  0.0481
1         train   14800/102467 92:18/546:45       7.718         {"mlm": 7.716650249366474, "mse": 0.0}  0.0389
1         train   14900/102467 92:54/546:00       7.718         {"mlm": 7.715149688826903, "mse": 0.0}  0.0385
1         train   15000/102467 93:30/545:17       7.718         {"mlm": 7.716592757639761, "mse": 0.0}  0.0358
1         train   15100/102467 94:07/544:33       7.718         {"mlm": 7.716168118739172, "mse": 0.0}  0.0439
1         train   15200/102467 94:43/543:49       7.718         {"mlm": 7.717216964954128, "mse": 0.0}  0.0394
1         train   15300/102467 95:19/543:04       7.718         {"mlm": 7.718385679145072, "mse": 0.0}  0.0466
1         train   15400/102467 95:55/542:20       7.718         {"mlm": 7.71899039994323, "mse": 0.0}  0.0428
1         train   15500/102467 96:32/541:37       7.718         {"mlm": 7.716965722146435, "mse": 0.0}  0.0355
1         train   15600/102467 97:08/540:54       7.718         {"mlm": 7.717382673029607, "mse": 0.0}  0.0433
1         train   15700/102467 97:44/540:10       7.718         {"mlm": 7.7173672635086294, "mse": 0.0}  0.0459
1         train   15800/102467 98:20/539:26       7.718         {"mlm": 7.719078868859602, "mse": 0.0}  0.0356
1         train   15900/102467 98:56/538:43       7.718         {"mlm": 7.719507400554902, "mse": 0.0}  0.0428
1         train   16000/102467 99:33/537:59       7.718         {"mlm": 7.718338528910914, "mse": 0.0}  0.0459
1         train   16100/102467 100:09/537:16      7.718         {"mlm": 7.715217536257715, "mse": 0.0}  0.0406
1         train   16200/102467 100:45/536:33      7.718         {"mlm": 7.720485256408072, "mse": 0.0}  0.0477
1         train   16300/102467 101:21/535:50      7.718         {"mlm": 7.722108404243032, "mse": 0.0}  0.0419
1         train   16400/102467 101:58/535:08      7.718         {"mlm": 7.722686520151287, "mse": 0.0}  0.0382
1         train   16500/102467 102:34/534:25      7.718         {"mlm": 7.720420869063563, "mse": 0.0}  0.038
1         train   16600/102467 103:10/533:42      7.718         {"mlm": 7.72170676338413, "mse": 0.0}  0.0408
1         train   16700/102467 103:46/533:00      7.718         {"mlm": 7.722166230380963, "mse": 0.0}  0.0363
1         train   16800/102467 104:23/532:17      7.718         {"mlm": 7.721866574161774, "mse": 0.0}  0.0417
1         train   16900/102467 104:59/531:34      7.718         {"mlm": 7.720801828165384, "mse": 0.0}  0.0388
1         train   17000/102467 105:35/530:52      7.718         {"mlm": 7.721910046718065, "mse": 0.0}  0.0392
1         train   17100/102467 106:12/530:10      7.718         {"mlm": 7.721326708467635, "mse": 0.0}  0.0409
1         train   17200/102467 106:48/529:28      7.718         {"mlm": 7.720705188505831, "mse": 0.0}  0.0343
1         train   17300/102467 107:24/528:46      7.718         {"mlm": 7.721546238536732, "mse": 0.0}  0.0386
1         train   17400/102467 108:00/528:04      7.718         {"mlm": 7.721609146320231, "mse": 0.0}  0.0391
1         train   17500/102467 108:37/527:23      7.718         {"mlm": 7.719723537753404, "mse": 0.0}  0.0388
1         train   17600/102467 109:13/526:41      7.718         {"mlm": 7.720992072194385, "mse": 0.0}  0.0395
1         train   17700/102467 109:49/525:59      7.718         {"mlm": 7.719607473754433, "mse": 0.0}  0.0373
1         train   17800/102467 110:26/525:18      7.718         {"mlm": 7.718914640168713, "mse": 0.0}  0.0377
1         train   17900/102467 111:02/524:36      7.718         {"mlm": 7.717968171310224, "mse": 0.0}  0.0395
1         train   18000/102467 111:38/523:54      7.718         {"mlm": 7.717344197859212, "mse": 0.0}  0.0393
1         train   18100/102467 112:14/523:12      7.718         {"mlm": 7.687079519033432, "mse": 0.0}  0.0385
1         train   18200/102467 112:51/522:30      7.718         {"mlm": 7.70304633403311, "mse": 0.0}  0.0422
1         train   18300/102467 113:27/521:48      7.718         {"mlm": 7.709506738830257, "mse": 0.0}  0.0433
1         train   18400/102467 114:03/521:06      7.718         {"mlm": 7.713117405621692, "mse": 0.0}  0.0391
1         train   18500/102467 114:39/520:25      7.718         {"mlm": 7.718874747714689, "mse": 0.0}  0.0379
1         train   18600/102467 115:15/519:43      7.718         {"mlm": 7.717190182449033, "mse": 0.0}  0.0454
1         train   18700/102467 115:52/519:02      7.718         {"mlm": 7.715585759316368, "mse": 0.0}  0.0423
1         train   18800/102467 116:28/518:21      7.718         {"mlm": 7.716181333340592, "mse": 0.0}  0.0352
1         train   18900/102467 117:04/517:40      7.718         {"mlm": 7.717303400593145, "mse": 0.0}  0.0467
1         train   19000/102467 117:40/516:58      7.718         {"mlm": 7.716711329169063, "mse": 0.0}  0.035
1         train   19100/102467 118:17/516:17      7.718         {"mlm": 7.716132356302581, "mse": 0.0}  0.0355
1         train   19200/102467 118:53/515:35      7.718         {"mlm": 7.716723110364831, "mse": 0.0}  0.0401
1         train   19300/102467 119:29/514:54      7.718         {"mlm": 7.71674598109575, "mse": 0.0}  0.0341
1         train   19400/102467 120:05/514:13      7.718         {"mlm": 7.716912967769328, "mse": 0.0}  0.0413
1         train   19500/102467 120:42/513:33      7.718         {"mlm": 7.717439300554959, "mse": 0.0}  0.0412
1         train   19600/102467 121:18/512:52      7.718         {"mlm": 7.71555989666989, "mse": 0.0}  0.0432
1         train   19700/102467 121:54/512:11      7.718         {"mlm": 7.716631130508657, "mse": 0.0}  0.0358
1         train   19800/102467 122:30/511:30      7.718         {"mlm": 7.7167258984793, "mse": 0.0}  0.0505
1         train   19900/102467 123:07/510:49      7.718         {"mlm": 7.715513186867227, "mse": 0.0}  0.0418
1         train   20000/102467 123:43/510:09      7.717         {"mlm": 7.714676047852618, "mse": 0.0}  0.0377

09/23/2022 20:57:16 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1.pkl
1         valid   1/781        0:41/533:49        7.658           None
1         valid   101/781      0:56/ 6:19         7.694           None
1         valid   201/781      1:11/ 3:26         7.707           None
1         valid   301/781      1:26/ 2:18         7.716           None
1         valid   401/781      1:42/ 1:36         7.718           None
1         valid   501/781      1:57/ 1:05         7.718           None
1         valid   601/781      2:12/ 0:39         7.718           None
1         valid   701/781      2:28/ 0:16         7.720           None
1         valid   781/781      2:42/ 0:00         7.720         {"mlm": 7.719590268886043, "mse": 0.0, "train": 0.0}  None
1         train   20100/102467 127:01/520:33      7.717         {"mlm": 7.717842016220093, "mse": 0.0}  0.0341
1         train   20200/102467 127:37/519:47      7.718         {"mlm": 7.721685996055603, "mse": 0.0}  0.0451
1         train   20300/102467 128:13/519:01      7.718         {"mlm": 7.722465702692667, "mse": 0.0}  0.0383
1         train   20400/102467 128:49/518:15      7.718         {"mlm": 7.719678643941879, "mse": 0.0}  0.0392
1         train   20500/102467 129:25/517:30      7.718         {"mlm": 7.71889790058136, "mse": 0.0}  0.0413
1         train   20600/102467 130:01/516:44      7.718         {"mlm": 7.719767959117889, "mse": 0.0}  0.044
1         train   20700/102467 130:37/515:59      7.718         {"mlm": 7.7196061870029995, "mse": 0.0}  0.0429
1         train   20800/102467 131:13/515:14      7.717         {"mlm": 7.7179041594266895, "mse": 0.0}  0.0342
1         train   20900/102467 131:49/514:30      7.717         {"mlm": 7.714187066819933, "mse": 0.0}  0.0474
1         train   21000/102467 132:25/513:45      7.717         {"mlm": 7.714209266662598, "mse": 0.0}  0.0364
1         train   21100/102467 133:02/513:00      7.717         {"mlm": 7.71394606590271, "mse": 0.0}  0.0402
1         train   21200/102467 133:38/512:16      7.717         {"mlm": 7.716038364569346, "mse": 0.0}  0.0398
1         train   21300/102467 134:14/511:32      7.717         {"mlm": 7.716116559322064, "mse": 0.0}  0.0366
1         train   21400/102467 134:50/510:48      7.717         {"mlm": 7.717205335412706, "mse": 0.0}  0.034
1         train   21500/102467 135:26/510:05      7.717         {"mlm": 7.716188876787822, "mse": 0.0}  0.0391
1         train   21600/102467 136:03/509:21      7.717         {"mlm": 7.716801429688931, "mse": 0.0}  0.0375
1         train   21700/102467 136:39/508:37      7.717         {"mlm": 7.716878108978271, "mse": 0.0}  0.0318
1         train   21800/102467 137:15/507:54      7.717         {"mlm": 7.715985071659088, "mse": 0.0}  0.0391
1         train   21900/102467 137:51/507:11      7.717         {"mlm": 7.71678089116749, "mse": 0.0}  0.0438
1         train   22000/102467 138:28/506:28      7.718         {"mlm": 7.718034612417221, "mse": 0.0}  0.0399
1         train   22100/102467 139:04/505:44      7.718         {"mlm": 7.728228217423564, "mse": 0.0}  0.0337
1         train   22200/102467 139:40/505:01      7.717         {"mlm": 7.714591220395649, "mse": 0.0}  0.0379
1         train   22300/102467 140:17/504:18      7.718         {"mlm": 7.718101876236523, "mse": 0.0}  0.0357
1         train   22400/102467 140:53/503:35      7.718         {"mlm": 7.718259780329272, "mse": 0.0}  0.0347
1         train   22500/102467 141:29/502:52      7.718         {"mlm": 7.718688617966218, "mse": 0.0}  0.0449
1         train   22600/102467 142:05/502:09      7.718         {"mlm": 7.720285537445884, "mse": 0.0}  0.0371
1         train   22700/102467 142:42/501:26      7.718         {"mlm": 7.721184924266199, "mse": 0.0}  0.0359
1         train   22800/102467 143:18/500:44      7.718         {"mlm": 7.7210875261710195, "mse": 0.0}  0.0425
1         train   22900/102467 143:54/500:01      7.718         {"mlm": 7.719772399864154, "mse": 0.0}  0.0406
1         train   23000/102467 144:30/499:18      7.718         {"mlm": 7.719858573841022, "mse": 0.0}  0.0454
1         train   23100/102467 145:07/498:36      7.718         {"mlm": 7.719637110625971, "mse": 0.0}  0.0448
1         train   23200/102467 145:43/497:53      7.718         {"mlm": 7.719774793047423, "mse": 0.0}  0.0376
1         train   23300/102467 146:19/497:11      7.718         {"mlm": 7.7185616485883495, "mse": 0.0}  0.0404
1         train   23400/102467 146:56/496:28      7.718         {"mlm": 7.718416050726214, "mse": 0.0}  0.037
1         train   23500/102467 147:32/495:46      7.718         {"mlm": 7.719565487607786, "mse": 0.0}  0.0369
1         train   23600/102467 148:08/495:04      7.718         {"mlm": 7.719702537541393, "mse": 0.0}  0.0431
1         train   23700/102467 148:44/494:22      7.718         {"mlm": 7.719905225159913, "mse": 0.0}  0.0342
1         train   23800/102467 149:21/493:40      7.718         {"mlm": 7.7199193428065, "mse": 0.0}  0.0351
1         train   23900/102467 149:57/492:58      7.718         {"mlm": 7.720653011147257, "mse": 0.0}  0.0376
1         train   24000/102467 150:33/492:16      7.718         {"mlm": 7.720310273917095, "mse": 0.0}  0.0447
1         train   24100/102467 151:10/491:34      7.718         {"mlm": 7.735927800742948, "mse": 0.0}  0.0381
1         train   24200/102467 151:46/490:52      7.718         {"mlm": 7.7207261721293134, "mse": 0.0}  0.0388
1         train   24300/102467 152:22/490:10      7.718         {"mlm": 7.721545523445078, "mse": 0.0}  0.0343
1         train   24400/102467 152:59/489:28      7.718         {"mlm": 7.724446012746149, "mse": 0.0}  0.0382
1         train   24500/102467 153:35/488:47      7.718         {"mlm": 7.7212629873589815, "mse": 0.0}  0.0383
1         train   24600/102467 154:11/488:05      7.718         {"mlm": 7.721113013583282, "mse": 0.0}  0.0443
1         train   24700/102467 154:48/487:23      7.718         {"mlm": 7.7238671397069805, "mse": 0.0}  0.0369
1         train   24800/102467 155:24/486:42      7.718         {"mlm": 7.726133262304435, "mse": 0.0}  0.0378
1         train   24900/102467 156:00/486:00      7.718         {"mlm": 7.723034034062069, "mse": 0.0}  0.0372
1         train   25000/102467 156:37/485:18      7.718         {"mlm": 7.722510122345063, "mse": 0.0}  0.0373
1         train   25100/102467 157:13/484:36      7.718         {"mlm": 7.721326662108764, "mse": 0.0}  0.0387
1         train   25200/102467 157:49/483:55      7.718         {"mlm": 7.720807987381899, "mse": 0.0}  0.0345
1         train   25300/102467 158:25/483:14      7.718         {"mlm": 7.723024905738184, "mse": 0.0}  0.033
1         train   25400/102467 159:02/482:32      7.718         {"mlm": 7.72336777187724, "mse": 0.0}  0.0411
1         train   25500/102467 159:38/481:51      7.718         {"mlm": 7.721991301856149, "mse": 0.0}  0.0417
1         train   25600/102467 160:14/481:09      7.718         {"mlm": 7.72263377032083, "mse": 0.0}  0.0371
1         train   25700/102467 160:51/480:28      7.718         {"mlm": 7.722330620488233, "mse": 0.0}  0.036
1         train   25800/102467 161:27/479:46      7.718         {"mlm": 7.722572738257081, "mse": 0.0}  0.0365
1         train   25900/102467 162:03/479:05      7.718         {"mlm": 7.7213952883026, "mse": 0.0}  0.0417
1         train   26000/102467 162:39/478:24      7.718         {"mlm": 7.722384521314451, "mse": 0.0}  0.0354
1         train   26100/102467 163:16/477:42      7.718         {"mlm": 7.716058888386205, "mse": 0.0}  0.0363
1         train   26200/102467 163:52/477:01      7.718         {"mlm": 7.712354689080098, "mse": 0.0}  0.0392
1         train   26300/102467 164:28/476:20      7.718         {"mlm": 7.714351130655719, "mse": 0.0}  0.0351
1         train   26400/102467 165:05/475:39      7.718         {"mlm": 7.713787287849023, "mse": 0.0}  0.0381
1         train   26500/102467 165:41/474:58      7.718         {"mlm": 7.717708003353065, "mse": 0.0}  0.0412
1         train   26600/102467 166:17/474:17      7.718         {"mlm": 7.718536543087344, "mse": 0.0}  0.0428
1         train   26700/102467 166:53/473:36      7.718         {"mlm": 7.720849466118614, "mse": 0.0}  0.0389
1         train   26800/102467 167:30/472:55      7.718         {"mlm": 7.723109236924232, "mse": 0.0}  0.04
1         train   26900/102467 168:06/472:14      7.718         {"mlm": 7.720455803865839, "mse": 0.0}  0.0362
1         train   27000/102467 168:42/471:32      7.718         {"mlm": 7.7194532783722565, "mse": 0.0}  0.0318
1         train   27100/102467 169:18/470:52      7.718         {"mlm": 7.719007638115826, "mse": 0.0}  0.0397
1         train   27200/102467 169:54/470:11      7.718         {"mlm": 7.718728569416374, "mse": 0.0}  0.0358
1         train   27300/102467 170:31/469:30      7.718         {"mlm": 7.717394826222863, "mse": 0.0}  0.0436
1         train   27400/102467 171:07/468:49      7.718         {"mlm": 7.718312104770603, "mse": 0.0}  0.0325
1         train   27500/102467 171:43/468:08      7.718         {"mlm": 7.71640988421902, "mse": 0.0}  0.0349
1         train   27600/102467 172:19/467:27      7.718         {"mlm": 7.716516899331032, "mse": 0.0}  0.0356
1         train   27700/102467 172:56/466:46      7.718         {"mlm": 7.714885419442082, "mse": 0.0}  0.0357
1         train   27800/102467 173:32/466:06      7.718         {"mlm": 7.714288639372166, "mse": 0.0}  0.0387
1         train   27900/102467 174:08/465:25      7.718         {"mlm": 7.715034152560317, "mse": 0.0}  0.0408
1         train   28000/102467 174:44/464:45      7.718         {"mlm": 7.7145276871930974, "mse": 0.0}  0.033
1         train   28100/102467 175:21/464:04      7.718         {"mlm": 7.706931218504906, "mse": 0.0}  0.0356
1         train   28200/102467 175:57/463:23      7.718         {"mlm": 7.711224920895635, "mse": 0.0}  0.0379
1         train   28300/102467 176:33/462:43      7.718         {"mlm": 7.710074448907697, "mse": 0.0}  0.0344
1         train   28400/102467 177:09/462:02      7.718         {"mlm": 7.711651946559097, "mse": 0.0}  0.0331
1         train   28500/102467 177:46/461:22      7.718         {"mlm": 7.7121776419301185, "mse": 0.0}  0.0433
1         train   28600/102467 178:22/460:41      7.718         {"mlm": 7.71699082531385, "mse": 0.0}  0.0341
1         train   28700/102467 178:58/460:01      7.718         {"mlm": 7.716770742131375, "mse": 0.0}  0.0363
1         train   28800/102467 179:35/459:21      7.718         {"mlm": 7.714386152262664, "mse": 0.0}  0.0348
1         train   28900/102467 180:11/458:41      7.718         {"mlm": 7.717041354094233, "mse": 0.0}  0.0415
1         train   29000/102467 180:47/458:00      7.718         {"mlm": 7.717761685091808, "mse": 0.0}  0.0395
1         train   29100/102467 181:23/457:20      7.718         {"mlm": 7.717126547855182, "mse": 0.0}  0.0475
1         train   29200/102467 182:00/456:40      7.718         {"mlm": 7.716091238136674, "mse": 0.0}  0.0358
1         train   29300/102467 182:36/455:59      7.718         {"mlm": 7.7182560195157555, "mse": 0.0}  0.0368
1         train   29400/102467 183:12/455:19      7.718         {"mlm": 7.720368989900736, "mse": 0.0}  0.0366
1         train   29500/102467 183:48/454:39      7.718         {"mlm": 7.7188093493329015, "mse": 0.0}  0.037
1         train   29600/102467 184:24/453:58      7.718         {"mlm": 7.719654351846318, "mse": 0.0}  0.0343
1         train   29700/102467 185:01/453:18      7.718         {"mlm": 7.719180197929436, "mse": 0.0}  0.038
1         train   29800/102467 185:37/452:38      7.718         {"mlm": 7.718574374715045, "mse": 0.0}  0.0306
1         train   29900/102467 186:13/451:58      7.718         {"mlm": 7.718148282057123, "mse": 0.0}  0.0422
1         train   30000/102467 186:50/451:18      7.718         {"mlm": 7.718514355485568, "mse": 0.0}  0.0408

09/23/2022 22:00:22 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1.pkl
1         valid   1/781        0:40/526:51        7.736           None
1         valid   101/781      0:55/ 6:15         7.690           None
1         valid   201/781      1:10/ 3:24         7.703           None
1         valid   301/781      1:26/ 2:17         7.701           None
1         valid   401/781      1:41/ 1:36         7.704           None
1         valid   501/781      1:56/ 1:05         7.701           None
1         valid   601/781      2:12/ 0:39         7.705           None
1         valid   701/781      2:27/ 0:16         7.710           None
1         valid   781/781      2:42/ 0:00         7.720         {"mlm": 7.719750320102433, "mse": 0.0, "train": 0.0}  None
1         train   30100/102467 190:08/457:07      7.718         {"mlm": 7.70948935508728, "mse": 0.0}  0.0405
1         train   30200/102467 190:43/456:24      7.718         {"mlm": 7.712641906738281, "mse": 0.0}  0.0373
1         train   30300/102467 191:19/455:42      7.718         {"mlm": 7.71212207476298, "mse": 0.0}  0.0429
1         train   30400/102467 191:55/454:59      7.718         {"mlm": 7.710705903768539, "mse": 0.0}  0.0426
1         train   30500/102467 192:31/454:17      7.718         {"mlm": 7.715550333976745, "mse": 0.0}  0.0333
1         train   30600/102467 193:07/453:34      7.718         {"mlm": 7.7174835173288985, "mse": 0.0}  0.0389
1         train   30700/102467 193:43/452:52      7.718         {"mlm": 7.721503251620701, "mse": 0.0}  0.0345
1         train   30800/102467 194:19/452:09      7.718         {"mlm": 7.720142738819122, "mse": 0.0}  0.0388
1         train   30900/102467 194:55/451:27      7.718         {"mlm": 7.719992746247185, "mse": 0.0}  0.0402
1         train   31000/102467 195:31/450:45      7.718         {"mlm": 7.719687977790833, "mse": 0.0}  0.0419
1         train   31100/102467 196:07/450:03      7.718         {"mlm": 7.719612710259177, "mse": 0.0}  0.0372
1         train   31200/102467 196:43/449:22      7.718         {"mlm": 7.721969177722931, "mse": 0.0}  0.0394
1         train   31300/102467 197:19/448:40      7.718         {"mlm": 7.72280421440418, "mse": 0.0}  0.0388
1         train   31400/102467 197:56/447:58      7.718         {"mlm": 7.722966710499355, "mse": 0.0}  0.0446
1         train   31500/102467 198:32/447:17      7.718         {"mlm": 7.723032689412435, "mse": 0.0}  0.0406
1         train   31600/102467 199:08/446:35      7.718         {"mlm": 7.722138008773327, "mse": 0.0}  0.0327
1         train   31700/102467 199:44/445:54      7.718         {"mlm": 7.721982951725231, "mse": 0.0}  0.0354
1         train   31800/102467 200:20/445:13      7.718         {"mlm": 7.7211983619795905, "mse": 0.0}  0.0351
1         train   31900/102467 200:57/444:31      7.718         {"mlm": 7.720954519322044, "mse": 0.0}  0.0356
1         train   32000/102467 201:33/443:50      7.718         {"mlm": 7.720310474157333, "mse": 0.0}  0.0392
1         train   32100/102467 202:09/443:09      7.718         {"mlm": 7.7098106519140375, "mse": 0.0}  0.0442
1         train   32200/102467 202:46/442:28      7.718         {"mlm": 7.721354031682614, "mse": 0.0}  0.0367
1         train   32300/102467 203:22/441:47      7.718         {"mlm": 7.720942755606661, "mse": 0.0}  0.0365
1         train   32400/102467 203:58/441:06      7.718         {"mlm": 7.725584670714567, "mse": 0.0}  0.0407
1         train   32500/102467 204:34/440:25      7.718         {"mlm": 7.722994896118531, "mse": 0.0}  0.0369
1         train   32600/102467 205:11/439:44      7.718         {"mlm": 7.720565200449031, "mse": 0.0}  0.0375
1         train   32700/102467 205:47/439:03      7.718         {"mlm": 7.722036607275705, "mse": 0.0}  0.0405
1         train   32800/102467 206:23/438:22      7.718         {"mlm": 7.720793110557432, "mse": 0.0}  0.0347
1         train   32900/102467 207:00/437:42      7.718         {"mlm": 7.720658383459615, "mse": 0.0}  0.0401
1         train   33000/102467 207:36/437:01      7.718         {"mlm": 7.719504797900164, "mse": 0.0}  0.0431
1         train   33100/102467 208:12/436:20      7.718         {"mlm": 7.719718600751271, "mse": 0.0}  0.0351
1         train   33200/102467 208:48/435:39      7.718         {"mlm": 7.719367168464692, "mse": 0.0}  0.0353
1         train   33300/102467 209:25/434:58      7.718         {"mlm": 7.716440369295468, "mse": 0.0}  0.0356
1         train   33400/102467 210:01/434:18      7.718         {"mlm": 7.71771591266962, "mse": 0.0}  0.0353
1         train   33500/102467 210:37/433:37      7.718         {"mlm": 7.717734802556563, "mse": 0.0}  0.0419
1         train   33600/102467 211:13/432:56      7.718         {"mlm": 7.717394861003024, "mse": 0.0}  0.0373
1         train   33700/102467 211:50/432:16      7.718         {"mlm": 7.718020462161867, "mse": 0.0}  0.0367
1         train   33800/102467 212:26/431:35      7.718         {"mlm": 7.717802899356946, "mse": 0.0}  0.0404
1         train   33900/102467 213:02/430:54      7.718         {"mlm": 7.717242614541197, "mse": 0.0}  0.0382
1         train   34000/102467 213:38/430:14      7.718         {"mlm": 7.717870816282775, "mse": 0.0}  0.0379
1         train   34100/102467 214:15/429:33      7.718         {"mlm": 7.711850550709938, "mse": 0.0}  0.0362
1         train   34200/102467 214:51/428:52      7.718         {"mlm": 7.707535644974372, "mse": 0.0}  0.0364
1         train   34300/102467 215:27/428:12      7.718         {"mlm": 7.716697600063862, "mse": 0.0}  0.0397
1         train   34400/102467 216:04/427:31      7.718         {"mlm": 7.718426385716577, "mse": 0.0}  0.0386
1         train   34500/102467 216:40/426:51      7.718         {"mlm": 7.720847451543233, "mse": 0.0}  0.0343
1         train   34600/102467 217:16/426:10      7.718         {"mlm": 7.7193369825548155, "mse": 0.0}  0.0345
1         train   34700/102467 217:52/425:30      7.718         {"mlm": 7.717462353856653, "mse": 0.0}  0.0371
1         train   34800/102467 218:28/424:49      7.718         {"mlm": 7.714880378622758, "mse": 0.0}  0.0355
1         train   34900/102467 219:05/424:09      7.718         {"mlm": 7.714762101991139, "mse": 0.0}  0.0317
1         train   35000/102467 219:41/423:28      7.718         {"mlm": 7.715080552684042, "mse": 0.0}  0.0372
1         train   35100/102467 220:17/422:48      7.718         {"mlm": 7.715439344364437, "mse": 0.0}  0.0362
1         train   35200/102467 220:53/422:08      7.718         {"mlm": 7.71591360939166, "mse": 0.0}  0.0428
1         train   35300/102467 221:30/421:27      7.718         {"mlm": 7.71508900336749, "mse": 0.0}  0.0399
1         train   35400/102467 222:06/420:47      7.718         {"mlm": 7.716630662800758, "mse": 0.0}  0.033
1         train   35500/102467 222:42/420:07      7.718         {"mlm": 7.716931977482123, "mse": 0.0}  0.0328
1         train   35600/102467 223:18/419:26      7.718         {"mlm": 7.716014964112054, "mse": 0.0}  0.0354
1         train   35700/102467 223:55/418:46      7.718         {"mlm": 7.716582018017628, "mse": 0.0}  0.0415
1         train   35800/102467 224:31/418:06      7.718         {"mlm": 7.717513808949506, "mse": 0.0}  0.0371
1         train   35900/102467 225:07/417:26      7.718         {"mlm": 7.717476050393976, "mse": 0.0}  0.036
1         train   36000/102467 225:44/416:46      7.718         {"mlm": 7.717273300951785, "mse": 0.0}  0.0349
1         train   36100/102467 226:20/416:06      7.718         {"mlm": 7.745509029663715, "mse": 0.0}  0.0342
1         train   36200/102467 226:56/415:26      7.718         {"mlm": 7.72865853091787, "mse": 0.0}  0.0346
1         train   36300/102467 227:32/414:46      7.718         {"mlm": 7.722183921120384, "mse": 0.0}  0.0385
1         train   36400/102467 228:09/414:06      7.718         {"mlm": 7.724318542768733, "mse": 0.0}  0.0384
1         train   36500/102467 228:45/413:26      7.718         {"mlm": 7.723015042617767, "mse": 0.0}  0.0366
1         train   36600/102467 229:21/412:46      7.718         {"mlm": 7.722168816992985, "mse": 0.0}  0.0361
1         train   36700/102467 229:58/412:06      7.718         {"mlm": 7.725269045344042, "mse": 0.0}  0.0344
1         train   36800/102467 230:34/411:26      7.718         {"mlm": 7.7238656538193915, "mse": 0.0}  0.0383
1         train   36900/102467 231:10/410:46      7.718         {"mlm": 7.723579583758096, "mse": 0.0}  0.0439
1         train   37000/102467 231:46/410:06      7.718         {"mlm": 7.722034589696672, "mse": 0.0}  0.0371
1         train   37100/102467 232:23/409:26      7.718         {"mlm": 7.719644085319888, "mse": 0.0}  0.0365
1         train   37200/102467 232:59/408:47      7.718         {"mlm": 7.719446925989468, "mse": 0.0}  0.0316
1         train   37300/102467 233:35/408:07      7.718         {"mlm": 7.718130265370459, "mse": 0.0}  0.0394
1         train   37400/102467 234:12/407:27      7.718         {"mlm": 7.718012292980721, "mse": 0.0}  0.0396
1         train   37500/102467 234:48/406:47      7.718         {"mlm": 7.719491483373648, "mse": 0.0}  0.0352
1         train   37600/102467 235:24/406:07      7.718         {"mlm": 7.718914840843951, "mse": 0.0}  0.0346
1         train   37700/102467 236:00/405:27      7.718         {"mlm": 7.719172670761135, "mse": 0.0}  0.04
1         train   37800/102467 236:37/404:48      7.718         {"mlm": 7.718199528252077, "mse": 0.0}  0.0322
1         train   37900/102467 237:13/404:08      7.718         {"mlm": 7.7186394861389225, "mse": 0.0}  0.0347
1         train   38000/102467 237:49/403:28      7.718         {"mlm": 7.717514495820956, "mse": 0.0}  0.0325
1         train   38100/102467 238:25/402:48      7.718         {"mlm": 7.7076525042454405, "mse": 0.0}  0.0349
1         train   38200/102467 239:02/402:09      7.718         {"mlm": 7.710417742631873, "mse": 0.0}  0.0317
1         train   38300/102467 239:38/401:29      7.718         {"mlm": 7.720042046662924, "mse": 0.0}  0.0433
1         train   38400/102467 240:14/400:49      7.718         {"mlm": 7.723009935533158, "mse": 0.0}  0.0374
1         train   38500/102467 240:51/400:10      7.718         {"mlm": 7.7237379300978874, "mse": 0.0}  0.033
1         train   38600/102467 241:27/399:30      7.718         {"mlm": 7.7188725623508425, "mse": 0.0}  0.0324
1         train   38700/102467 242:03/398:51      7.718         {"mlm": 7.71730938108488, "mse": 0.0}  0.0309
1         train   38800/102467 242:40/398:11      7.718         {"mlm": 7.7180511160711545, "mse": 0.0}  0.0342
1         train   38900/102467 243:16/397:32      7.718         {"mlm": 7.717697109494891, "mse": 0.0}  0.0358
1         train   39000/102467 243:52/396:52      7.718         {"mlm": 7.7189568080097795, "mse": 0.0}  0.0394
1         train   39100/102467 244:28/396:13      7.718         {"mlm": 7.719266209289105, "mse": 0.0}  0.0342
1         train   39200/102467 245:05/395:33      7.718         {"mlm": 7.719290655990907, "mse": 0.0}  0.0354
1         train   39300/102467 245:41/394:54      7.718         {"mlm": 7.71909377751527, "mse": 0.0}  0.0369
1         train   39400/102467 246:17/394:14      7.718         {"mlm": 7.718309873837796, "mse": 0.0}  0.0413
1         train   39500/102467 246:53/393:34      7.718         {"mlm": 7.718239292422718, "mse": 0.0}  0.0384
1         train   39600/102467 247:30/392:55      7.718         {"mlm": 7.719170984170192, "mse": 0.0}  0.0463
1         train   39700/102467 248:06/392:15      7.718         {"mlm": 7.719465198381892, "mse": 0.0}  0.0326
1         train   39800/102467 248:42/391:36      7.718         {"mlm": 7.718393856804727, "mse": 0.0}  0.0355
1         train   39900/102467 249:18/390:56      7.718         {"mlm": 7.717551218306465, "mse": 0.0}  0.0439
1         train   40000/102467 249:55/390:17      7.718         {"mlm": 7.717692896693885, "mse": 0.0}  0.037

09/23/2022 23:03:27 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1.pkl
1         valid   1/781        0:36/470:54        7.576           None
1         valid   101/781      0:51/ 5:48         7.712           None
1         valid   201/781      1:07/ 3:13         7.719           None
1         valid   301/781      1:22/ 2:11         7.719           None
1         valid   401/781      1:37/ 1:32         7.721           None
1         valid   501/781      1:52/ 1:03         7.721           None
1         valid   601/781      2:08/ 0:38         7.721           None
1         valid   701/781      2:23/ 0:16         7.720           None
1         valid   781/781      2:37/ 0:00         7.721         {"mlm": 7.720550576184379, "mse": 0.0, "train": 0.0}  None
1         train   40100/102467 253:08/393:42      7.718         {"mlm": 7.713904790878296, "mse": 0.0}  0.0396
1         train   40200/102467 253:44/393:01      7.718         {"mlm": 7.730276694297791, "mse": 0.0}  0.0338
1         train   40300/102467 254:20/392:20      7.718         {"mlm": 7.726627958615621, "mse": 0.0}  0.0395
1         train   40400/102467 254:56/391:39      7.718         {"mlm": 7.726755164861679, "mse": 0.0}  0.0369
1         train   40500/102467 255:32/390:59      7.718         {"mlm": 7.724655535697937, "mse": 0.0}  0.0409
1         train   40600/102467 256:08/390:18      7.718         {"mlm": 7.724188693364462, "mse": 0.0}  0.0359
1         train   40700/102467 256:44/389:37      7.718         {"mlm": 7.727208739008223, "mse": 0.0}  0.0324
1         train   40800/102467 257:20/388:57      7.718         {"mlm": 7.724425184130669, "mse": 0.0}  0.031
1         train   40900/102467 257:56/388:16      7.718         {"mlm": 7.723395176463657, "mse": 0.0}  0.0323
1         train   41000/102467 258:32/387:36      7.718         {"mlm": 7.721851087093353, "mse": 0.0}  0.0434
1         train   41100/102467 259:08/386:55      7.718         {"mlm": 7.721783018112182, "mse": 0.0}  0.0312
1         train   41200/102467 259:44/386:15      7.718         {"mlm": 7.7232792715231575, "mse": 0.0}  0.0382
1         train   41300/102467 260:20/385:35      7.718         {"mlm": 7.722880317247831, "mse": 0.0}  0.0325
1         train   41400/102467 260:57/384:55      7.718         {"mlm": 7.721143118313381, "mse": 0.0}  0.0348
1         train   41500/102467 261:33/384:14      7.718         {"mlm": 7.722154774347941, "mse": 0.0}  0.0358
1         train   41600/102467 262:09/383:34      7.718         {"mlm": 7.723091559112072, "mse": 0.0}  0.0377
1         train   41700/102467 262:45/382:54      7.718         {"mlm": 7.7220437047060795, "mse": 0.0}  0.0356
1         train   41800/102467 263:22/382:14      7.718         {"mlm": 7.721633601983388, "mse": 0.0}  0.0346
1         train   41900/102467 263:58/381:34      7.718         {"mlm": 7.7214759289590935, "mse": 0.0}  0.0347
1         train   42000/102467 264:34/380:54      7.718         {"mlm": 7.7207399361133575, "mse": 0.0}  0.0352
1         train   42100/102467 265:10/380:14      7.718         {"mlm": 7.709429948016851, "mse": 0.0}  0.0346
1         train   42200/102467 265:47/379:34      7.718         {"mlm": 7.715238626278825, "mse": 0.0}  0.0391
1         train   42300/102467 266:23/378:54      7.718         {"mlm": 7.721170623166904, "mse": 0.0}  0.042
1         train   42400/102467 266:59/378:14      7.718         {"mlm": 7.722764208800811, "mse": 0.0}  0.0344
1         train   42500/102467 267:35/377:34      7.718         {"mlm": 7.729143278393335, "mse": 0.0}  0.0355
1         train   42600/102467 268:12/376:54      7.718         {"mlm": 7.729242737185776, "mse": 0.0}  0.0342
1         train   42700/102467 268:48/376:14      7.718         {"mlm": 7.727406511320406, "mse": 0.0}  0.0314
1         train   42800/102467 269:24/375:35      7.718         {"mlm": 7.728084517062381, "mse": 0.0}  0.0406
1         train   42900/102467 270:01/374:55      7.718         {"mlm": 7.728970593949977, "mse": 0.0}  0.0406
1         train   43000/102467 270:37/374:15      7.718         {"mlm": 7.726054349103132, "mse": 0.0}  0.033
1         train   43100/102467 271:13/373:35      7.718         {"mlm": 7.7254393510757735, "mse": 0.0}  0.0383
1         train   43200/102467 271:49/372:55      7.718         {"mlm": 7.724048198114543, "mse": 0.0}  0.0368
1         train   43300/102467 272:26/372:16      7.718         {"mlm": 7.722992361463704, "mse": 0.0}  0.0372
1         train   43400/102467 273:02/371:36      7.718         {"mlm": 7.722184520009758, "mse": 0.0}  0.0315
1         train   43500/102467 273:38/370:56      7.718         {"mlm": 7.721850315040553, "mse": 0.0}  0.035
1         train   43600/102467 274:14/370:16      7.718         {"mlm": 7.723056786652279, "mse": 0.0}  0.0409
1         train   43700/102467 274:51/369:36      7.718         {"mlm": 7.722791162359497, "mse": 0.0}  0.0335
1         train   43800/102467 275:27/368:57      7.718         {"mlm": 7.7228629532093604, "mse": 0.0}  0.0466
1         train   43900/102467 276:03/368:17      7.718         {"mlm": 7.722977699512805, "mse": 0.0}  0.0455
1         train   44000/102467 276:39/367:37      7.718         {"mlm": 7.721540462738159, "mse": 0.0}  0.0338
1         train   44100/102467 277:15/366:57      7.718         {"mlm": 7.70619333033659, "mse": 0.0}  0.0345
1         train   44200/102467 277:52/366:18      7.718         {"mlm": 7.714504949974291, "mse": 0.0}  0.0329
1         train   44300/102467 278:28/365:38      7.718         {"mlm": 7.719317209000556, "mse": 0.0}  0.0348
1         train   44400/102467 279:04/364:59      7.718         {"mlm": 7.721446817244717, "mse": 0.0}  0.0378
1         train   44500/102467 279:41/364:19      7.718         {"mlm": 7.723201953742398, "mse": 0.0}  0.0357
1         train   44600/102467 280:17/363:39      7.718         {"mlm": 7.72073183968713, "mse": 0.0}  0.0339
1         train   44700/102467 280:53/363:00      7.718         {"mlm": 7.720319618809873, "mse": 0.0}  0.0334
1         train   44800/102467 281:29/362:20      7.718         {"mlm": 7.72263290768578, "mse": 0.0}  0.0324
1         train   44900/102467 282:05/361:40      7.718         {"mlm": 7.7220269014150364, "mse": 0.0}  0.0336
1         train   45000/102467 282:42/361:01      7.718         {"mlm": 7.722753258649715, "mse": 0.0}  0.0362
1         train   45100/102467 283:18/360:22      7.718         {"mlm": 7.722594922574709, "mse": 0.0}  0.0433
1         train   45200/102467 283:54/359:42      7.718         {"mlm": 7.723456420166067, "mse": 0.0}  0.0349
1         train   45300/102467 284:31/359:03      7.718         {"mlm": 7.723307594863586, "mse": 0.0}  0.0292
1         train   45400/102467 285:07/358:23      7.718         {"mlm": 7.723137684646082, "mse": 0.0}  0.0434
1         train   45500/102467 285:43/357:44      7.718         {"mlm": 7.721964711022473, "mse": 0.0}  0.0381
1         train   45600/102467 286:20/357:05      7.718         {"mlm": 7.722417907213538, "mse": 0.0}  0.0316
1         train   45700/102467 286:56/356:25      7.718         {"mlm": 7.72294455928151, "mse": 0.0}  0.0366
1         train   45800/102467 287:32/355:46      7.718         {"mlm": 7.724346975065577, "mse": 0.0}  0.0391
1         train   45900/102467 288:09/355:06      7.719         {"mlm": 7.724837148654323, "mse": 0.0}  0.0319
1         train   46000/102467 288:45/354:27      7.718         {"mlm": 7.724091023176879, "mse": 0.0}  0.0307
1         train   46100/102467 289:21/353:48      7.718         {"mlm": 7.710255052625518, "mse": 0.0}  0.0335
1         train   46200/102467 289:57/353:08      7.719         {"mlm": 7.729133363907712, "mse": 0.0}  0.0385
1         train   46300/102467 290:34/352:29      7.719         {"mlm": 7.728972420547947, "mse": 0.0}  0.0316
1         train   46400/102467 291:10/351:50      7.719         {"mlm": 7.726586995256938, "mse": 0.0}  0.0365
1         train   46500/102467 291:46/351:10      7.719         {"mlm": 7.72126306998178, "mse": 0.0}  0.0378
1         train   46600/102467 292:22/350:31      7.719         {"mlm": 7.720335073806533, "mse": 0.0}  0.036
1         train   46700/102467 292:59/349:52      7.718         {"mlm": 7.717020926892843, "mse": 0.0}  0.0313
1         train   46800/102467 293:35/349:12      7.718         {"mlm": 7.719307036740867, "mse": 0.0}  0.0355
1         train   46900/102467 294:11/348:33      7.718         {"mlm": 7.718857349493565, "mse": 0.0}  0.035
1         train   47000/102467 294:47/347:54      7.719         {"mlm": 7.7212902574147, "mse": 0.0}  0.0344
1         train   47100/102467 295:24/347:15      7.719         {"mlm": 7.719309197845737, "mse": 0.0}  0.0405
1         train   47200/102467 296:00/346:35      7.718         {"mlm": 7.718485205593763, "mse": 0.0}  0.0304
1         train   47300/102467 296:36/345:56      7.719         {"mlm": 7.719170710446748, "mse": 0.0}  0.0338
1         train   47400/102467 297:13/345:17      7.718         {"mlm": 7.717588223980936, "mse": 0.0}  0.0365
1         train   47500/102467 297:49/344:38      7.718         {"mlm": 7.717379137127098, "mse": 0.0}  0.042
1         train   47600/102467 298:25/343:59      7.718         {"mlm": 7.718711211372333, "mse": 0.0}  0.0365
1         train   47700/102467 299:02/343:20      7.719         {"mlm": 7.719157099513074, "mse": 0.0}  0.0364
1         train   47800/102467 299:38/342:41      7.718         {"mlm": 7.718422321325948, "mse": 0.0}  0.0388
1         train   47900/102467 300:14/342:01      7.718         {"mlm": 7.718086996264501, "mse": 0.0}  0.0345
1         train   48000/102467 300:50/341:22      7.719         {"mlm": 7.719309964416381, "mse": 0.0}  0.0349
1         train   48100/102467 301:27/340:43      7.719         {"mlm": 7.717724055051804, "mse": 0.0}  0.0326
1         train   48200/102467 302:03/340:04      7.719         {"mlm": 7.720639856494203, "mse": 0.0}  0.0369
1         train   48300/102467 302:39/339:25      7.719         {"mlm": 7.715000455443923, "mse": 0.0}  0.0334
1         train   48400/102467 303:16/338:46      7.718         {"mlm": 7.711414152925665, "mse": 0.0}  0.0354
1         train   48500/102467 303:52/338:07      7.718         {"mlm": 7.710474610328674, "mse": 0.0}  0.0343
1         train   48600/102467 304:28/337:28      7.718         {"mlm": 7.713126404173422, "mse": 0.0}  0.0377
1         train   48700/102467 305:05/336:49      7.718         {"mlm": 7.716054683444144, "mse": 0.0}  0.0308
1         train   48800/102467 305:41/336:10      7.718         {"mlm": 7.715080518818381, "mse": 0.0}  0.038
1         train   48900/102467 306:17/335:31      7.718         {"mlm": 7.715901721801076, "mse": 0.0}  0.036
1         train   49000/102467 306:53/334:52      7.719         {"mlm": 7.717792949523313, "mse": 0.0}  0.0371
1         train   49100/102467 307:30/334:13      7.719         {"mlm": 7.718030043327025, "mse": 0.0}  0.039
1         train   49200/102467 308:06/333:34      7.719         {"mlm": 7.719771974860226, "mse": 0.0}  0.0315
1         train   49300/102467 308:42/332:55      7.719         {"mlm": 7.7190458391919545, "mse": 0.0}  0.0381
1         train   49400/102467 309:18/332:16      7.719         {"mlm": 7.717768150619245, "mse": 0.0}  0.0331
1         train   49500/102467 309:55/331:37      7.718         {"mlm": 7.7177785568058805, "mse": 0.0}  0.0396
1         train   49600/102467 310:31/330:58      7.718         {"mlm": 7.716940126323461, "mse": 0.0}  0.0315
1         train   49700/102467 311:07/330:19      7.718         {"mlm": 7.717423416814714, "mse": 0.0}  0.0372
1         train   49800/102467 311:43/329:40      7.718         {"mlm": 7.717058718602748, "mse": 0.0}  0.0383
1         train   49900/102467 312:20/329:01      7.718         {"mlm": 7.717676356371948, "mse": 0.0}  0.0367
1         train   50000/102467 312:56/328:22      7.718         {"mlm": 7.717846517811318, "mse": 0.0}  0.0351

09/24/2022 00:06:29 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1.pkl
1         valid   1/781        0:34/454:18        7.646           None
1         valid   101/781      0:50/ 5:37         7.695           None
1         valid   201/781      1:05/ 3:08         7.724           None
1         valid   301/781      1:20/ 2:08         7.719           None
1         valid   401/781      1:35/ 1:30         7.726           None
1         valid   501/781      1:51/ 1:02         7.726           None
1         valid   601/781      2:06/ 0:37         7.724           None
1         valid   701/781      2:21/ 0:16         7.725           None
1         valid   781/781      2:36/ 0:00         7.722         {"mlm": 7.7219910371318825, "mse": 0.0, "train": 0.0}  None
1         train   50100/102467 316:08/330:26      7.718         {"mlm": 7.700166034698486, "mse": 0.0}  0.0406
1         train   50200/102467 316:44/329:46      7.718         {"mlm": 7.698410217761993, "mse": 0.0}  0.029
1         train   50300/102467 317:20/329:06      7.718         {"mlm": 7.704762218793233, "mse": 0.0}  0.0315
1         train   50400/102467 317:56/328:27      7.718         {"mlm": 7.708822242021561, "mse": 0.0}  0.0367
1         train   50500/102467 318:32/327:47      7.718         {"mlm": 7.709723219871521, "mse": 0.0}  0.0361
1         train   50600/102467 319:08/327:07      7.718         {"mlm": 7.712678274313609, "mse": 0.0}  0.0348
1         train   50700/102467 319:44/326:28      7.718         {"mlm": 7.718447162083217, "mse": 0.0}  0.0438
1         train   50800/102467 320:20/325:48      7.718         {"mlm": 7.718499398231506, "mse": 0.0}  0.0465
1         train   50900/102467 320:56/325:08      7.719         {"mlm": 7.719552899996439, "mse": 0.0}  0.0335
1         train   51000/102467 321:32/324:29      7.719         {"mlm": 7.718951406478882, "mse": 0.0}  0.032
1         train   51100/102467 322:08/323:49      7.719         {"mlm": 7.719235832907937, "mse": 0.0}  0.0384
1         train   51200/102467 322:44/323:10      7.718         {"mlm": 7.718358563184738, "mse": 0.0}  0.0361
1         train   51300/102467 323:21/322:30      7.718         {"mlm": 7.717630279614375, "mse": 0.0}  0.0323
1         train   51400/102467 323:57/321:51      7.718         {"mlm": 7.7181568305832995, "mse": 0.0}  0.0324
1         train   51500/102467 324:33/321:11      7.719         {"mlm": 7.7187823333740235, "mse": 0.0}  0.0322
1         train   51600/102467 325:09/320:32      7.718         {"mlm": 7.717938489019871, "mse": 0.0}  0.0352
1         train   51700/102467 325:45/319:53      7.718         {"mlm": 7.7171827206892125, "mse": 0.0}  0.0305
1         train   51800/102467 326:22/319:13      7.718         {"mlm": 7.717482831478119, "mse": 0.0}  0.0391
1         train   51900/102467 326:58/318:34      7.718         {"mlm": 7.716723219971907, "mse": 0.0}  0.0332
1         train   52000/102467 327:34/317:55      7.718         {"mlm": 7.7170598921775815, "mse": 0.0}  0.032
1         train   52100/102467 328:10/317:15      7.718         {"mlm": 7.688236660427517, "mse": 0.0}  0.0388
1         train   52200/102467 328:47/316:36      7.718         {"mlm": 7.712078844482575, "mse": 0.0}  0.0333
1         train   52300/102467 329:23/315:57      7.718         {"mlm": 7.715904390532835, "mse": 0.0}  0.0383
1         train   52400/102467 329:59/315:17      7.718         {"mlm": 7.72062953910732, "mse": 0.0}  0.0385
1         train   52500/102467 330:35/314:38      7.718         {"mlm": 7.721503554938551, "mse": 0.0}  0.0345
1         train   52600/102467 331:11/313:59      7.718         {"mlm": 7.71863146258117, "mse": 0.0}  0.0402
1         train   52700/102467 331:48/313:20      7.718         {"mlm": 7.719694855898747, "mse": 0.0}  0.0361
1         train   52800/102467 332:24/312:40      7.718         {"mlm": 7.721286695501831, "mse": 0.0}  0.0385
1         train   52900/102467 333:00/312:01      7.718         {"mlm": 7.7214123743925, "mse": 0.0}  0.032
1         train   53000/102467 333:36/311:22      7.718         {"mlm": 7.721752342877087, "mse": 0.0}  0.0378
1         train   53100/102467 334:12/310:43      7.719         {"mlm": 7.721524251602909, "mse": 0.0}  0.0362
1         train   53200/102467 334:49/310:03      7.719         {"mlm": 7.721532433901954, "mse": 0.0}  0.0312
1         train   53300/102467 335:25/309:24      7.719         {"mlm": 7.721907167823797, "mse": 0.0}  0.0357
1         train   53400/102467 336:01/308:45      7.719         {"mlm": 7.722598132787218, "mse": 0.0}  0.038
1         train   53500/102467 336:37/308:06      7.719         {"mlm": 7.721627277402261, "mse": 0.0}  0.0303
1         train   53600/102467 337:14/307:27      7.719         {"mlm": 7.7211644272270465, "mse": 0.0}  0.0326
1         train   53700/102467 337:50/306:48      7.719         {"mlm": 7.720508876023677, "mse": 0.0}  0.0319
1         train   53800/102467 338:26/306:09      7.719         {"mlm": 7.720442153269082, "mse": 0.0}  0.0352
1         train   53900/102467 339:02/305:29      7.718         {"mlm": 7.720102386765884, "mse": 0.0}  0.0366
1         train   54000/102467 339:38/304:50      7.719         {"mlm": 7.720938393686819, "mse": 0.0}  0.0463
1         train   54100/102467 340:15/304:11      7.719         {"mlm": 7.741098768857061, "mse": 0.0}  0.0365
1         train   54200/102467 340:51/303:32      7.719         {"mlm": 7.729022368036135, "mse": 0.0}  0.0321
1         train   54300/102467 341:27/302:53      7.719         {"mlm": 7.729648314866443, "mse": 0.0}  0.0514
1         train   54400/102467 342:03/302:14      7.719         {"mlm": 7.728173176846911, "mse": 0.0}  0.0393
1         train   54500/102467 342:39/301:35      7.719         {"mlm": 7.7238813250897875, "mse": 0.0}  0.0386
1         train   54600/102467 343:16/300:56      7.719         {"mlm": 7.72328482583215, "mse": 0.0}  0.0377
1         train   54700/102467 343:52/300:17      7.719         {"mlm": 7.722781235304125, "mse": 0.0}  0.0347
1         train   54800/102467 344:28/299:38      7.719         {"mlm": 7.723898644435375, "mse": 0.0}  0.0366
1         train   54900/102467 345:04/298:59      7.719         {"mlm": 7.724723051810318, "mse": 0.0}  0.0366
1         train   55000/102467 345:41/298:20      7.719         {"mlm": 7.721898826664101, "mse": 0.0}  0.0318
1         train   55100/102467 346:17/297:41      7.719         {"mlm": 7.7239070746416605, "mse": 0.0}  0.0376
1         train   55200/102467 346:53/297:02      7.719         {"mlm": 7.722280281413974, "mse": 0.0}  0.0331
1         train   55300/102467 347:29/296:23      7.719         {"mlm": 7.72078653770529, "mse": 0.0}  0.0316
1         train   55400/102467 348:05/295:44      7.719         {"mlm": 7.718686231386679, "mse": 0.0}  0.0403
1         train   55500/102467 348:42/295:05      7.719         {"mlm": 7.720238747042871, "mse": 0.0}  0.0372
1         train   55600/102467 349:18/294:26      7.719         {"mlm": 7.72167194829566, "mse": 0.0}  0.0335
1         train   55700/102467 349:54/293:47      7.719         {"mlm": 7.721299010536275, "mse": 0.0}  0.04
1         train   55800/102467 350:31/293:08      7.719         {"mlm": 7.720182567337596, "mse": 0.0}  0.0326
1         train   55900/102467 351:07/292:29      7.719         {"mlm": 7.720331593986809, "mse": 0.0}  0.0353
1         train   56000/102467 351:43/291:51      7.719         {"mlm": 7.719572442668575, "mse": 0.0}  0.0352
1         train   56100/102467 352:19/291:12      7.719         {"mlm": 7.693048147811103, "mse": 0.0}  0.0381
1         train   56200/102467 352:56/290:33      7.719         {"mlm": 7.713942999767168, "mse": 0.0}  0.0313
1         train   56300/102467 353:32/289:54      7.719         {"mlm": 7.713953154657023, "mse": 0.0}  0.0388
1         train   56400/102467 354:08/289:15      7.719         {"mlm": 7.711302297241441, "mse": 0.0}  0.0358
1         train   56500/102467 354:45/288:37      7.719         {"mlm": 7.712988439939871, "mse": 0.0}  0.0409
1         train   56600/102467 355:21/287:58      7.719         {"mlm": 7.720174299013276, "mse": 0.0}  0.0307
1         train   56700/102467 355:57/287:19      7.719         {"mlm": 7.7200525829747555, "mse": 0.0}  0.0305
1         train   56800/102467 356:34/286:40      7.719         {"mlm": 7.72060708987668, "mse": 0.0}  0.0321
1         train   56900/102467 357:10/286:01      7.719         {"mlm": 7.720310639644015, "mse": 0.0}  0.0333
1         train   57000/102467 357:46/285:23      7.719         {"mlm": 7.721711540413477, "mse": 0.0}  0.0396
1         train   57100/102467 358:22/284:44      7.719         {"mlm": 7.722269605047179, "mse": 0.0}  0.037
1         train   57200/102467 358:59/284:05      7.719         {"mlm": 7.721436414503513, "mse": 0.0}  0.0307
1         train   57300/102467 359:35/283:26      7.719         {"mlm": 7.718800767164738, "mse": 0.0}  0.0345
1         train   57400/102467 360:11/282:48      7.719         {"mlm": 7.719207747970723, "mse": 0.0}  0.0337
1         train   57500/102467 360:47/282:09      7.719         {"mlm": 7.7187710305254065, "mse": 0.0}  0.0378
1         train   57600/102467 361:24/281:30      7.719         {"mlm": 7.719122797082795, "mse": 0.0}  0.0355
1         train   57700/102467 362:00/280:52      7.719         {"mlm": 7.719136834355334, "mse": 0.0}  0.0482
1         train   57800/102467 362:36/280:13      7.719         {"mlm": 7.719918299331622, "mse": 0.0}  0.0349
1         train   57900/102467 363:13/279:34      7.719         {"mlm": 7.7210320715032, "mse": 0.0}  0.0345
1         train   58000/102467 363:49/278:56      7.719         {"mlm": 7.720810417897354, "mse": 0.0}  0.0365
1         train   58100/102467 364:25/278:17      7.719         {"mlm": 7.735368723670642, "mse": 0.0}  0.036
1         train   58200/102467 365:02/277:38      7.719         {"mlm": 7.7192584957395285, "mse": 0.0}  0.0363
1         train   58300/102467 365:38/277:00      7.719         {"mlm": 7.717268410566691, "mse": 0.0}  0.0421
1         train   58400/102467 366:14/276:21      7.719         {"mlm": 7.716921682309622, "mse": 0.0}  0.031
1         train   58500/102467 366:50/275:42      7.719         {"mlm": 7.712291573324511, "mse": 0.0}  0.0335
1         train   58600/102467 367:27/275:04      7.719         {"mlm": 7.710451190903683, "mse": 0.0}  0.0361
1         train   58700/102467 368:03/274:25      7.719         {"mlm": 7.712628211098155, "mse": 0.0}  0.0311
1         train   58800/102467 368:39/273:46      7.719         {"mlm": 7.7147750051776365, "mse": 0.0}  0.0343
1         train   58900/102467 369:16/273:08      7.719         {"mlm": 7.715762972299542, "mse": 0.0}  0.0404
1         train   59000/102467 369:52/272:29      7.719         {"mlm": 7.717797129987234, "mse": 0.0}  0.033
1         train   59100/102467 370:28/271:51      7.719         {"mlm": 7.7214057941506375, "mse": 0.0}  0.0517
1         train   59200/102467 371:04/271:12      7.719         {"mlm": 7.72327799980457, "mse": 0.0}  0.0311
1         train   59300/102467 371:41/270:33      7.719         {"mlm": 7.7246990678487, "mse": 0.0}  0.0303
1         train   59400/102467 372:17/269:55      7.719         {"mlm": 7.724411968173817, "mse": 0.0}  0.0336
1         train   59500/102467 372:53/269:16      7.719         {"mlm": 7.724003867669539, "mse": 0.0}  0.0343
1         train   59600/102467 373:29/268:38      7.719         {"mlm": 7.7236717775053245, "mse": 0.0}  0.0359
1         train   59700/102467 374:05/267:59      7.719         {"mlm": 7.722647983229385, "mse": 0.0}  0.038
1         train   59800/102467 374:42/267:20      7.719         {"mlm": 7.721160697246713, "mse": 0.0}  0.0397
1         train   59900/102467 375:18/266:42      7.719         {"mlm": 7.721380800386019, "mse": 0.0}  0.0352
1         train   60000/102467 375:54/266:03      7.719         {"mlm": 7.720339504893652, "mse": 0.0}  0.0329

09/24/2022 01:09:27 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1.pkl
1         valid   1/781        0:29/379:37        7.647           None
1         valid   101/781      0:44/ 4:59         7.700           None
1         valid   201/781      0:59/ 2:52         7.720           None
1         valid   301/781      1:15/ 1:59         7.725           None
1         valid   401/781      1:30/ 1:25         7.726           None
1         valid   501/781      1:45/ 0:59         7.722           None
1         valid   601/781      2:01/ 0:36         7.721           None
1         valid   701/781      2:16/ 0:15         7.720           None
1         valid   781/781      2:31/ 0:00         7.722         {"mlm": 7.7216709346991035, "mse": 0.0, "train": 0.0}  None
1         train   60100/102467 379:01/267:11      7.719         {"mlm": 7.74285050868988, "mse": 0.0}  0.0329
1         train   60200/102467 379:37/266:32      7.719         {"mlm": 7.736357522010803, "mse": 0.0}  0.0312
1         train   60300/102467 380:13/265:53      7.719         {"mlm": 7.727589524586995, "mse": 0.0}  0.0319
1         train   60400/102467 380:49/265:14      7.719         {"mlm": 7.726627055406571, "mse": 0.0}  0.0311
1         train   60500/102467 381:25/264:35      7.719         {"mlm": 7.723636287689209, "mse": 0.0}  0.0351
1         train   60600/102467 382:02/263:56      7.719         {"mlm": 7.722411746184031, "mse": 0.0}  0.0298
1         train   60700/102467 382:38/263:17      7.719         {"mlm": 7.723052966254098, "mse": 0.0}  0.034
1         train   60800/102467 383:14/262:38      7.719         {"mlm": 7.722539779543877, "mse": 0.0}  0.0345
1         train   60900/102467 383:50/261:59      7.719         {"mlm": 7.722939365175035, "mse": 0.0}  0.0432
1         train   61000/102467 384:26/261:20      7.719         {"mlm": 7.723762699127198, "mse": 0.0}  0.0332
1         train   61100/102467 385:02/260:41      7.719         {"mlm": 7.723109658848156, "mse": 0.0}  0.0368
1         train   61200/102467 385:38/260:02      7.719         {"mlm": 7.723103707631429, "mse": 0.0}  0.0339
1         train   61300/102467 386:14/259:23      7.719         {"mlm": 7.721272004934457, "mse": 0.0}  0.037
1         train   61400/102467 386:51/258:44      7.719         {"mlm": 7.72199106659208, "mse": 0.0}  0.0327
1         train   61500/102467 387:27/258:05      7.719         {"mlm": 7.724499073982239, "mse": 0.0}  0.0338
1         train   61600/102467 388:03/257:26      7.719         {"mlm": 7.7245043557882305, "mse": 0.0}  0.0364
1         train   61700/102467 388:39/256:47      7.719         {"mlm": 7.725100959329044, "mse": 0.0}  0.0368
1         train   61800/102467 389:15/256:08      7.719         {"mlm": 7.725991903146108, "mse": 0.0}  0.0368
1         train   61900/102467 389:51/255:30      7.719         {"mlm": 7.724675076133327, "mse": 0.0}  0.0298
1         train   62000/102467 390:28/254:51      7.719         {"mlm": 7.724331387996673, "mse": 0.0}  0.0301
1         train   62100/102467 391:04/254:12      7.719         {"mlm": 7.726991542662033, "mse": 0.0}  0.0326
1         train   62200/102467 391:40/253:33      7.719         {"mlm": 7.726166938417521, "mse": 0.0}  0.0342
1         train   62300/102467 392:16/252:54      7.719         {"mlm": 7.723914487704784, "mse": 0.0}  0.0344
1         train   62400/102467 392:52/252:16      7.719         {"mlm": 7.7245355070683, "mse": 0.0}  0.0324
1         train   62500/102467 393:29/251:37      7.719         {"mlm": 7.723155062757656, "mse": 0.0}  0.0305
1         train   62600/102467 394:05/250:58      7.719         {"mlm": 7.718579034375428, "mse": 0.0}  0.0441
1         train   62700/102467 394:41/250:19      7.719         {"mlm": 7.71747295436941, "mse": 0.0}  0.0332
1         train   62800/102467 395:17/249:41      7.719         {"mlm": 7.71916617559402, "mse": 0.0}  0.0333
1         train   62900/102467 395:53/249:02      7.719         {"mlm": 7.720443966921763, "mse": 0.0}  0.0362
1         train   63000/102467 396:30/248:23      7.719         {"mlm": 7.720264829553522, "mse": 0.0}  0.0314
1         train   63100/102467 397:06/247:44      7.719         {"mlm": 7.719103211809875, "mse": 0.0}  0.0352
1         train   63200/102467 397:42/247:05      7.719         {"mlm": 7.71875428079664, "mse": 0.0}  0.0323
1         train   63300/102467 398:18/246:27      7.719         {"mlm": 7.719491611727391, "mse": 0.0}  0.0331
1         train   63400/102467 398:54/245:48      7.719         {"mlm": 7.717520306841487, "mse": 0.0}  0.0322
1         train   63500/102467 399:30/245:09      7.719         {"mlm": 7.7174925171112205, "mse": 0.0}  0.0319
1         train   63600/102467 400:06/244:31      7.719         {"mlm": 7.717889821253545, "mse": 0.0}  0.0352
1         train   63700/102467 400:43/243:52      7.719         {"mlm": 7.717898477449917, "mse": 0.0}  0.0327
1         train   63800/102467 401:19/243:13      7.719         {"mlm": 7.7166615070535975, "mse": 0.0}  0.0371
1         train   63900/102467 401:55/242:34      7.719         {"mlm": 7.718031511362004, "mse": 0.0}  0.0348
1         train   64000/102467 402:31/241:56      7.719         {"mlm": 7.717657254778665, "mse": 0.0}  0.0325
1         train   64100/102467 403:07/241:17      7.719         {"mlm": 7.721189897887561, "mse": 0.0}  0.0334
1         train   64200/102467 403:43/240:38      7.719         {"mlm": 7.711647965691307, "mse": 0.0}  0.0348
1         train   64300/102467 404:19/240:00      7.719         {"mlm": 7.709181732779381, "mse": 0.0}  0.0335
1         train   64400/102467 404:56/239:21      7.719         {"mlm": 7.706339366471947, "mse": 0.0}  0.0364
1         train   64500/102467 405:32/238:42      7.719         {"mlm": 7.705877909219887, "mse": 0.0}  0.0302
1         train   64600/102467 406:08/238:04      7.719         {"mlm": 7.707108575763511, "mse": 0.0}  0.0329
1         train   64700/102467 406:44/237:25      7.719         {"mlm": 7.707321022164856, "mse": 0.0}  0.034
1         train   64800/102467 407:20/236:46      7.719         {"mlm": 7.708452539635183, "mse": 0.0}  0.0361
1         train   64900/102467 407:57/236:08      7.719         {"mlm": 7.711298630869468, "mse": 0.0}  0.0324
1         train   65000/102467 408:33/235:29      7.719         {"mlm": 7.710565415556302, "mse": 0.0}  0.0302
1         train   65100/102467 409:09/234:51      7.719         {"mlm": 7.710519984337367, "mse": 0.0}  0.0326
1         train   65200/102467 409:45/234:12      7.719         {"mlm": 7.712057211960297, "mse": 0.0}  0.0399
1         train   65300/102467 410:21/233:34      7.719         {"mlm": 7.713396363339181, "mse": 0.0}  0.0402
1         train   65400/102467 410:57/232:55      7.719         {"mlm": 7.714837559302989, "mse": 0.0}  0.0416
1         train   65500/102467 411:34/232:16      7.719         {"mlm": 7.713593713431874, "mse": 0.0}  0.0333
1         train   65600/102467 412:10/231:38      7.719         {"mlm": 7.713879593919604, "mse": 0.0}  0.0362
1         train   65700/102467 412:46/230:59      7.719         {"mlm": 7.7140778551674565, "mse": 0.0}  0.0314
1         train   65800/102467 413:22/230:21      7.719         {"mlm": 7.71379604302471, "mse": 0.0}  0.0316
1         train   65900/102467 413:59/229:42      7.719         {"mlm": 7.714201284032727, "mse": 0.0}  0.0341
1         train   66000/102467 414:35/229:04      7.719         {"mlm": 7.714744675028193, "mse": 0.0}  0.0365
1         train   66100/102467 415:11/228:25      7.719         {"mlm": 7.734955094524266, "mse": 0.0}  0.0346
1         train   66200/102467 415:47/227:47      7.719         {"mlm": 7.722141643466078, "mse": 0.0}  0.0363
1         train   66300/102467 416:24/227:08      7.719         {"mlm": 7.719440930620187, "mse": 0.0}  0.0321
1         train   66400/102467 417:00/226:30      7.719         {"mlm": 7.715105556420776, "mse": 0.0}  0.0307
1         train   66500/102467 417:36/225:52      7.719         {"mlm": 7.716582303075963, "mse": 0.0}  0.0361
1         train   66600/102467 418:12/225:13      7.719         {"mlm": 7.718105356896942, "mse": 0.0}  0.0344
1         train   66700/102467 418:49/224:35      7.719         {"mlm": 7.7178670378292304, "mse": 0.0}  0.0315
1         train   66800/102467 419:25/223:56      7.719         {"mlm": 7.7174827451239265, "mse": 0.0}  0.0382
1         train   66900/102467 420:01/223:18      7.719         {"mlm": 7.715459944811155, "mse": 0.0}  0.0301
1         train   67000/102467 420:37/222:39      7.719         {"mlm": 7.716314142661444, "mse": 0.0}  0.0301
1         train   67100/102467 421:13/222:01      7.719         {"mlm": 7.71897547529737, "mse": 0.0}  0.0325
1         train   67200/102467 421:50/221:22      7.719         {"mlm": 7.72034378019889, "mse": 0.0}  0.0336
1         train   67300/102467 422:26/220:44      7.719         {"mlm": 7.720278889928861, "mse": 0.0}  0.0318
1         train   67400/102467 423:02/220:05      7.719         {"mlm": 7.72103612288118, "mse": 0.0}  0.0373
1         train   67500/102467 423:38/219:27      7.719         {"mlm": 7.7199877625556494, "mse": 0.0}  0.0359
1         train   67600/102467 424:14/218:49      7.719         {"mlm": 7.721257112439752, "mse": 0.0}  0.0335
1         train   67700/102467 424:50/218:10      7.719         {"mlm": 7.721943086501635, "mse": 0.0}  0.0311
1         train   67800/102467 425:26/217:32      7.719         {"mlm": 7.720895881578533, "mse": 0.0}  0.0322
1         train   67900/102467 426:02/216:53      7.719         {"mlm": 7.72112717072965, "mse": 0.0}  0.0327
1         train   68000/102467 426:39/216:15      7.719         {"mlm": 7.721213622993389, "mse": 0.0}  0.0321
1         train   68100/102467 427:15/215:36      7.719         {"mlm": 7.730957493185997, "mse": 0.0}  0.0335
1         train   68200/102467 427:51/214:58      7.719         {"mlm": 7.716436291227535, "mse": 0.0}  0.032
1         train   68300/102467 428:27/214:20      7.719         {"mlm": 7.722042296383832, "mse": 0.0}  0.0315
1         train   68400/102467 429:03/213:41      7.719         {"mlm": 7.71833965152201, "mse": 0.0}  0.0374
1         train   68500/102467 429:39/213:03      7.719         {"mlm": 7.72481277104347, "mse": 0.0}  0.0307
1         train   68600/102467 430:15/212:25      7.719         {"mlm": 7.719150631219748, "mse": 0.0}  0.0362
1         train   68700/102467 430:52/211:46      7.719         {"mlm": 7.721166459308273, "mse": 0.0}  0.0369
1         train   68800/102467 431:28/211:08      7.719         {"mlm": 7.7211175192540615, "mse": 0.0}  0.0297
1         train   68900/102467 432:04/210:30      7.719         {"mlm": 7.7165760062634945, "mse": 0.0}  0.032
1         train   69000/102467 432:40/209:51      7.719         {"mlm": 7.7164327303568525, "mse": 0.0}  0.0304
1         train   69100/102467 433:17/209:13      7.719         {"mlm": 7.716303419892806, "mse": 0.0}  0.031
1         train   69200/102467 433:53/208:35      7.719         {"mlm": 7.716404118266792, "mse": 0.0}  0.0543
1         train   69300/102467 434:29/207:56      7.719         {"mlm": 7.71547203078682, "mse": 0.0}  0.0304
1         train   69400/102467 435:06/207:18      7.719         {"mlm": 7.714409528284155, "mse": 0.0}  0.037
1         train   69500/102467 435:42/206:40      7.719         {"mlm": 7.71485865243616, "mse": 0.0}  0.0347
1         train   69600/102467 436:18/206:02      7.719         {"mlm": 7.7147296341439535, "mse": 0.0}  0.0363
1         train   69700/102467 436:54/205:24      7.719         {"mlm": 7.714461001022807, "mse": 0.0}  0.0319
1         train   69800/102467 437:31/204:45      7.719         {"mlm": 7.71477509049371, "mse": 0.0}  0.0345
1         train   69900/102467 438:07/204:07      7.719         {"mlm": 7.714237422882756, "mse": 0.0}  0.036
1         train   70000/102467 438:43/203:29      7.719         {"mlm": 7.714075253339473, "mse": 0.0}  0.0337

09/24/2022 02:12:16 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1.pkl
1         valid   1/781        0:35/464:54        7.497           None
1         valid   101/781      0:51/ 5:43         7.697           None
1         valid   201/781      1:06/ 3:11         7.708           None
1         valid   301/781      1:21/ 2:10         7.709           None
1         valid   401/781      1:37/ 1:31         7.716           None
1         valid   501/781      1:52/ 1:02         7.712           None
1         valid   601/781      2:07/ 0:38         7.716           None
1         valid   701/781      2:22/ 0:16         7.716           None
1         valid   781/781      2:37/ 0:00         7.719         {"mlm": 7.719270166453265, "mse": 0.0, "train": 0.0}  None
1         train   70100/102467 441:56/204:03      7.719         {"mlm": 7.715771980285645, "mse": 0.0}  0.0376
1         train   70200/102467 442:32/203:24      7.719         {"mlm": 7.705150730609894, "mse": 0.0}  0.0328
1         train   70300/102467 443:08/202:46      7.719         {"mlm": 7.719496272404989, "mse": 0.0}  0.036
1         train   70400/102467 443:44/202:07      7.719         {"mlm": 7.726048055887222, "mse": 0.0}  0.0372
1         train   70500/102467 444:20/201:28      7.719         {"mlm": 7.724738733291626, "mse": 0.0}  0.0315
1         train   70600/102467 444:56/200:50      7.719         {"mlm": 7.727265768845876, "mse": 0.0}  0.0325
1         train   70700/102467 445:32/200:11      7.719         {"mlm": 7.726168297358922, "mse": 0.0}  0.0363
1         train   70800/102467 446:08/199:32      7.719         {"mlm": 7.7220867264270785, "mse": 0.0}  0.031
1         train   70900/102467 446:44/198:54      7.719         {"mlm": 7.717286313904657, "mse": 0.0}  0.0323
1         train   71000/102467 447:20/198:15      7.719         {"mlm": 7.717964201450348, "mse": 0.0}  0.0318
1         train   71100/102467 447:56/197:37      7.719         {"mlm": 7.719470893252979, "mse": 0.0}  0.0283
1         train   71200/102467 448:32/196:58      7.719         {"mlm": 7.722452026605606, "mse": 0.0}  0.0295
1         train   71300/102467 449:08/196:19      7.719         {"mlm": 7.721328441546514, "mse": 0.0}  0.0308
1         train   71400/102467 449:44/195:41      7.719         {"mlm": 7.7202916809490745, "mse": 0.0}  0.0358
1         train   71500/102467 450:20/195:02      7.719         {"mlm": 7.720751456260681, "mse": 0.0}  0.0302
1         train   71600/102467 450:57/194:24      7.719         {"mlm": 7.721333257555962, "mse": 0.0}  0.0335
1         train   71700/102467 451:33/193:45      7.719         {"mlm": 7.722317186523886, "mse": 0.0}  0.0351
1         train   71800/102467 452:09/193:07      7.719         {"mlm": 7.722291175789303, "mse": 0.0}  0.0291
1         train   71900/102467 452:45/192:29      7.719         {"mlm": 7.721915762298986, "mse": 0.0}  0.0304
1         train   72000/102467 453:22/191:50      7.719         {"mlm": 7.721290368318558, "mse": 0.0}  0.0324
1         train   72100/102467 453:58/191:12      7.719         {"mlm": 7.703609745911877, "mse": 0.0}  0.0323
1         train   72200/102467 454:34/190:33      7.719         {"mlm": 7.71015084328963, "mse": 0.0}  0.0353
1         train   72300/102467 455:11/189:55      7.719         {"mlm": 7.713094932977173, "mse": 0.0}  0.0328
1         train   72400/102467 455:47/189:17      7.719         {"mlm": 7.716073595491567, "mse": 0.0}  0.0356
1         train   72500/102467 456:23/188:38      7.719         {"mlm": 7.713538124948322, "mse": 0.0}  0.0367
1         train   72600/102467 456:59/188:00      7.719         {"mlm": 7.708414918393245, "mse": 0.0}  0.0303
1         train   72700/102467 457:36/187:21      7.719         {"mlm": 7.707204879438757, "mse": 0.0}  0.0368
1         train   72800/102467 458:12/186:43      7.719         {"mlm": 7.707997151996675, "mse": 0.0}  0.0345
1         train   72900/102467 458:48/186:05      7.719         {"mlm": 7.710001251721408, "mse": 0.0}  0.0308
1         train   73000/102467 459:25/185:26      7.719         {"mlm": 7.71100364910351, "mse": 0.0}  0.0295
1         train   73100/102467 460:01/184:48      7.719         {"mlm": 7.710100203454223, "mse": 0.0}  0.0334
1         train   73200/102467 460:37/184:10      7.719         {"mlm": 7.709349913831748, "mse": 0.0}  0.0314
1         train   73300/102467 461:13/183:31      7.719         {"mlm": 7.71269807198857, "mse": 0.0}  0.0318
1         train   73400/102467 461:50/182:53      7.719         {"mlm": 7.71178218686129, "mse": 0.0}  0.0282
1         train   73500/102467 462:26/182:15      7.719         {"mlm": 7.711997634971675, "mse": 0.0}  0.0331
1         train   73600/102467 463:02/181:36      7.719         {"mlm": 7.7148089098736525, "mse": 0.0}  0.0309
1         train   73700/102467 463:38/180:58      7.719         {"mlm": 7.715050475607205, "mse": 0.0}  0.0318
1         train   73800/102467 464:14/180:19      7.719         {"mlm": 7.715878691522197, "mse": 0.0}  0.0334
1         train   73900/102467 464:50/179:41      7.719         {"mlm": 7.7157623462767395, "mse": 0.0}  0.0351
1         train   74000/102467 465:27/179:03      7.719         {"mlm": 7.715108210471584, "mse": 0.0}  0.0398
1         train   74100/102467 466:03/178:24      7.719         {"mlm": 7.709915827731697, "mse": 0.0}  0.0343
1         train   74200/102467 466:39/177:46      7.719         {"mlm": 7.7186935550034645, "mse": 0.0}  0.0343
1         train   74300/102467 467:15/177:08      7.719         {"mlm": 7.7177099205503525, "mse": 0.0}  0.0324
1         train   74400/102467 467:51/176:29      7.719         {"mlm": 7.71472786539164, "mse": 0.0}  0.0332
1         train   74500/102467 468:27/175:51      7.719         {"mlm": 7.718510783819789, "mse": 0.0}  0.0334
1         train   74600/102467 469:04/175:13      7.719         {"mlm": 7.719067244226717, "mse": 0.0}  0.0322
1         train   74700/102467 469:40/174:34      7.719         {"mlm": 7.718351699561308, "mse": 0.0}  0.0379
1         train   74800/102467 470:16/173:56      7.719         {"mlm": 7.718151483918192, "mse": 0.0}  0.036
1         train   74900/102467 470:52/173:18      7.719         {"mlm": 7.719474728760582, "mse": 0.0}  0.0283
1         train   75000/102467 471:28/172:40      7.719         {"mlm": 7.719110247128473, "mse": 0.0}  0.0331
1         train   75100/102467 472:05/172:01      7.719         {"mlm": 7.718563121524665, "mse": 0.0}  0.0334
1         train   75200/102467 472:41/171:23      7.719         {"mlm": 7.719476150948934, "mse": 0.0}  0.0358
1         train   75300/102467 473:17/170:45      7.719         {"mlm": 7.719224907400428, "mse": 0.0}  0.0333
1         train   75400/102467 473:53/170:07      7.719         {"mlm": 7.720540957389471, "mse": 0.0}  0.028
1         train   75500/102467 474:30/169:28      7.719         {"mlm": 7.7224453326379345, "mse": 0.0}  0.0286
1         train   75600/102467 475:06/168:50      7.719         {"mlm": 7.723074573151608, "mse": 0.0}  0.0294
1         train   75700/102467 475:42/168:12      7.719         {"mlm": 7.7219372963877255, "mse": 0.0}  0.041
1         train   75800/102467 476:18/167:34      7.719         {"mlm": 7.721816404775464, "mse": 0.0}  0.0337
1         train   75900/102467 476:55/166:56      7.719         {"mlm": 7.722483725392027, "mse": 0.0}  0.037
1         train   76000/102467 477:31/166:17      7.719         {"mlm": 7.722766912496603, "mse": 0.0}  0.0317
1         train   76100/102467 478:07/165:39      7.719         {"mlm": 7.73105530886306, "mse": 0.0}  0.0326
1         train   76200/102467 478:43/165:01      7.719         {"mlm": 7.7270947543497615, "mse": 0.0}  0.0314
1         train   76300/102467 479:20/164:23      7.719         {"mlm": 7.722814299843528, "mse": 0.0}  0.0379
1         train   76400/102467 479:56/163:45      7.719         {"mlm": 7.716953947802335, "mse": 0.0}  0.0283
1         train   76500/102467 480:32/163:06      7.719         {"mlm": 7.715837489191435, "mse": 0.0}  0.0337
1         train   76600/102467 481:08/162:28      7.719         {"mlm": 7.7176363336381, "mse": 0.0}  0.0305
1         train   76700/102467 481:45/161:50      7.719         {"mlm": 7.720557419436221, "mse": 0.0}  0.0332
1         train   76800/102467 482:21/161:12      7.719         {"mlm": 7.721865802965918, "mse": 0.0}  0.0361
1         train   76900/102467 482:57/160:34      7.719         {"mlm": 7.721353256051224, "mse": 0.0}  0.0287
1         train   77000/102467 483:33/159:55      7.719         {"mlm": 7.720435592092746, "mse": 0.0}  0.0296
1         train   77100/102467 484:09/159:17      7.719         {"mlm": 7.721462597060225, "mse": 0.0}  0.0302
1         train   77200/102467 484:45/158:39      7.719         {"mlm": 7.723067163325591, "mse": 0.0}  0.0297
1         train   77300/102467 485:22/158:01      7.719         {"mlm": 7.72189712634708, "mse": 0.0}  0.034
1         train   77400/102467 485:58/157:23      7.719         {"mlm": 7.72310797043502, "mse": 0.0}  0.0425
1         train   77500/102467 486:34/156:45      7.719         {"mlm": 7.72468920365603, "mse": 0.0}  0.0344
1         train   77600/102467 487:10/156:06      7.719         {"mlm": 7.725271270360212, "mse": 0.0}  0.0314
1         train   77700/102467 487:46/155:28      7.719         {"mlm": 7.724884152623156, "mse": 0.0}  0.0391
1         train   77800/102467 488:22/154:50      7.719         {"mlm": 7.723497047647212, "mse": 0.0}  0.0306
1         train   77900/102467 488:58/154:12      7.719         {"mlm": 7.722350158751733, "mse": 0.0}  0.0357
1         train   78000/102467 489:35/153:34      7.719         {"mlm": 7.721733101857682, "mse": 0.0}  0.0337
1         train   78100/102467 490:11/152:56      7.719         {"mlm": 7.7216696590185165, "mse": 0.0}  0.0337
1         train   78200/102467 490:47/152:18      7.719         {"mlm": 7.730603578139324, "mse": 0.0}  0.0452
1         train   78300/102467 491:23/151:39      7.719         {"mlm": 7.726024067079699, "mse": 0.0}  0.0323
1         train   78400/102467 491:59/151:01      7.719         {"mlm": 7.720070111631143, "mse": 0.0}  0.0292
1         train   78500/102467 492:35/150:23      7.719         {"mlm": 7.723713620055106, "mse": 0.0}  0.0339
1         train   78600/102467 493:11/149:45      7.719         {"mlm": 7.727348560454861, "mse": 0.0}  0.0297
1         train   78700/102467 493:48/149:07      7.719         {"mlm": 7.727406386671396, "mse": 0.0}  0.0291
1         train   78800/102467 494:24/148:29      7.719         {"mlm": 7.72493291620034, "mse": 0.0}  0.0319
1         train   78900/102467 495:00/147:51      7.719         {"mlm": 7.725248210132122, "mse": 0.0}  0.0323
1         train   79000/102467 495:36/147:13      7.719         {"mlm": 7.725611097362625, "mse": 0.0}  0.0295
1         train   79100/102467 496:12/146:35      7.719         {"mlm": 7.726611928783194, "mse": 0.0}  0.0298
1         train   79200/102467 496:48/145:57      7.719         {"mlm": 7.725265165635176, "mse": 0.0}  0.0284
1         train   79300/102467 497:25/145:19      7.719         {"mlm": 7.722496637591609, "mse": 0.0}  0.0312
1         train   79400/102467 498:01/144:40      7.719         {"mlm": 7.722754246867488, "mse": 0.0}  0.0328
1         train   79500/102467 498:37/144:02      7.719         {"mlm": 7.724076292731545, "mse": 0.0}  0.037
1         train   79600/102467 499:13/143:24      7.719         {"mlm": 7.724022056524616, "mse": 0.0}  0.0347
1         train   79700/102467 499:49/142:46      7.719         {"mlm": 7.724449323595695, "mse": 0.0}  0.032
1         train   79800/102467 500:26/142:08      7.719         {"mlm": 7.723932937152667, "mse": 0.0}  0.0311
1         train   79900/102467 501:02/141:30      7.719         {"mlm": 7.723805130282535, "mse": 0.0}  0.0361
1         train   80000/102467 501:38/140:52      7.719         {"mlm": 7.7234961076346575, "mse": 0.0}  0.0281

09/24/2022 03:15:11 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1.pkl
1         valid   1/781        0:38/506:29        7.680           None
1         valid   101/781      0:54/ 6:05         7.705           None
1         valid   201/781      1:09/ 3:20         7.716           None
1         valid   301/781      1:24/ 2:15         7.723           None
1         valid   401/781      1:40/ 1:34         7.726           None
1         valid   501/781      1:55/ 1:04         7.725           None
1         valid   601/781      2:10/ 0:39         7.723           None
1         valid   701/781      2:26/ 0:16         7.724           None
1         valid   781/781      2:40/ 0:00         7.719         {"mlm": 7.718950064020486, "mse": 0.0, "train": 0.0}  None
1         train   80100/102467 504:55/140:59      7.719         {"mlm": 7.707816462516785, "mse": 0.0}  0.0354
1         train   80200/102467 505:31/140:21      7.719         {"mlm": 7.7182763171195985, "mse": 0.0}  0.0334
1         train   80300/102467 506:07/139:42      7.719         {"mlm": 7.7177760251363114, "mse": 0.0}  0.0341
1         train   80400/102467 506:43/139:04      7.719         {"mlm": 7.723442217111588, "mse": 0.0}  0.027
1         train   80500/102467 507:19/138:26      7.719         {"mlm": 7.723540509223938, "mse": 0.0}  0.0329
1         train   80600/102467 507:55/137:48      7.719         {"mlm": 7.722336908181508, "mse": 0.0}  0.032
1         train   80700/102467 508:31/137:09      7.719         {"mlm": 7.71976490020752, "mse": 0.0}  0.0305
1         train   80800/102467 509:07/136:31      7.719         {"mlm": 7.7222276878356935, "mse": 0.0}  0.0305
1         train   80900/102467 509:43/135:53      7.719         {"mlm": 7.719521418677436, "mse": 0.0}  0.0365
1         train   81000/102467 510:19/135:14      7.719         {"mlm": 7.719931122303009, "mse": 0.0}  0.0411
1         train   81100/102467 510:55/134:36      7.719         {"mlm": 7.719664003198797, "mse": 0.0}  0.0433
1         train   81200/102467 511:31/133:58      7.719         {"mlm": 7.719791315396627, "mse": 0.0}  0.0375
1         train   81300/102467 512:07/133:20      7.719         {"mlm": 7.71948432628925, "mse": 0.0}  0.0318
1         train   81400/102467 512:43/132:41      7.719         {"mlm": 7.719767844676971, "mse": 0.0}  0.0307
1         train   81500/102467 513:19/132:03      7.719         {"mlm": 7.719759256998698, "mse": 0.0}  0.03
1         train   81600/102467 513:56/131:25      7.719         {"mlm": 7.719013369679451, "mse": 0.0}  0.0346
1         train   81700/102467 514:32/130:47      7.719         {"mlm": 7.716985501401565, "mse": 0.0}  0.0307
1         train   81800/102467 515:08/130:09      7.719         {"mlm": 7.7168997987111405, "mse": 0.0}  0.0302
1         train   81900/102467 515:44/129:30      7.719         {"mlm": 7.7172072420622175, "mse": 0.0}  0.0337
1         train   82000/102467 516:20/128:52      7.719         {"mlm": 7.715812837839127, "mse": 0.0}  0.0321
1         train   82100/102467 516:57/128:14      7.719         {"mlm": 7.71847711909901, "mse": 0.0}  0.0314
1         train   82200/102467 517:33/127:36      7.719         {"mlm": 7.726405345015789, "mse": 0.0}  0.0317
1         train   82300/102467 518:09/126:58      7.719         {"mlm": 7.721797443951253, "mse": 0.0}  0.0354
1         train   82400/102467 518:45/126:20      7.719         {"mlm": 7.721822138717001, "mse": 0.0}  0.0313
1         train   82500/102467 519:21/125:41      7.719         {"mlm": 7.716633999276018, "mse": 0.0}  0.0306
1         train   82600/102467 519:57/125:03      7.719         {"mlm": 7.7162318142109205, "mse": 0.0}  0.0334
1         train   82700/102467 520:34/124:25      7.719         {"mlm": 7.716622229127243, "mse": 0.0}  0.0312
1         train   82800/102467 521:10/123:47      7.719         {"mlm": 7.71377347974813, "mse": 0.0}  0.033
1         train   82900/102467 521:46/123:09      7.719         {"mlm": 7.7142611531182315, "mse": 0.0}  0.0286
1         train   83000/102467 522:22/122:31      7.719         {"mlm": 7.714741337406743, "mse": 0.0}  0.0353
1         train   83100/102467 522:58/121:53      7.719         {"mlm": 7.716068666560526, "mse": 0.0}  0.0294
1         train   83200/102467 523:35/121:14      7.719         {"mlm": 7.71512169436279, "mse": 0.0}  0.0332
1         train   83300/102467 524:11/120:36      7.719         {"mlm": 7.715896986373302, "mse": 0.0}  0.0312
1         train   83400/102467 524:47/119:58      7.719         {"mlm": 7.715511312137083, "mse": 0.0}  0.0368
1         train   83500/102467 525:23/119:20      7.719         {"mlm": 7.715127987572159, "mse": 0.0}  0.0411
1         train   83600/102467 525:59/118:42      7.719         {"mlm": 7.7152659533693315, "mse": 0.0}  0.032
1         train   83700/102467 526:36/118:04      7.719         {"mlm": 7.716040974718602, "mse": 0.0}  0.0295
1         train   83800/102467 527:12/117:26      7.719         {"mlm": 7.7161158855389465, "mse": 0.0}  0.03
1         train   83900/102467 527:48/116:48      7.719         {"mlm": 7.716002770886414, "mse": 0.0}  0.0311
1         train   84000/102467 528:24/116:10      7.719         {"mlm": 7.715719764264838, "mse": 0.0}  0.0297
1         train   84100/102467 529:00/115:32      7.719         {"mlm": 7.711383031339062, "mse": 0.0}  0.0306
1         train   84200/102467 529:37/114:53      7.719         {"mlm": 7.70880096849769, "mse": 0.0}  0.0317
1         train   84300/102467 530:13/114:15      7.719         {"mlm": 7.712710004524896, "mse": 0.0}  0.0305
1         train   84400/102467 530:49/113:37      7.719         {"mlm": 7.706623236737658, "mse": 0.0}  0.0294
1         train   84500/102467 531:25/112:59      7.719         {"mlm": 7.709133111808194, "mse": 0.0}  0.0356
1         train   84600/102467 532:01/112:21      7.719         {"mlm": 7.714505946755808, "mse": 0.0}  0.0298
1         train   84700/102467 532:38/111:43      7.719         {"mlm": 7.715478944915072, "mse": 0.0}  0.0324
1         train   84800/102467 533:14/111:05      7.719         {"mlm": 7.717422024050452, "mse": 0.0}  0.0288
1         train   84900/102467 533:50/110:27      7.719         {"mlm": 7.7167511623527, "mse": 0.0}  0.0322
1         train   85000/102467 534:26/109:49      7.719         {"mlm": 7.716527931675882, "mse": 0.0}  0.0325
1         train   85100/102467 535:02/109:11      7.719         {"mlm": 7.7187851899742865, "mse": 0.0}  0.0293
1         train   85200/102467 535:38/108:33      7.719         {"mlm": 7.718142065261561, "mse": 0.0}  0.0319
1         train   85300/102467 536:15/107:55      7.719         {"mlm": 7.718229282434989, "mse": 0.0}  0.0285
1         train   85400/102467 536:51/107:17      7.719         {"mlm": 7.718601111178746, "mse": 0.0}  0.0296
1         train   85500/102467 537:27/106:39      7.719         {"mlm": 7.7197347753675025, "mse": 0.0}  0.0312
1         train   85600/102467 538:03/106:01      7.719         {"mlm": 7.7205406360244275, "mse": 0.0}  0.0381
1         train   85700/102467 538:39/105:23      7.719         {"mlm": 7.720590741951699, "mse": 0.0}  0.0347
1         train   85800/102467 539:16/104:45      7.719         {"mlm": 7.720426915882692, "mse": 0.0}  0.032
1         train   85900/102467 539:52/104:07      7.719         {"mlm": 7.720750895390646, "mse": 0.0}  0.032
1         train   86000/102467 540:28/103:29      7.719         {"mlm": 7.720496071232213, "mse": 0.0}  0.0269
1         train   86100/102467 541:05/102:51      7.719         {"mlm": 7.731475864489054, "mse": 0.0}  0.0292
1         train   86200/102467 541:41/102:13      7.719         {"mlm": 7.717320173524963, "mse": 0.0}  0.0309
1         train   86300/102467 542:17/101:35      7.719         {"mlm": 7.711730110926259, "mse": 0.0}  0.0341
1         train   86400/102467 542:53/100:57      7.719         {"mlm": 7.716712496442819, "mse": 0.0}  0.0371
1         train   86500/102467 543:30/100:19      7.719         {"mlm": 7.7163052597276165, "mse": 0.0}  0.0337
1         train   86600/102467 544:06/99:41       7.719         {"mlm": 7.713157262434712, "mse": 0.0}  0.0341
1         train   86700/102467 544:42/99:03       7.719         {"mlm": 7.711134065685519, "mse": 0.0}  0.036
1         train   86800/102467 545:19/98:25       7.719         {"mlm": 7.710862701779776, "mse": 0.0}  0.0341
1         train   86900/102467 545:55/97:47       7.719         {"mlm": 7.711250654431091, "mse": 0.0}  0.0332
1         train   87000/102467 546:31/97:09       7.719         {"mlm": 7.710281393592552, "mse": 0.0}  0.0306
1         train   87100/102467 547:08/96:31       7.719         {"mlm": 7.712545557900135, "mse": 0.0}  0.0301
1         train   87200/102467 547:44/95:53       7.719         {"mlm": 7.713962292412269, "mse": 0.0}  0.0322
1         train   87300/102467 548:20/95:15       7.719         {"mlm": 7.712565611028267, "mse": 0.0}  0.0316
1         train   87400/102467 548:56/94:37       7.719         {"mlm": 7.711674798789328, "mse": 0.0}  0.0399
1         train   87500/102467 549:32/94:00       7.719         {"mlm": 7.713608334680837, "mse": 0.0}  0.0295
1         train   87600/102467 550:08/93:22       7.719         {"mlm": 7.715258844658667, "mse": 0.0}  0.0311
1         train   87700/102467 550:45/92:44       7.719         {"mlm": 7.714874644664434, "mse": 0.0}  0.0302
1         train   87800/102467 551:21/92:06       7.719         {"mlm": 7.717072002875785, "mse": 0.0}  0.0356
1         train   87900/102467 551:57/91:28       7.719         {"mlm": 7.716688053068765, "mse": 0.0}  0.0355
1         train   88000/102467 552:33/90:50       7.719         {"mlm": 7.716713746786715, "mse": 0.0}  0.0328
1         train   88100/102467 553:09/90:12       7.719         {"mlm": 7.715765575567882, "mse": 0.0}  0.0384
1         train   88200/102467 553:45/89:34       7.719         {"mlm": 7.713874461699505, "mse": 0.0}  0.0323
1         train   88300/102467 554:21/88:56       7.719         {"mlm": 7.718867345436199, "mse": 0.0}  0.0262
1         train   88400/102467 554:57/88:18       7.719         {"mlm": 7.724419189221932, "mse": 0.0}  0.0346
1         train   88500/102467 555:33/87:40       7.719         {"mlm": 7.724925137335254, "mse": 0.0}  0.0318
1         train   88600/102467 556:10/87:02       7.719         {"mlm": 7.725716401266571, "mse": 0.0}  0.0327
1         train   88700/102467 556:46/86:24       7.719         {"mlm": 7.722546583619611, "mse": 0.0}  0.0323
1         train   88800/102467 557:22/85:47       7.719         {"mlm": 7.723667481436801, "mse": 0.0}  0.0284
1         train   88900/102467 557:58/85:09       7.719         {"mlm": 7.721894775650331, "mse": 0.0}  0.0299
1         train   89000/102467 558:34/84:31       7.719         {"mlm": 7.7184278337831, "mse": 0.0}  0.0345
1         train   89100/102467 559:10/83:53       7.719         {"mlm": 7.719853553893793, "mse": 0.0}  0.0331
1         train   89200/102467 559:47/83:15       7.719         {"mlm": 7.718991227771925, "mse": 0.0}  0.0314
1         train   89300/102467 560:23/82:37       7.719         {"mlm": 7.718409320454539, "mse": 0.0}  0.0306
1         train   89400/102467 560:59/81:59       7.719         {"mlm": 7.718206029908363, "mse": 0.0}  0.0325
1         train   89500/102467 561:35/81:21       7.719         {"mlm": 7.718513106279832, "mse": 0.0}  0.0327
1         train   89600/102467 562:12/80:44       7.719         {"mlm": 7.717478387337878, "mse": 0.0}  0.0319
1         train   89700/102467 562:48/80:06       7.719         {"mlm": 7.719012618346034, "mse": 0.0}  0.0342
1         train   89800/102467 563:24/79:28       7.719         {"mlm": 7.7197392703695655, "mse": 0.0}  0.034
1         train   89900/102467 564:00/78:50       7.719         {"mlm": 7.720362657484626, "mse": 0.0}  0.0309
1         train   90000/102467 564:37/78:12       7.719         {"mlm": 7.721542339047831, "mse": 0.0}  0.0291

09/24/2022 04:18:09 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1.pkl
1         valid   1/781        0:40/531:26        7.747           None
1         valid   101/781      0:56/ 6:17         7.711           None
1         valid   201/781      1:11/ 3:26         7.727           None
1         valid   301/781      1:26/ 2:18         7.723           None
1         valid   401/781      1:41/ 1:36         7.726           None
1         valid   501/781      1:57/ 1:05         7.725           None
1         valid   601/781      2:12/ 0:39         7.723           None
1         valid   701/781      2:27/ 0:16         7.723           None
1         valid   781/781      2:42/ 0:00         7.719         {"mlm": 7.718950064020486, "mse": 0.0, "train": 0.0}  None
1         train   90100/102467 567:55/77:57       7.719         {"mlm": 7.71181282043457, "mse": 0.0}  0.0323
1         train   90200/102467 568:31/77:19       7.719         {"mlm": 7.713755147457123, "mse": 0.0}  0.0303
1         train   90300/102467 569:07/76:40       7.719         {"mlm": 7.716561694145202, "mse": 0.0}  0.0287
1         train   90400/102467 569:43/76:02       7.719         {"mlm": 7.712742048501968, "mse": 0.0}  0.0301
1         train   90500/102467 570:18/75:24       7.719         {"mlm": 7.715543922424317, "mse": 0.0}  0.0294
1         train   90600/102467 570:54/74:46       7.719         {"mlm": 7.715764268239339, "mse": 0.0}  0.0281
1         train   90700/102467 571:30/74:08       7.719         {"mlm": 7.7156082643781385, "mse": 0.0}  0.03
1         train   90800/102467 572:06/73:30       7.719         {"mlm": 7.714229630827904, "mse": 0.0}  0.0291
1         train   90900/102467 572:42/72:52       7.719         {"mlm": 7.716218367152744, "mse": 0.0}  0.0362
1         train   91000/102467 573:18/72:14       7.719         {"mlm": 7.717105128765106, "mse": 0.0}  0.0288
1         train   91100/102467 573:54/71:36       7.719         {"mlm": 7.7170212273164225, "mse": 0.0}  0.0286
1         train   91200/102467 574:31/70:58       7.719         {"mlm": 7.716661233504613, "mse": 0.0}  0.0336
1         train   91300/102467 575:07/70:20       7.719         {"mlm": 7.7183460191579965, "mse": 0.0}  0.0324
1         train   91400/102467 575:43/69:42       7.719         {"mlm": 7.718076971939632, "mse": 0.0}  0.0292
1         train   91500/102467 576:19/69:04       7.719         {"mlm": 7.7186927207311, "mse": 0.0}  0.0335
1         train   91600/102467 576:55/68:26       7.719         {"mlm": 7.718459959924221, "mse": 0.0}  0.0295
1         train   91700/102467 577:31/67:48       7.719         {"mlm": 7.7180677506502935, "mse": 0.0}  0.0385
1         train   91800/102467 578:07/67:10       7.719         {"mlm": 7.718919175995721, "mse": 0.0}  0.0268
1         train   91900/102467 578:43/66:32       7.719         {"mlm": 7.718312862797787, "mse": 0.0}  0.0327
1         train   92000/102467 579:20/65:54       7.719         {"mlm": 7.71891747379303, "mse": 0.0}  0.0345
1         train   92100/102467 579:56/65:16       7.719         {"mlm": 7.710601117875841, "mse": 0.0}  0.0277
1         train   92200/102467 580:32/64:38       7.719         {"mlm": 7.718714905743623, "mse": 0.0}  0.0337
1         train   92300/102467 581:08/64:00       7.719         {"mlm": 7.718303356680981, "mse": 0.0}  0.0294
1         train   92400/102467 581:45/63:22       7.719         {"mlm": 7.716292926243374, "mse": 0.0}  0.03
1         train   92500/102467 582:21/62:45       7.719         {"mlm": 7.718802125277166, "mse": 0.0}  0.036
1         train   92600/102467 582:57/62:07       7.719         {"mlm": 7.718957808658555, "mse": 0.0}  0.0305
1         train   92700/102467 583:34/61:29       7.719         {"mlm": 7.718455414915289, "mse": 0.0}  0.0275
1         train   92800/102467 584:10/60:51       7.719         {"mlm": 7.720529633857431, "mse": 0.0}  0.0265
1         train   92900/102467 584:46/60:13       7.719         {"mlm": 7.723987790978658, "mse": 0.0}  0.0295
1         train   93000/102467 585:23/59:35       7.719         {"mlm": 7.722141238184901, "mse": 0.0}  0.0334
1         train   93100/102467 585:59/58:57       7.719         {"mlm": 7.723299433905174, "mse": 0.0}  0.0322
1         train   93200/102467 586:35/58:19       7.719         {"mlm": 7.724023831298294, "mse": 0.0}  0.0303
1         train   93300/102467 587:12/57:41       7.719         {"mlm": 7.720969167831221, "mse": 0.0}  0.0338
1         train   93400/102467 587:48/57:03       7.719         {"mlm": 7.721610344674777, "mse": 0.0}  0.0262
1         train   93500/102467 588:24/56:25       7.719         {"mlm": 7.720537202210328, "mse": 0.0}  0.029
1         train   93600/102467 589:00/55:47       7.719         {"mlm": 7.7194063828988995, "mse": 0.0}  0.0293
1         train   93700/102467 589:37/55:10       7.719         {"mlm": 7.7199081181778215, "mse": 0.0}  0.0286
1         train   93800/102467 590:13/54:32       7.719         {"mlm": 7.719398324127791, "mse": 0.0}  0.03
1         train   93900/102467 590:49/53:54       7.719         {"mlm": 7.720300584042807, "mse": 0.0}  0.0311
1         train   94000/102467 591:25/53:16       7.719         {"mlm": 7.720166061090314, "mse": 0.0}  0.0302
1         train   94100/102467 592:01/52:38       7.719         {"mlm": 7.722120484527276, "mse": 0.0}  0.042
1         train   94200/102467 592:37/52:00       7.719         {"mlm": 7.722663243611653, "mse": 0.0}  0.0279
1         train   94300/102467 593:14/51:22       7.719         {"mlm": 7.722082005251174, "mse": 0.0}  0.0295
1         train   94400/102467 593:50/50:44       7.719         {"mlm": 7.726064269866177, "mse": 0.0}  0.0291
1         train   94500/102467 594:26/50:06       7.719         {"mlm": 7.724751644823924, "mse": 0.0}  0.032
1         train   94600/102467 595:02/49:29       7.719         {"mlm": 7.724701645382272, "mse": 0.0}  0.0352
1         train   94700/102467 595:38/48:51       7.719         {"mlm": 7.724328286326717, "mse": 0.0}  0.0304
1         train   94800/102467 596:15/48:13       7.719         {"mlm": 7.722858387724798, "mse": 0.0}  0.0314
1         train   94900/102467 596:51/47:35       7.719         {"mlm": 7.72108156590791, "mse": 0.0}  0.0254
1         train   95000/102467 597:27/46:57       7.719         {"mlm": 7.7184186320983335, "mse": 0.0}  0.0379
1         train   95100/102467 598:03/46:19       7.719         {"mlm": 7.718316390431861, "mse": 0.0}  0.0444
1         train   95200/102467 598:39/45:41       7.719         {"mlm": 7.718600574041249, "mse": 0.0}  0.0312
1         train   95300/102467 599:15/45:04       7.719         {"mlm": 7.717680225019646, "mse": 0.0}  0.0262
1         train   95400/102467 599:52/44:26       7.719         {"mlm": 7.7161361659545245, "mse": 0.0}  0.0331
1         train   95500/102467 600:28/43:48       7.719         {"mlm": 7.717519257829409, "mse": 0.0}  0.0283
1         train   95600/102467 601:04/43:10       7.719         {"mlm": 7.717285861360266, "mse": 0.0}  0.0321
1         train   95700/102467 601:40/42:32       7.719         {"mlm": 7.7179545696267535, "mse": 0.0}  0.031
1         train   95800/102467 602:16/41:54       7.719         {"mlm": 7.717856668922076, "mse": 0.0}  0.0314
1         train   95900/102467 602:52/41:17       7.719         {"mlm": 7.716664594644239, "mse": 0.0}  0.0302
1         train   96000/102467 603:29/40:39       7.719         {"mlm": 7.716929937148834, "mse": 0.0}  0.0283
1         train   96100/102467 604:05/40:01       7.719         {"mlm": 7.723723534456234, "mse": 0.0}  0.032
1         train   96200/102467 604:41/39:23       7.719         {"mlm": 7.722689035580243, "mse": 0.0}  0.0304
1         train   96300/102467 605:17/38:45       7.719         {"mlm": 7.722529424160016, "mse": 0.0}  0.0364
1         train   96400/102467 605:53/38:07       7.719         {"mlm": 7.719547550383983, "mse": 0.0}  0.0318
1         train   96500/102467 606:29/37:30       7.719         {"mlm": 7.713887344183816, "mse": 0.0}  0.0331
1         train   96600/102467 607:06/36:52       7.719         {"mlm": 7.711521514496412, "mse": 0.0}  0.0279
1         train   96700/102467 607:42/36:14       7.719         {"mlm": 7.7107690396575705, "mse": 0.0}  0.0309
1         train   96800/102467 608:18/35:36       7.719         {"mlm": 7.713910102245948, "mse": 0.0}  0.0348
1         train   96900/102467 608:54/34:58       7.719         {"mlm": 7.714827013919452, "mse": 0.0}  0.0341
1         train   97000/102467 609:30/34:21       7.719         {"mlm": 7.718287856791658, "mse": 0.0}  0.0322
1         train   97100/102467 610:07/33:43       7.719         {"mlm": 7.718056804392701, "mse": 0.0}  0.0353
1         train   97200/102467 610:43/33:05       7.719         {"mlm": 7.7168112832900375, "mse": 0.0}  0.0305
1         train   97300/102467 611:19/32:27       7.719         {"mlm": 7.717327604315882, "mse": 0.0}  0.0345
1         train   97400/102467 611:55/31:50       7.719         {"mlm": 7.716239099086142, "mse": 0.0}  0.0335
1         train   97500/102467 612:31/31:12       7.719         {"mlm": 7.715265464209364, "mse": 0.0}  0.0287
1         train   97600/102467 613:07/30:34       7.719         {"mlm": 7.715791966815106, "mse": 0.0}  0.0315
1         train   97700/102467 613:44/29:56       7.719         {"mlm": 7.7146346621044115, "mse": 0.0}  0.0358
1         train   97800/102467 614:20/29:18       7.719         {"mlm": 7.714095716683415, "mse": 0.0}  0.0289
1         train   97900/102467 614:56/28:41       7.719         {"mlm": 7.713748939928408, "mse": 0.0}  0.0291
1         train   98000/102467 615:32/28:03       7.719         {"mlm": 7.713093602663288, "mse": 0.0}  0.0297
1         train   98100/102467 616:08/27:25       7.719         {"mlm": 7.748218109210332, "mse": 0.0}  0.0308
1         train   98200/102467 616:44/26:47       7.719         {"mlm": 7.739609584516408, "mse": 0.0}  0.0348
1         train   98300/102467 617:21/26:10       7.719         {"mlm": 7.727737384873468, "mse": 0.0}  0.029
1         train   98400/102467 617:57/25:32       7.719         {"mlm": 7.729123627296602, "mse": 0.0}  0.028
1         train   98500/102467 618:33/24:54       7.719         {"mlm": 7.727225214242935, "mse": 0.0}  0.032
1         train   98600/102467 619:09/24:16       7.719         {"mlm": 7.726569829371152, "mse": 0.0}  0.0276
1         train   98700/102467 619:46/23:39       7.719         {"mlm": 7.721970496506526, "mse": 0.0}  0.0298
1         train   98800/102467 620:22/23:01       7.719         {"mlm": 7.7218403235152735, "mse": 0.0}  0.0289
1         train   98900/102467 620:58/22:23       7.719         {"mlm": 7.720646374991962, "mse": 0.0}  0.0297
1         train   99000/102467 621:35/21:46       7.719         {"mlm": 7.719674785453153, "mse": 0.0}  0.0317
1         train   99100/102467 622:11/21:08       7.719         {"mlm": 7.719018140848536, "mse": 0.0}  0.0281
1         train   99200/102467 622:47/20:30       7.719         {"mlm": 7.719645004607363, "mse": 0.0}  0.028
1         train   99300/102467 623:24/19:52       7.719         {"mlm": 7.719396520176051, "mse": 0.0}  0.0275
1         train   99400/102467 624:00/19:15       7.719         {"mlm": 7.7181003845182055, "mse": 0.0}  0.0315
1         train   99500/102467 624:36/18:37       7.719         {"mlm": 7.718891479752281, "mse": 0.0}  0.0335
1         train   99600/102467 625:12/17:59       7.719         {"mlm": 7.718103813049488, "mse": 0.0}  0.0266
1         train   99700/102467 625:49/17:22       7.719         {"mlm": 7.717804957673235, "mse": 0.0}  0.0332
1         train   99800/102467 626:25/16:44       7.719         {"mlm": 7.717099842621648, "mse": 0.0}  0.0269
1         train   99900/102467 627:01/16:06       7.719         {"mlm": 7.716797568124055, "mse": 0.0}  0.0301
1         train   100000/102467 627:37/15:29      7.719         {"mlm": 7.717506285181982, "mse": 0.0}  0.0288

09/24/2022 05:21:10 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1.pkl
1         valid   1/781        0:37/485:33        7.601           None
1         valid   101/781      0:52/ 5:54         7.700           None
1         valid   201/781      1:07/ 3:16         7.724           None
1         valid   301/781      1:23/ 2:12         7.722           None
1         valid   401/781      1:38/ 1:33         7.722           None
1         valid   501/781      1:53/ 1:03         7.722           None
1         valid   601/781      2:08/ 0:38         7.722           None
1         valid   701/781      2:24/ 0:16         7.722           None
1         valid   781/781      2:38/ 0:00         7.719         {"mlm": 7.719270166453265, "mse": 0.0, "train": 0.0}  None
1         train   100100/102467 630:52/14:55      7.719         {"mlm": 7.7279606485366825, "mse": 0.0}  0.0366
1         train   100200/102467 631:27/14:17      7.719         {"mlm": 7.723381922245026, "mse": 0.0}  0.0268
1         train   100300/102467 632:03/13:39      7.719         {"mlm": 7.718484050432841, "mse": 0.0}  0.0351
1         train   100400/102467 632:39/13:01      7.719         {"mlm": 7.717517515420914, "mse": 0.0}  0.0274
1         train   100500/102467 633:15/12:23      7.719         {"mlm": 7.7200225009918215, "mse": 0.0}  0.0272
1         train   100600/102467 633:51/11:45      7.719         {"mlm": 7.723073510328929, "mse": 0.0}  0.0294
1         train   100700/102467 634:27/11:07      7.719         {"mlm": 7.722713971819196, "mse": 0.0}  0.0373
1         train   100800/102467 635:03/10:30      7.719         {"mlm": 7.7215527880191805, "mse": 0.0}  0.0269
1         train   100900/102467 635:39/ 9:52      7.719         {"mlm": 7.721164949735006, "mse": 0.0}  0.0284
1         train   101000/102467 636:15/ 9:14      7.719         {"mlm": 7.720584349632263, "mse": 0.0}  0.0346
1         train   101100/102467 636:51/ 8:36      7.719         {"mlm": 7.719063723737543, "mse": 0.0}  0.0282
1         train   101200/102467 637:27/ 7:58      7.719         {"mlm": 7.719048095146815, "mse": 0.0}  0.0305
1         train   101300/102467 638:03/ 7:21      7.719         {"mlm": 7.718835959067712, "mse": 0.0}  0.0273
1         train   101400/102467 638:40/ 6:43      7.719         {"mlm": 7.719705883775439, "mse": 0.0}  0.0296
1         train   101500/102467 639:16/ 6:05      7.719         {"mlm": 7.71994896697998, "mse": 0.0}  0.0334
1         train   101600/102467 639:52/ 5:27      7.719         {"mlm": 7.718888880610466, "mse": 0.0}  0.0309
1         train   101700/102467 640:28/ 4:49      7.719         {"mlm": 7.718648233133204, "mse": 0.0}  0.0304
1         train   101800/102467 641:04/ 4:12      7.719         {"mlm": 7.719174263212416, "mse": 0.0}  0.0344
1         train   101900/102467 641:40/ 3:34      7.719         {"mlm": 7.718751758525246, "mse": 0.0}  0.0329
1         train   102000/102467 642:17/ 2:56      7.719         {"mlm": 7.718085660934448, "mse": 0.0}  0.03
1         train   102100/102467 642:53/ 2:18      7.719         {"mlm": 7.733618191998414, "mse": 0.0}  0.0312
1         train   102200/102467 643:29/ 1:40      7.719         {"mlm": 7.737545284194563, "mse": 0.0}  0.0365
1         train   102300/102467 644:05/ 1:03      7.719         {"mlm": 7.7364692719883745, "mse": 0.0}  0.0295
1         train   102400/102467 644:41/ 0:25      7.719         {"mlm": 7.7275203821951886, "mse": 0.0}  0.0311
True False
skip validation
2         train   100/102467   0:49/847:28        7.739         {"mlm": 7.739409613609314, "mse": 0.0}  0.0288
2         train   200/102467   1:25/731:44        7.736         {"mlm": 7.73580156326294, "mse": 0.0}  0.0312
2         train   300/102467   2:01/692:17        7.740         {"mlm": 7.740434222221374, "mse": 0.0}  0.0431
2         train   400/102467   2:38/672:14        7.731         {"mlm": 7.7312683629989625, "mse": 0.0}  0.0283
2         train   500/102467   3:14/660:22        7.731         {"mlm": 7.730583873748779, "mse": 0.0}  0.0387
2         train   600/102467   3:50/652:02        7.730         {"mlm": 7.730271567503611, "mse": 0.0}  0.0316
2         train   700/102467   4:26/646:12        7.727         {"mlm": 7.726567442757743, "mse": 0.0}  0.0318
2         train   800/102467   5:02/641:22        7.728         {"mlm": 7.728240078091622, "mse": 0.0}  0.0286
2         train   900/102467   5:39/637:42        7.727         {"mlm": 7.726839590072632, "mse": 0.0}  0.0287
2         train   1000/102467  6:15/634:36        7.723         {"mlm": 7.722966039657593, "mse": 0.0}  0.0335
2         train   1100/102467  6:51/631:55        7.722         {"mlm": 7.722382000576366, "mse": 0.0}  0.0342
2         train   1200/102467  7:27/629:33        7.721         {"mlm": 7.7212443880240125, "mse": 0.0}  0.0283
2         train   1300/102467  8:03/627:33        7.722         {"mlm": 7.721973537298349, "mse": 0.0}  0.0253
2         train   1400/102467  8:40/625:45        7.723         {"mlm": 7.722775045803615, "mse": 0.0}  0.0323
2         train   1500/102467  9:16/624:04        7.723         {"mlm": 7.722836555480957, "mse": 0.0}  0.0303
2         train   1600/102467  9:52/622:32        7.722         {"mlm": 7.721979775428772, "mse": 0.0}  0.0292
2         train   1700/102467 10:28/621:09        7.721         {"mlm": 7.7214615493662215, "mse": 0.0}  0.029
2         train   1800/102467 11:04/619:50        7.721         {"mlm": 7.721369426250458, "mse": 0.0}  0.0294
2         train   1900/102467 11:41/618:31        7.721         {"mlm": 7.720986230498866, "mse": 0.0}  0.0289
2         train   2000/102467 12:17/617:17        7.721         {"mlm": 7.720930884122849, "mse": 0.0}  0.0336
2         train   2100/102467 12:53/616:06        7.721         {"mlm": 7.717217902944546, "mse": 0.0}  0.0311
2         train   2200/102467 13:29/615:07        7.720         {"mlm": 7.709749715412082, "mse": 0.0}  0.0343
2         train   2300/102467 14:05/614:03        7.719         {"mlm": 7.703671892351132, "mse": 0.0}  0.0275
2         train   2400/102467 14:42/613:01        7.719         {"mlm": 7.710224759907352, "mse": 0.0}  0.0285
2         train   2500/102467 15:18/612:02        7.718         {"mlm": 7.705753369417363, "mse": 0.0}  0.0365
2         train   2600/102467 15:54/611:08        7.718         {"mlm": 7.706765612696168, "mse": 0.0}  0.0312
2         train   2700/102467 16:30/610:14        7.718         {"mlm": 7.710177547771361, "mse": 0.0}  0.0277
2         train   2800/102467 17:07/609:19        7.719         {"mlm": 7.712547039060628, "mse": 0.0}  0.0302
2         train   2900/102467 17:43/608:28        7.719         {"mlm": 7.713252660032109, "mse": 0.0}  0.0277
2         train   3000/102467 18:19/607:38        7.718         {"mlm": 7.711627627039576, "mse": 0.0}  0.0293
2         train   3100/102467 18:55/606:48        7.717         {"mlm": 7.710931923304828, "mse": 0.0}  0.0356
2         train   3200/102467 19:32/605:57        7.717         {"mlm": 7.709432105206767, "mse": 0.0}  0.03
2         train   3300/102467 20:08/605:05        7.717         {"mlm": 7.710778140214152, "mse": 0.0}  0.029
2         train   3400/102467 20:44/604:16        7.717         {"mlm": 7.7107971046889485, "mse": 0.0}  0.0316
2         train   3500/102467 21:20/603:31        7.716         {"mlm": 7.710455118297656, "mse": 0.0}  0.0317
2         train   3600/102467 21:56/602:46        7.717         {"mlm": 7.7117946575849485, "mse": 0.0}  0.0344
2         train   3700/102467 22:33/602:01        7.718         {"mlm": 7.713863005141358, "mse": 0.0}  0.0409
2         train   3800/102467 23:09/601:20        7.717         {"mlm": 7.712730915828702, "mse": 0.0}  0.03
2         train   3900/102467 23:45/600:36        7.716         {"mlm": 7.7115593770104, "mse": 0.0}  0.029
2         train   4000/102467 24:22/599:55        7.717         {"mlm": 7.712386497680756, "mse": 0.0}  0.0326
2         train   4100/102467 24:58/599:10        7.717         {"mlm": 7.726915831468543, "mse": 0.0}  0.033
2         train   4200/102467 25:34/598:26        7.717         {"mlm": 7.724262842024215, "mse": 0.0}  0.0342
2         train   4300/102467 26:11/597:45        7.717         {"mlm": 7.718308773616816, "mse": 0.0}  0.0293
2         train   4400/102467 26:47/597:03        7.717         {"mlm": 7.724330104176124, "mse": 0.0}  0.0335
2         train   4500/102467 27:23/596:20        7.717         {"mlm": 7.719814806099397, "mse": 0.0}  0.0327
2         train   4600/102467 27:59/595:39        7.717         {"mlm": 7.719752054948073, "mse": 0.0}  0.03
2         train   4700/102467 28:36/594:56        7.717         {"mlm": 7.717766453680131, "mse": 0.0}  0.0315
2         train   4800/102467 29:12/594:13        7.717         {"mlm": 7.71912704075787, "mse": 0.0}  0.0299
2         train   4900/102467 29:48/593:32        7.717         {"mlm": 7.719610584870744, "mse": 0.0}  0.0324
2         train   5000/102467 30:24/592:49        7.717         {"mlm": 7.719912575337595, "mse": 0.0}  0.0294
2         train   5100/102467 31:00/592:07        7.718         {"mlm": 7.720767995698856, "mse": 0.0}  0.0351
2         train   5200/102467 31:37/591:26        7.717         {"mlm": 7.719514513652592, "mse": 0.0}  0.0288
2         train   5300/102467 32:13/590:44        7.717         {"mlm": 7.719089953300582, "mse": 0.0}  0.0298
2         train   5400/102467 32:49/590:01        7.717         {"mlm": 7.719026912435441, "mse": 0.0}  0.0305
2         train   5500/102467 33:25/589:21        7.717         {"mlm": 7.7180670291304745, "mse": 0.0}  0.0305
2         train   5600/102467 34:01/588:41        7.717         {"mlm": 7.7176595369775844, "mse": 0.0}  0.0337
2         train   5700/102467 34:38/587:59        7.717         {"mlm": 7.717609414503909, "mse": 0.0}  0.0347
2         train   5800/102467 35:14/587:17        7.717         {"mlm": 7.718034979498824, "mse": 0.0}  0.029
2         train   5900/102467 35:50/586:37        7.717         {"mlm": 7.717673268283004, "mse": 0.0}  0.0304
2         train   6000/102467 36:26/585:55        7.716         {"mlm": 7.7159897130769535, "mse": 0.0}  0.0337
2         train   6100/102467 37:02/585:15        7.717         {"mlm": 7.731585674679156, "mse": 0.0}  0.0292
2         train   6200/102467 37:38/584:33        7.716         {"mlm": 7.713061613479846, "mse": 0.0}  0.0273
2         train   6300/102467 38:15/583:52        7.716         {"mlm": 7.7169492092196785, "mse": 0.0}  0.0276
2         train   6400/102467 38:51/583:10        7.717         {"mlm": 7.718252965124789, "mse": 0.0}  0.0279
2         train   6500/102467 39:27/582:29        7.716         {"mlm": 7.715412577393069, "mse": 0.0}  0.0292
2         train   6600/102467 40:03/581:48        7.716         {"mlm": 7.714823425714694, "mse": 0.0}  0.0359
2         train   6700/102467 40:39/581:07        7.716         {"mlm": 7.716306105576765, "mse": 0.0}  0.0285
2         train   6800/102467 41:15/580:27        7.716         {"mlm": 7.716709074142437, "mse": 0.0}  0.0277
2         train   6900/102467 41:51/579:48        7.717         {"mlm": 7.71678948801098, "mse": 0.0}  0.0311
2         train   7000/102467 42:27/579:08        7.716         {"mlm": 7.716044318353161, "mse": 0.0}  0.0296
2         train   7100/102467 43:04/578:28        7.716         {"mlm": 7.716468595871626, "mse": 0.0}  0.0293
2         train   7200/102467 43:40/577:50        7.717         {"mlm": 7.717143201389807, "mse": 0.0}  0.0269
2         train   7300/102467 44:16/577:11        7.716         {"mlm": 7.716269396044423, "mse": 0.0}  0.0257
2         train   7400/102467 44:52/576:32        7.716         {"mlm": 7.716273319746822, "mse": 0.0}  0.0277
2         train   7500/102467 45:28/575:54        7.716         {"mlm": 7.715554188312335, "mse": 0.0}  0.0376
2         train   7600/102467 46:05/575:16        7.716         {"mlm": 7.715122103765747, "mse": 0.0}  0.0334
2         train   7700/102467 46:41/574:37        7.716         {"mlm": 7.7165084511796795, "mse": 0.0}  0.0277
2         train   7800/102467 47:17/573:59        7.716         {"mlm": 7.7146609454401744, "mse": 0.0}  0.0362
2         train   7900/102467 47:53/573:22        7.716         {"mlm": 7.714867815571706, "mse": 0.0}  0.0294
2         train   8000/102467 48:30/572:45        7.716         {"mlm": 7.714653102290231, "mse": 0.0}  0.0272
2         train   8100/102467 49:06/572:06        7.716         {"mlm": 7.71752808491389, "mse": 0.0}  0.0278
2         train   8200/102467 49:42/571:28        7.716         {"mlm": 7.712814737339409, "mse": 0.0}  0.0289
2         train   8300/102467 50:18/570:50        7.716         {"mlm": 7.713466164228079, "mse": 0.0}  0.0254
2         train   8400/102467 50:55/570:12        7.716         {"mlm": 7.718107611241967, "mse": 0.0}  0.033
2         train   8500/102467 51:31/569:35        7.716         {"mlm": 7.719785372095723, "mse": 0.0}  0.0361
2         train   8600/102467 52:07/568:56        7.716         {"mlm": 7.723093628883362, "mse": 0.0}  0.0297
2         train   8700/102467 52:43/568:17        7.716         {"mlm": 7.722549076052918, "mse": 0.0}  0.0372
2         train   8800/102467 53:19/567:37        7.716         {"mlm": 7.720462267722317, "mse": 0.0}  0.0333
2         train   8900/102467 53:55/566:59        7.716         {"mlm": 7.719546322311674, "mse": 0.0}  0.0318
2         train   9000/102467 54:32/566:21        7.717         {"mlm": 7.7212526941873945, "mse": 0.0}  0.0319
2         train   9100/102467 55:08/565:43        7.717         {"mlm": 7.721688432850107, "mse": 0.0}  0.0285
2         train   9200/102467 55:44/565:04        7.717         {"mlm": 7.720875820587311, "mse": 0.0}  0.0412
2         train   9300/102467 56:20/564:26        7.717         {"mlm": 7.720981300245096, "mse": 0.0}  0.0279
2         train   9400/102467 56:56/563:48        7.717         {"mlm": 7.721850516802943, "mse": 0.0}  0.0294
2         train   9500/102467 57:32/563:10        7.717         {"mlm": 7.721736364186129, "mse": 0.0}  0.0267
2         train   9600/102467 58:09/562:32        7.717         {"mlm": 7.720846101753693, "mse": 0.0}  0.0303
2         train   9700/102467 58:45/561:54        7.717         {"mlm": 7.720877795849207, "mse": 0.0}  0.0284
2         train   9800/102467 59:21/561:15        7.717         {"mlm": 7.721565422342721, "mse": 0.0}  0.0294
2         train   9900/102467 59:57/560:39        7.717         {"mlm": 7.720659196879794, "mse": 0.0}  0.0295
2         train   10000/102467 60:33/560:01       7.717         {"mlm": 7.7197730875684165, "mse": 0.0}  0.0329

09/24/2022 06:39:14 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/2.pkl
2         valid   1/781        0:37/488:41        7.938           None
2         valid   101/781      0:52/ 5:55         7.712           None
2         valid   201/781      1:08/ 3:16         7.718           None
2         valid   301/781      1:23/ 2:12         7.715           None
2         valid   401/781      1:38/ 1:33         7.726           None
2         valid   501/781      1:53/ 1:03         7.723           None
2         valid   601/781      2:09/ 0:38         7.721           None
2         valid   701/781      2:24/ 0:16         7.722           None
2         valid   781/781      2:38/ 0:00         7.718         {"mlm": 7.71814980793854, "mse": 0.0, "train": 0.0}  None
2         train   10100/102467 63:48/583:36       7.717         {"mlm": 7.717068643569946, "mse": 0.0}  0.0307
2         train   10200/102467 64:24/582:38       7.717         {"mlm": 7.710810205936432, "mse": 0.0}  0.0325
2         train   10300/102467 65:00/581:42       7.716         {"mlm": 7.705907843907674, "mse": 0.0}  0.03
2         train   10400/102467 65:36/580:45       7.716         {"mlm": 7.711400679349899, "mse": 0.0}  0.0364
2         train   10500/102467 66:12/579:50       7.716         {"mlm": 7.71253231716156, "mse": 0.0}  0.038
2         train   10600/102467 66:47/578:55       7.717         {"mlm": 7.713885360558828, "mse": 0.0}  0.0298
2         train   10700/102467 67:23/578:01       7.717         {"mlm": 7.713966251100812, "mse": 0.0}  0.0289
2         train   10800/102467 67:59/577:07       7.716         {"mlm": 7.713420157432556, "mse": 0.0}  0.0346
2         train   10900/102467 68:35/576:14       7.716         {"mlm": 7.712398117383321, "mse": 0.0}  0.0266
2         train   11000/102467 69:11/575:22       7.716         {"mlm": 7.713161140441895, "mse": 0.0}  0.0334
2         train   11100/102467 69:47/574:30       7.716         {"mlm": 7.714378418922425, "mse": 0.0}  0.0292
2         train   11200/102467 70:23/573:39       7.717         {"mlm": 7.714971757729848, "mse": 0.0}  0.0318
2         train   11300/102467 70:59/572:48       7.717         {"mlm": 7.71623195208036, "mse": 0.0}  0.0306
2         train   11400/102467 71:35/571:57       7.717         {"mlm": 7.715404025145939, "mse": 0.0}  0.0288
2         train   11500/102467 72:12/571:08       7.717         {"mlm": 7.716564461708069, "mse": 0.0}  0.0299
2         train   11600/102467 72:48/570:18       7.716         {"mlm": 7.715195899903774, "mse": 0.0}  0.0465
2         train   11700/102467 73:24/569:29       7.717         {"mlm": 7.716105869517607, "mse": 0.0}  0.0273
2         train   11800/102467 74:00/568:41       7.717         {"mlm": 7.71675156434377, "mse": 0.0}  0.0333
2         train   11900/102467 74:36/567:52       7.717         {"mlm": 7.715624775635569, "mse": 0.0}  0.0307
2         train   12000/102467 75:13/567:04       7.717         {"mlm": 7.716098693132401, "mse": 0.0}  0.0279
2         train   12100/102467 75:49/566:17       7.716         {"mlm": 7.699722636829723, "mse": 0.0}  0.0308
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin


/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin



/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config fromread config fromread config from config/pre-train/adapter.config 
read config fromconfig/pre-train/adapter.config 
config/pre-train/adapter.config
 read config fromconfig/pre-train/adapter.config
 config/pre-train/adapter.config
read config from config/pre-train/adapter.config
NoneNoneNone


None
None
None
None
None
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 06:58:13 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 06:58:13 - INFO - __main__ -   CUDA available: True
09/24/2022 06:58:13 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
formatter roberta-base
09/24/2022 06:58:13 - INFO - __main__ -   CUDA available: True
09/24/2022 06:58:13 - INFO - __main__ -   CUDA available: True
09/24/2022 06:58:13 - INFO - __main__ -   CUDA available: True
09/24/2022 06:58:13 - INFO - __main__ -   CUDA available: True
formatterformatter 09/24/2022 06:58:13 - INFO - __main__ -   CUDA available: True
 roberta-baseroberta-base

09/24/2022 06:58:13 - INFO - __main__ -   CUDA available: True
09/24/2022 06:58:13 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatterformatter  roberta-baseroberta-base

formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/24/2022 06:58:40 - INFO - tools.init_tool -   Begin to initialize models...
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-24 06:58:52,445 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:675]2022-09-24 06:58:52,445 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 06:58:52,445 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 06:58:52,445 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:677]2022-09-24 06:58:52,445 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 06:58:52,445 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-24 06:58:52,445 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 06:58:52,445 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 06:58:52,445 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-24 06:58:52,445 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 06:58:52,445 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 06:58:52,446 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-24 06:58:52,446 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 06:58:52,446 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 06:58:52,446 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
[INFO|(OpenDelta)basemodel:675]2022-09-24 06:58:52,447 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 06:58:52,447 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 06:58:52,447 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-24 06:58:52,447 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 06:58:52,447 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 06:58:52,447 >> Static Memory 0.00 GB, Max Memory 0.00 GB
[INFO|(OpenDelta)basemodel:675]2022-09-24 06:58:52,449 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 06:58:52,450 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 06:58:52,450 >> Static Memory 0.00 GB, Max Memory 0.00 GB
may load from checkpoints/BERT-Adapter ['2.pkl', '1.pkl', '0.pkl']
09/24/2022 06:59:02 - INFO - tools.init_tool -   Begin to load checkpoint... from checkpoints/BERT-Adapter/2.pkl
09/24/2022 06:59:04 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
grad_accumulate: 2
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
load_from_path: True
========
09/24/2022 06:59:04 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/24/2022 06:59:04 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 2 step: 10000
skip step 0
skip step 1000
skip step 2000
skip step 3000
skip step 4000
/home/xiaochaojun/env/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin
/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin


/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

/data_new/private/xiaochaojun/DomainPlugin/DomainPlugin

read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config from config/pre-train/adapter.config
read config fromread config from  config/pre-train/adapter.configconfig/pre-train/adapter.config
read config from
read config from config/pre-train/adapter.config
 config/pre-train/adapter.configread config from
 config/pre-train/adapter.config
NoneNoneNoneNone

NoneNone
None



None
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 07:06:25 - INFO - __main__ -   CUDA available: True
09/24/2022 07:06:25 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
09/24/2022 07:06:25 - INFO - __main__ -   CUDA available: True
09/24/2022 07:06:25 - INFO - __main__ -   CUDA available: True
09/24/2022 07:06:25 - INFO - __main__ -   CUDA available: True
09/24/2022 07:06:25 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatterformatter roberta-base roberta-base

formatter roberta-base
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 07:06:25 - INFO - __main__ -   CUDA available: True
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
09/24/2022 07:06:25 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
formatter roberta-base
09/24/2022 07:06:25 - INFO - __main__ -   CUDA available: True
09/24/2022 07:06:25 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
09/24/2022 07:07:07 - INFO - tools.init_tool -   Begin to initialize models...
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-24 07:07:10,569 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 07:07:10,569 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 07:07:10,569 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-24 07:07:11,090 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 07:07:11,090 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 07:07:11,090 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-24 07:07:11,533 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 07:07:11,533 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 07:07:11,533 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-24 07:07:15,797 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 07:07:15,797 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 07:07:15,797 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-24 07:07:16,255 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 07:07:16,255 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 07:07:16,255 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-24 07:07:16,995 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 07:07:16,995 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 07:07:16,995 >> Static Memory 0.00 GB, Max Memory 0.00 GB
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-24 07:07:17,801 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 07:07:17,801 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 07:07:17,801 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
root
 roberta (RobertaModel)
    embeddings (RobertaEmbeddings)
       word_embeddings (Embedding) weight:[50265, 768]
       position_embeddings (Embedding) weight:[514, 768]
       token_type_embeddings (Embedding) weight:[1, 768]
       LayerNorm (LayerNorm) weight:[768] bias:[768]
    encoder (RobertaEncoder)
        layer (ModuleList)
            0-11(RobertaLayer)
                attention (RobertaAttention)
                   self (RobertaSelfAttention)
                      query,key,value(Linear) weight:[768, 768] bias:[768]
                   output (RobertaSelfOutput)
                       dense (Linear) weight:[768, 768] bias:[768]
                       LayerNorm (LayerNorm) weight:[768] bias:[768]
                           adapter (AdapterLayer)
                               modulelist (Sequential)
                                   down_proj (Linear) weight:[256, 768] bias:[256]
                                   up_proj (Linear) weight:[768, 256] bias:[768]
                intermediate (RobertaIntermediate)
                   dense (Linear) weight:[3072, 768] bias:[3072]
                output (RobertaOutput)
                    dense (Linear) weight:[768, 3072] bias:[768]
                    LayerNorm (LayerNorm) weight:[768] bias:[768]
                        adapter (AdapterLayer)
                            modulelist (Sequential)
                                down_proj (Linear) weight:[256, 768] bias:[256]
                                up_proj (Linear) weight:[768, 256] bias:[768]
 lm_head (RobertaLMHead) bias:[50265]
     dense (Linear) weight:[768, 768] bias:[768]
     layer_norm (LayerNorm) weight:[768] bias:[768]
     decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-09-24 07:07:22,084 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-09-24 07:07:22,084 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-09-24 07:07:22,084 >> Static Memory 0.00 GB, Max Memory 0.00 GB
09/24/2022 07:08:00 - INFO - tools.init_tool -   Begin to load checkpoint... from None
09/24/2022 07:08:00 - WARNING - tools.init_tool -   Cannot load checkpoint file with error 'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.
09/24/2022 07:08:00 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
grad_accumulate: 2
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data_new/private/xiaochaojun/DomainPlugin/data/pretrain/bio-kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
load_from_path: False
========
09/24/2022 07:08:00 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
09/24/2022 07:08:00 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
09/24/2022 07:08:44 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/24/2022 07:08:44 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/24/2022 07:08:44 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/24/2022 07:08:44 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/24/2022 07:08:44 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/24/2022 07:08:44 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/24/2022 07:08:44 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
09/24/2022 07:08:44 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   100/102467   1:19/1358:04       1.501         {"mlm": 1.5013752830028535, "mse": 0.0}  0.5637
0         train   200/102467   1:55/981:29        1.508         {"mlm": 1.5082148540019988, "mse": 0.0}  0.5294
0         train   300/102467   2:30/856:31        1.502         {"mlm": 1.5022717078526815, "mse": 0.0}  0.5277
0         train   400/102467   3:06/794:01        1.503         {"mlm": 1.5032505324482919, "mse": 0.0}  0.5269
0         train   500/102467   3:42/756:26        1.500         {"mlm": 1.4995790495872499, "mse": 0.0}  0.5614
0         train   600/102467   4:18/731:10        1.493         {"mlm": 1.493331237634023, "mse": 0.0}  0.5234
0         train   700/102467   4:54/713:04        1.492         {"mlm": 1.491918499980654, "mse": 0.0}  0.4905
0         train   800/102467   5:30/699:24        1.493         {"mlm": 1.4930635225772857, "mse": 0.0}  0.5001
0         train   900/102467   6:06/688:40        1.492         {"mlm": 1.4918000467618306, "mse": 0.0}  0.5006
0         train   1000/102467  6:42/680:01        1.491         {"mlm": 1.49106179189682, "mse": 0.0}  0.5026
0         train   1100/102467  7:18/673:00        1.487         {"mlm": 1.4868381945653395, "mse": 0.0}  0.5015
0         train   1200/102467  7:54/667:00        1.484         {"mlm": 1.4840833258628845, "mse": 0.0}  0.5917
0         train   1300/102467  8:30/662:00        1.484         {"mlm": 1.4842108593537258, "mse": 0.0}  0.4948
0         train   1400/102467  9:06/657:42        1.481         {"mlm": 1.4808565202355384, "mse": 0.0}  0.4973
0         train   1500/102467  9:42/653:50        1.479         {"mlm": 1.4792195647160211, "mse": 0.0}  0.6052
0         train   1600/102467 10:19/650:25        1.478         {"mlm": 1.4780107760056853, "mse": 0.0}  0.5072
0         train   1700/102467 10:55/647:19        1.478         {"mlm": 1.4779898754288168, "mse": 0.0}  0.4962
0         train   1800/102467 11:31/644:36        1.476         {"mlm": 1.476107882724868, "mse": 0.0}  0.7367
0         train   1900/102467 12:08/642:15        1.476         {"mlm": 1.4756164855078646, "mse": 0.0}  0.5021
0         train   2000/102467 12:44/640:01        1.475         {"mlm": 1.4748843536973, "mse": 0.0}  0.4799
0         train   2100/102467 13:20/637:54        1.474         {"mlm": 1.4613412931711987, "mse": 0.0}  0.4808
0         train   2200/102467 13:57/635:59        1.473         {"mlm": 1.4493405884833792, "mse": 0.0}  0.4965
0         train   2300/102467 14:33/634:09        1.473         {"mlm": 1.458766717177171, "mse": 0.0}  0.4776
0         train   2400/102467 15:10/632:26        1.472         {"mlm": 1.4564517192673265, "mse": 0.0}  0.4847
0         train   2500/102467 15:46/630:52        1.471         {"mlm": 1.4547797729352672, "mse": 0.0}  0.4923
0         train   2600/102467 16:23/629:25        1.470         {"mlm": 1.4529040111524234, "mse": 0.0}  0.4907
0         train   2700/102467 16:59/628:01        1.468         {"mlm": 1.4486283919159777, "mse": 0.0}  0.5684
0         train   2800/102467 17:36/626:37        1.467         {"mlm": 1.4476365891654739, "mse": 0.0}  0.4543
0         train   2900/102467 18:12/625:18        1.465         {"mlm": 1.443672450684069, "mse": 0.0}  0.4635
0         train   3000/102467 18:49/624:02        1.465         {"mlm": 1.4454467720216937, "mse": 0.0}  0.4625
0         train   3100/102467 19:25/622:49        1.464         {"mlm": 1.444445769389832, "mse": 0.0}  0.4652
0         train   3200/102467 20:02/621:38        1.464         {"mlm": 1.4460187074141864, "mse": 0.0}  0.4645
0         train   3300/102467 20:38/620:28        1.463         {"mlm": 1.44482627068234, "mse": 0.0}  0.46
0         train   3400/102467 21:15/619:22        1.462         {"mlm": 1.442928884905351, "mse": 0.0}  0.4356
0         train   3500/102467 21:51/618:14        1.461         {"mlm": 1.4422308971199533, "mse": 0.0}  0.4729
0         train   3600/102467 22:28/617:06        1.460         {"mlm": 1.4416251192620726, "mse": 0.0}  0.472
0         train   3700/102467 23:04/616:02        1.460         {"mlm": 1.441808167838433, "mse": 0.0}  0.4485
0         train   3800/102467 23:41/614:59        1.459         {"mlm": 1.4404565799295936, "mse": 0.0}  0.4573
0         train   3900/102467 24:17/613:56        1.458         {"mlm": 1.4407329833338045, "mse": 0.0}  0.4543
0         train   4000/102467 24:53/612:55        1.458         {"mlm": 1.441285682267222, "mse": 0.0}  0.486
0         train   4100/102467 25:30/611:56        1.458         {"mlm": 1.4354543977854204, "mse": 0.0}  0.4678
0         train   4200/102467 26:06/610:56        1.457         {"mlm": 1.426973866091834, "mse": 0.0}  0.4576
0         train   4300/102467 26:43/609:56        1.456         {"mlm": 1.4314736369872254, "mse": 0.0}  0.5269
0         train   4400/102467 27:19/608:58        1.455         {"mlm": 1.425915830848205, "mse": 0.0}  0.4609
0         train   4500/102467 27:55/608:03        1.455         {"mlm": 1.4289057452276528, "mse": 0.0}  0.4446
0         train   4600/102467 28:32/607:08        1.455         {"mlm": 1.4329056276326195, "mse": 0.0}  0.4731
0         train   4700/102467 29:08/606:13        1.454         {"mlm": 1.432257473724961, "mse": 0.0}  0.4385
0         train   4800/102467 29:44/605:19        1.454         {"mlm": 1.4340492711778272, "mse": 0.0}  0.4724
0         train   4900/102467 30:21/604:25        1.453         {"mlm": 1.4329516535877915, "mse": 0.0}  0.4593
0         train   5000/102467 30:57/603:33        1.453         {"mlm": 1.4337904078807524, "mse": 0.0}  0.4793
0         train   5100/102467 31:34/602:39        1.453         {"mlm": 1.4356402083281394, "mse": 0.0}  0.4722
0         train   5200/102467 32:10/601:48        1.453         {"mlm": 1.4373998810632003, "mse": 0.0}  0.449
0         train   5300/102467 32:46/600:58        1.453         {"mlm": 1.4386361582529013, "mse": 0.0}  0.5279
0         train   5400/102467 33:23/600:05        1.453         {"mlm": 1.4391138292262824, "mse": 0.0}  0.4604
0         train   5500/102467 33:59/599:16        1.452         {"mlm": 1.4375754536312317, "mse": 0.0}  0.4915
0         train   5600/102467 34:35/598:28        1.453         {"mlm": 1.4390294922606668, "mse": 0.0}  0.4621
0         train   5700/102467 35:12/597:42        1.452         {"mlm": 1.43894044937318, "mse": 0.0}  0.4447
0         train   5800/102467 35:48/596:55        1.452         {"mlm": 1.4396363352244104, "mse": 0.0}  0.4563
0         train   5900/102467 36:25/596:09        1.452         {"mlm": 1.4398150032011299, "mse": 0.0}  0.4652
0         train   6000/102467 37:01/595:24        1.452         {"mlm": 1.439976634862306, "mse": 0.0}  0.443
0         train   6100/102467 37:38/594:39        1.452         {"mlm": 1.4284890668908345, "mse": 0.0}  1.1442
0         train   6200/102467 38:14/593:53        1.451         {"mlm": 1.430669596352553, "mse": 0.0}  0.459
0         train   6300/102467 38:51/593:08        1.451         {"mlm": 1.435774438308947, "mse": 0.0}  0.6346
0         train   6400/102467 39:27/592:22        1.451         {"mlm": 1.433815363372303, "mse": 0.0}  0.4607
0         train   6500/102467 40:04/591:37        1.451         {"mlm": 1.4382634045372547, "mse": 0.0}  0.453
0         train   6600/102467 40:40/590:54        1.451         {"mlm": 1.4398179912886628, "mse": 0.0}  0.4518
0         train   6700/102467 41:17/590:09        1.451         {"mlm": 1.4374575621770478, "mse": 0.0}  0.4773
0         train   6800/102467 41:53/589:24        1.450         {"mlm": 1.432709577989997, "mse": 0.0}  0.465
0         train   6900/102467 42:30/588:38        1.449         {"mlm": 1.4300491856359185, "mse": 0.0}  0.4548
0         train   7000/102467 43:06/587:54        1.449         {"mlm": 1.4299581597298534, "mse": 0.0}  0.4763
0         train   7100/102467 43:42/587:10        1.449         {"mlm": 1.4298741553953374, "mse": 0.0}  0.4643
0         train   7200/102467 44:19/586:26        1.449         {"mlm": 1.4315602854578915, "mse": 0.0}  0.4816
0         train   7300/102467 44:55/585:42        1.448         {"mlm": 1.430824003407104, "mse": 0.0}  0.483
0         train   7400/102467 45:31/584:57        1.448         {"mlm": 1.429932993142709, "mse": 0.0}  0.5043
0         train   7500/102467 46:08/584:14        1.447         {"mlm": 1.4288760792357011, "mse": 0.0}  0.4583
0         train   7600/102467 46:44/583:30        1.447         {"mlm": 1.4273121535367495, "mse": 0.0}  0.4796
0         train   7700/102467 47:21/582:47        1.446         {"mlm": 1.4262122064741063, "mse": 0.0}  0.4442
0         train   7800/102467 47:57/582:04        1.446         {"mlm": 1.4248827752233812, "mse": 0.0}  0.4861
0         train   7900/102467 48:34/581:23        1.446         {"mlm": 1.425008665968634, "mse": 0.0}  0.4652
0         train   8000/102467 49:10/580:40        1.445         {"mlm": 1.424013288007955, "mse": 0.0}  0.4542
0         train   8100/102467 49:46/579:57        1.445         {"mlm": 1.4238432285686333, "mse": 0.0}  0.4801
0         train   8200/102467 50:23/579:15        1.445         {"mlm": 1.431609376048555, "mse": 0.0}  0.4601
0         train   8300/102467 50:59/578:32        1.445         {"mlm": 1.4312908341353003, "mse": 0.0}  0.4787
0         train   8400/102467 51:36/577:50        1.444         {"mlm": 1.4319426855354598, "mse": 0.0}  0.4756
0         train   8500/102467 52:12/577:08        1.444         {"mlm": 1.4284089605414099, "mse": 0.0}  0.5082
0         train   8600/102467 52:48/576:26        1.444         {"mlm": 1.432204132812135, "mse": 0.0}  0.4804
0         train   8700/102467 53:25/575:44        1.444         {"mlm": 1.4302336006671532, "mse": 0.0}  0.4825
0         train   8800/102467 54:01/575:02        1.444         {"mlm": 1.431199408056748, "mse": 0.0}  0.4624
0         train   8900/102467 54:37/574:20        1.444         {"mlm": 1.4309362545609474, "mse": 0.0}  0.461
0         train   9000/102467 55:14/573:39        1.443         {"mlm": 1.4295951600773746, "mse": 0.0}  0.4828
0         train   9100/102467 55:50/572:57        1.443         {"mlm": 1.4288768660630622, "mse": 0.0}  0.4713
0         train   9200/102467 56:26/572:16        1.443         {"mlm": 1.428642043102545, "mse": 0.0}  0.5002
0         train   9300/102467 57:03/571:35        1.443         {"mlm": 1.428608242744281, "mse": 0.0}  0.5304
0         train   9400/102467 57:39/570:55        1.443         {"mlm": 1.4281172906088624, "mse": 0.0}  0.4976
0         train   9500/102467 58:16/570:14        1.442         {"mlm": 1.4273838152222456, "mse": 0.0}  0.478
0         train   9600/102467 58:52/569:34        1.442         {"mlm": 1.425793205250176, "mse": 0.0}  0.5439
0         train   9700/102467 59:29/568:54        1.442         {"mlm": 1.4250315905390483, "mse": 0.0}  0.4882
0         train   9800/102467 60:05/568:14        1.442         {"mlm": 1.4262441430232573, "mse": 0.0}  0.5332
0         train   9900/102467 60:42/567:34        1.442         {"mlm": 1.4278068897404752, "mse": 0.0}  0.5185
0         train   10000/102467 61:18/566:54       1.442         {"mlm": 1.427603913601749, "mse": 0.0}  0.4915

09/24/2022 08:09:19 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step10000.pkl
0         valid   1/781        0:23/308:48        1.249           None
0         valid   101/781      0:39/ 4:24         1.310           None
0         valid   201/781      0:54/ 2:38         1.318           None
0         valid   301/781      1:10/ 1:52         1.319           None
0         valid   401/781      1:26/ 1:21         1.321           None
0         valid   501/781      1:41/ 0:56         1.321           None
0         valid   601/781      1:57/ 0:35         1.321           None
0         valid   701/781      2:12/ 0:15         1.324           None
0         valid   781/781      2:28/ 0:00         1.314         {"mlm": 1.314180537772087, "mse": 0.0, "train": 0.0}  None
0         train   10100/102467 64:22/588:47       1.441         {"mlm": 1.3957957696914673, "mse": 0.0}  0.4917
0         train   10200/102467 64:58/587:49       1.441         {"mlm": 1.406560150384903, "mse": 0.0}  0.4868
0         train   10300/102467 65:35/586:51       1.441         {"mlm": 1.4180937655766805, "mse": 0.0}  0.4929
0         train   10400/102467 66:11/585:54       1.441         {"mlm": 1.4175089168548585, "mse": 0.0}  0.5204
0         train   10500/102467 66:47/584:58       1.440         {"mlm": 1.4152313389778137, "mse": 0.0}  0.4948
0         train   10600/102467 67:23/584:04       1.440         {"mlm": 1.4135145125786464, "mse": 0.0}  0.5019
0         train   10700/102467 67:59/583:09       1.440         {"mlm": 1.4122510375295367, "mse": 0.0}  0.4725
0         train   10800/102467 68:36/582:16       1.440         {"mlm": 1.4141173885762692, "mse": 0.0}  0.4881
0         train   10900/102467 69:12/581:23       1.439         {"mlm": 1.4140405074755351, "mse": 0.0}  0.4714
0         train   11000/102467 69:48/580:30       1.439         {"mlm": 1.4131592440605163, "mse": 0.0}  0.5887
0         train   11100/102467 70:25/579:39       1.439         {"mlm": 1.411076536395333, "mse": 0.0}  0.4991
0         train   11200/102467 71:01/578:47       1.439         {"mlm": 1.4131547506650288, "mse": 0.0}  0.4998
0         train   11300/102467 71:38/577:56       1.438         {"mlm": 1.4138860445756178, "mse": 0.0}  0.4844
0         train   11400/102467 72:14/577:06       1.438         {"mlm": 1.4125813041414534, "mse": 0.0}  0.4861
0         train   11500/102467 72:51/576:16       1.438         {"mlm": 1.4122343419392904, "mse": 0.0}  0.505
0         train   11600/102467 73:27/575:26       1.438         {"mlm": 1.4132845263928175, "mse": 0.0}  0.5203
0         train   11700/102467 74:04/574:36       1.437         {"mlm": 1.413480364855598, "mse": 0.0}  0.4792
0         train   11800/102467 74:40/573:47       1.437         {"mlm": 1.413776845269733, "mse": 0.0}  0.5016
0         train   11900/102467 75:17/572:57       1.437         {"mlm": 1.4112582376756166, "mse": 0.0}  1.1705
0         train   12000/102467 75:53/572:09       1.436         {"mlm": 1.4105429020524025, "mse": 0.0}  0.4878
0         train   12100/102467 76:29/571:19       1.436         {"mlm": 1.4061861832936604, "mse": 0.0}  0.4961
0         train   12200/102467 77:06/570:30       1.436         {"mlm": 1.4048555412484174, "mse": 0.0}  0.4863
0         train   12300/102467 77:42/569:41       1.436         {"mlm": 1.4032778066137563, "mse": 0.0}  0.497
0         train   12400/102467 78:19/568:53       1.435         {"mlm": 1.4062891904274024, "mse": 0.0}  0.5101
0         train   12500/102467 78:55/568:04       1.435         {"mlm": 1.412505963402903, "mse": 0.0}  0.4705
0         train   12600/102467 79:32/567:16       1.435         {"mlm": 1.4131835507033066, "mse": 0.0}  0.491
0         train   12700/102467 80:08/566:28       1.435         {"mlm": 1.4071407827184947, "mse": 0.0}  0.5092
0         train   12800/102467 80:45/565:40       1.435         {"mlm": 1.407036556394885, "mse": 0.0}  0.4976
0         train   12900/102467 81:21/564:53       1.434         {"mlm": 1.4060032909385884, "mse": 0.0}  0.4909
0         train   13000/102467 81:57/564:05       1.434         {"mlm": 1.4068243501780628, "mse": 0.0}  0.513
0         train   13100/102467 82:34/563:18       1.434         {"mlm": 1.4092184130010874, "mse": 0.0}  0.5172
0         train   13200/102467 83:10/562:31       1.434         {"mlm": 1.4093544454848994, "mse": 0.0}  0.4993
0         train   13300/102467 83:47/561:44       1.434         {"mlm": 1.4091731113044, "mse": 0.0}  0.7629
0         train   13400/102467 84:23/560:57       1.433         {"mlm": 1.407471084483953, "mse": 0.0}  0.4826
0         train   13500/102467 85:00/560:11       1.433         {"mlm": 1.4068573446493295, "mse": 0.0}  0.6259
0         train   13600/102467 85:36/559:24       1.433         {"mlm": 1.408790180316338, "mse": 0.0}  0.4839
0         train   13700/102467 86:13/558:37       1.433         {"mlm": 1.4101595486732985, "mse": 0.0}  0.5101
0         train   13800/102467 86:49/557:51       1.433         {"mlm": 1.4112283689833933, "mse": 0.0}  0.5231
0         train   13900/102467 87:25/557:05       1.433         {"mlm": 1.4126648714568755, "mse": 0.0}  0.5152
0         train   14000/102467 88:02/556:18       1.433         {"mlm": 1.4132166440752878, "mse": 0.0}  0.4926
0         train   14100/102467 88:38/555:33       1.433         {"mlm": 1.4299840464883922, "mse": 0.0}  0.5118
0         train   14200/102467 89:15/554:46       1.433         {"mlm": 1.4148941362144971, "mse": 0.0}  0.5316
0         train   14300/102467 89:51/554:01       1.433         {"mlm": 1.4118658792252508, "mse": 0.0}  0.5626
0         train   14400/102467 90:27/553:16       1.432         {"mlm": 1.4087037396191353, "mse": 0.0}  0.5361
0         train   14500/102467 91:04/552:30       1.432         {"mlm": 1.4110482789905197, "mse": 0.0}  0.5191
0         train   14600/102467 91:40/551:44       1.432         {"mlm": 1.4125498053820238, "mse": 0.0}  0.5057
0         train   14700/102467 92:17/550:59       1.432         {"mlm": 1.4137247092915128, "mse": 0.0}  0.4807
0         train   14800/102467 92:53/550:14       1.432         {"mlm": 1.4146853127425774, "mse": 0.0}  0.7443
0         train   14900/102467 93:30/549:30       1.432         {"mlm": 1.4135761409002316, "mse": 0.0}  0.5104
0         train   15000/102467 94:06/548:45       1.432         {"mlm": 1.4132777462621968, "mse": 0.0}  0.5284
0         train   15100/102467 94:42/548:00       1.432         {"mlm": 1.4128140758295529, "mse": 0.0}  0.51
0         train   15200/102467 95:19/547:16       1.431         {"mlm": 1.4121621338672352, "mse": 0.0}  0.5214
0         train   15300/102467 95:55/546:31       1.431         {"mlm": 1.4119492659032438, "mse": 0.0}  0.54
0         train   15400/102467 96:32/545:47       1.431         {"mlm": 1.411243661619904, "mse": 0.0}  0.517
0         train   15500/102467 97:08/545:03       1.431         {"mlm": 1.409953443723941, "mse": 0.0}  0.5853
0         train   15600/102467 97:45/544:19       1.431         {"mlm": 1.4084451226552526, "mse": 0.0}  0.5264
0         train   15700/102467 98:21/543:34       1.430         {"mlm": 1.4083957855707063, "mse": 0.0}  0.5434
0         train   15800/102467 98:57/542:50       1.430         {"mlm": 1.4078992904558596, "mse": 0.0}  0.6459
0         train   15900/102467 99:34/542:06       1.430         {"mlm": 1.4088348636700054, "mse": 0.0}  0.5323
0         train   16000/102467 100:10/541:22      1.430         {"mlm": 1.4069622688524954, "mse": 0.0}  0.556
0         train   16100/102467 100:47/540:38      1.430         {"mlm": 1.3907201296275424, "mse": 0.0}  0.5044
0         train   16200/102467 101:23/539:55      1.429         {"mlm": 1.3929324552492441, "mse": 0.0}  0.5221
0         train   16300/102467 101:59/539:11      1.429         {"mlm": 1.4040654414029234, "mse": 0.0}  0.5635
0         train   16400/102467 102:37/538:33      1.429         {"mlm": 1.4029363416304217, "mse": 0.0}  0.5537
0         train   16500/102467 103:13/537:49      1.429         {"mlm": 1.4006602446318153, "mse": 0.0}  0.55
0         train   16600/102467 103:49/537:05      1.429         {"mlm": 1.4021235238167908, "mse": 0.0}  0.5163
0         train   16700/102467 104:26/536:22      1.429         {"mlm": 1.4024342410533637, "mse": 0.0}  0.5527
0         train   16800/102467 105:02/535:38      1.428         {"mlm": 1.3989721243474236, "mse": 0.0}  0.54
0         train   16900/102467 105:39/534:55      1.428         {"mlm": 1.399237835885689, "mse": 0.0}  0.5432
0         train   17000/102467 106:15/534:12      1.428         {"mlm": 1.3999504279588146, "mse": 0.0}  0.5465
0         train   17100/102467 106:51/533:29      1.428         {"mlm": 1.400648646606786, "mse": 0.0}  0.5015
0         train   17200/102467 107:28/532:46      1.428         {"mlm": 1.4006133369335854, "mse": 0.0}  0.5304
0         train   17300/102467 108:04/532:04      1.427         {"mlm": 1.3984319340199256, "mse": 0.0}  0.5253
0         train   17400/102467 108:41/531:22      1.427         {"mlm": 1.3981324382654325, "mse": 0.0}  0.5329
0         train   17500/102467 109:17/530:40      1.427         {"mlm": 1.3991947317011928, "mse": 0.0}  0.574
0         train   17600/102467 109:54/529:57      1.427         {"mlm": 1.3966265492463157, "mse": 0.0}  0.5386
0         train   17700/102467 110:30/529:15      1.427         {"mlm": 1.3973686154196385, "mse": 0.0}  0.5565
0         train   17800/102467 111:07/528:32      1.427         {"mlm": 1.3984691483415892, "mse": 0.0}  0.5199
0         train   17900/102467 111:43/527:51      1.427         {"mlm": 1.398936882261609, "mse": 0.0}  0.5581
0         train   18000/102467 112:20/527:09      1.426         {"mlm": 1.3981191746640576, "mse": 0.0}  0.5224
0         train   18100/102467 112:56/526:27      1.426         {"mlm": 1.3738985620439053, "mse": 0.0}  0.5511
0         train   18200/102467 113:33/525:45      1.426         {"mlm": 1.382149193359881, "mse": 0.0}  0.5124
0         train   18300/102467 114:09/525:03      1.426         {"mlm": 1.3750532974262495, "mse": 0.0}  0.5129
0         train   18400/102467 114:46/524:22      1.425         {"mlm": 1.3773230902775369, "mse": 0.0}  0.5508
0         train   18500/102467 115:22/523:40      1.425         {"mlm": 1.385219169600356, "mse": 0.0}  0.5957
0         train   18600/102467 115:59/522:59      1.425         {"mlm": 1.3905549782434565, "mse": 0.0}  0.5337
0         train   18700/102467 116:35/522:17      1.425         {"mlm": 1.3946618591916973, "mse": 0.0}  0.5788
0         train   18800/102467 117:12/521:35      1.425         {"mlm": 1.3954740684835156, "mse": 0.0}  0.5159
0         train   18900/102467 117:48/520:54      1.425         {"mlm": 1.398547827665295, "mse": 0.0}  0.6395
0         train   19000/102467 118:25/520:12      1.425         {"mlm": 1.3976440672654225, "mse": 0.0}  0.5559
0         train   19100/102467 119:01/519:30      1.425         {"mlm": 1.398933963183939, "mse": 0.0}  0.5451
0         train   19200/102467 119:37/518:49      1.425         {"mlm": 1.3979330696290153, "mse": 0.0}  0.5324
0         train   19300/102467 120:14/518:07      1.424         {"mlm": 1.3982203576575827, "mse": 0.0}  0.5288
0         train   19400/102467 120:50/517:26      1.424         {"mlm": 1.3988762741700285, "mse": 0.0}  0.5281
0         train   19500/102467 121:27/516:44      1.424         {"mlm": 1.3985737764978792, "mse": 0.0}  0.5787
0         train   19600/102467 122:03/516:02      1.424         {"mlm": 1.4002118884097963, "mse": 0.0}  0.5259
0         train   19700/102467 122:39/515:21      1.424         {"mlm": 1.398696247031385, "mse": 0.0}  0.5532
0         train   19800/102467 123:16/514:40      1.424         {"mlm": 1.3988362370859542, "mse": 0.0}  0.5568
0         train   19900/102467 123:52/513:58      1.424         {"mlm": 1.3981598895436098, "mse": 0.0}  0.8048
0         train   20000/102467 124:28/513:17      1.423         {"mlm": 1.39717348986016, "mse": 0.0}  1.8411

09/24/2022 09:12:29 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step20000.pkl
0         valid   1/781        0:38/495:11        1.570           None
0         valid   101/781      0:53/ 6:00         1.294           None
0         valid   201/781      1:09/ 3:19         1.302           None
0         valid   301/781      1:24/ 2:15         1.293           None
0         valid   401/781      1:40/ 1:34         1.290           None
0         valid   501/781      1:55/ 1:04         1.293           None
0         valid   601/781      2:11/ 0:39         1.290           None
0         valid   701/781      2:26/ 0:16         1.294           None
0         valid   781/781      2:41/ 0:00         1.291         {"mlm": 1.291293213828425, "mse": 0.0, "train": 0.0}  None
0         train   20100/102467 127:46/523:34      1.423         {"mlm": 1.3916209876537322, "mse": 0.0}  0.5195
0         train   20200/102467 128:22/522:48      1.423         {"mlm": 1.3920987755060197, "mse": 0.0}  1.6592
0         train   20300/102467 128:58/522:01      1.423         {"mlm": 1.3886098810036978, "mse": 0.0}  0.5427
0         train   20400/102467 129:34/521:14      1.423         {"mlm": 1.38475870937109, "mse": 0.0}  0.5661
0         train   20500/102467 130:10/520:28      1.423         {"mlm": 1.388584318637848, "mse": 0.0}  0.544
0         train   20600/102467 130:46/519:42      1.422         {"mlm": 1.3890189085404079, "mse": 0.0}  0.5416
0         train   20700/102467 131:22/518:57      1.422         {"mlm": 1.3892054312569755, "mse": 0.0}  0.5534
0         train   20800/102467 131:58/518:11      1.422         {"mlm": 1.3871456547826528, "mse": 0.0}  0.5701
0         train   20900/102467 132:35/517:26      1.422         {"mlm": 1.3874866294198567, "mse": 0.0}  0.6218
0         train   21000/102467 133:11/516:40      1.422         {"mlm": 1.387489696919918, "mse": 0.0}  0.5244
0         train   21100/102467 133:47/515:56      1.422         {"mlm": 1.3878388020125303, "mse": 0.0}  0.5665
0         train   21200/102467 134:23/515:11      1.421         {"mlm": 1.3870915739238263, "mse": 0.0}  0.5505
0         train   21300/102467 135:00/514:27      1.421         {"mlm": 1.3881078765484003, "mse": 0.0}  0.56
0         train   21400/102467 135:36/513:42      1.421         {"mlm": 1.3871181762644222, "mse": 0.0}  1.3308
0         train   21500/102467 136:12/512:58      1.421         {"mlm": 1.3876604108015695, "mse": 0.0}  0.6444
0         train   21600/102467 136:49/512:14      1.421         {"mlm": 1.3880199026688933, "mse": 0.0}  0.6775
0         train   21700/102467 137:25/511:30      1.421         {"mlm": 1.3875000588683521, "mse": 0.0}  0.5267
0         train   21800/102467 138:02/510:46      1.420         {"mlm": 1.386873214178615, "mse": 0.0}  0.5381
0         train   21900/102467 138:38/510:03      1.420         {"mlm": 1.3865128010825107, "mse": 0.0}  0.5575
0         train   22000/102467 139:15/509:19      1.420         {"mlm": 1.3860691983997822, "mse": 0.0}  0.5125
0         train   22100/102467 139:51/508:36      1.420         {"mlm": 1.4201118777496646, "mse": 0.0}  0.64
0         train   22200/102467 140:28/507:52      1.420         {"mlm": 1.4022310451047504, "mse": 0.0}  0.573
0         train   22300/102467 141:04/507:09      1.420         {"mlm": 1.394529668782467, "mse": 0.0}  0.5735
0         train   22400/102467 141:40/506:25      1.420         {"mlm": 1.393124553045832, "mse": 0.0}  0.5541
0         train   22500/102467 142:17/505:42      1.419         {"mlm": 1.391020958910963, "mse": 0.0}  0.526
0         train   22600/102467 142:53/504:59      1.419         {"mlm": 1.3922558572137098, "mse": 0.0}  1.2364
0         train   22700/102467 143:30/504:16      1.419         {"mlm": 1.394495764941787, "mse": 0.0}  0.5456
0         train   22800/102467 144:06/503:32      1.419         {"mlm": 1.3932190043606956, "mse": 0.0}  0.5974
0         train   22900/102467 144:43/502:49      1.419         {"mlm": 1.3938762873112824, "mse": 0.0}  0.6296
0         train   23000/102467 145:19/502:06      1.419         {"mlm": 1.3928153202817724, "mse": 0.0}  0.6496
0         train   23100/102467 145:56/501:23      1.419         {"mlm": 1.3923446661890149, "mse": 0.0}  0.5491
0         train   23200/102467 146:32/500:41      1.419         {"mlm": 1.3930206656257145, "mse": 0.0}  0.5662
0         train   23300/102467 147:08/499:58      1.419         {"mlm": 1.3929989003878176, "mse": 0.0}  0.5986
0         train   23400/102467 147:45/499:15      1.418         {"mlm": 1.3926435340038787, "mse": 0.0}  0.5639
0         train   23500/102467 148:21/498:33      1.418         {"mlm": 1.3929427082813763, "mse": 0.0}  0.6357
0         train   23600/102467 148:58/497:50      1.418         {"mlm": 1.392373352292331, "mse": 0.0}  0.5884
0         train   23700/102467 149:35/497:08      1.418         {"mlm": 1.3919763634526217, "mse": 0.0}  0.5411
0         train   23800/102467 150:11/496:26      1.418         {"mlm": 1.3910840728204736, "mse": 0.0}  0.571
0         train   23900/102467 150:48/495:44      1.418         {"mlm": 1.3910365266320328, "mse": 0.0}  1.0529
0         train   24000/102467 151:24/495:02      1.418         {"mlm": 1.3909374782894777, "mse": 0.0}  0.5545
0         train   24100/102467 152:01/494:19      1.417         {"mlm": 1.3871554829636399, "mse": 0.0}  0.5571
0         train   24200/102467 152:37/493:37      1.417         {"mlm": 1.3950616359108625, "mse": 0.0}  0.5745
0         train   24300/102467 153:14/492:55      1.417         {"mlm": 1.3926599393754997, "mse": 0.0}  0.5432
0         train   24400/102467 153:50/492:13      1.417         {"mlm": 1.385846345568422, "mse": 0.0}  0.5618
0         train   24500/102467 154:27/491:31      1.417         {"mlm": 1.386192500591278, "mse": 0.0}  0.5466
0         train   24600/102467 155:03/490:49      1.417         {"mlm": 1.387560018129572, "mse": 0.0}  0.5385
0         train   24700/102467 155:40/490:07      1.417         {"mlm": 1.3918274453171344, "mse": 0.0}  0.5386
0         train   24800/102467 156:16/489:25      1.417         {"mlm": 1.3877438725086682, "mse": 0.0}  0.6073
0         train   24900/102467 156:53/488:43      1.416         {"mlm": 1.386285966249776, "mse": 0.0}  0.5248
0         train   25000/102467 157:29/488:01      1.416         {"mlm": 1.3863588642022893, "mse": 0.0}  0.5511
0         train   25100/102467 158:06/487:19      1.416         {"mlm": 1.3868332406970756, "mse": 0.0}  0.5568
0         train   25200/102467 158:42/486:37      1.416         {"mlm": 1.3855937971595134, "mse": 0.0}  0.5719
0         train   25300/102467 159:19/485:56      1.416         {"mlm": 1.3848652984309822, "mse": 0.0}  0.5418
0         train   25400/102467 159:55/485:14      1.416         {"mlm": 1.3854152750389088, "mse": 0.0}  0.5448
0         train   25500/102467 160:32/484:32      1.416         {"mlm": 1.3853993131735614, "mse": 0.0}  0.5444
0         train   25600/102467 161:08/483:50      1.416         {"mlm": 1.3854682717439677, "mse": 0.0}  0.6386
0         train   25700/102467 161:44/483:09      1.416         {"mlm": 1.3865546593607103, "mse": 0.0}  0.643
0         train   25800/102467 162:21/482:27      1.416         {"mlm": 1.387809280658325, "mse": 0.0}  0.5233
0         train   25900/102467 162:57/481:45      1.415         {"mlm": 1.3880943048929892, "mse": 0.0}  0.5829
0         train   26000/102467 163:34/481:03      1.415         {"mlm": 1.386060687962237, "mse": 0.0}  0.5423
0         train   26100/102467 164:10/480:22      1.415         {"mlm": 1.3909874877978845, "mse": 0.0}  0.5252
0         train   26200/102467 164:47/479:40      1.415         {"mlm": 1.3692746159388933, "mse": 0.0}  0.5199
0         train   26300/102467 165:23/478:59      1.415         {"mlm": 1.3678847027547432, "mse": 0.0}  0.5713
0         train   26400/102467 165:59/478:17      1.415         {"mlm": 1.3742561044560873, "mse": 0.0}  0.5032
0         train   26500/102467 166:36/477:36      1.414         {"mlm": 1.378990003760432, "mse": 0.0}  0.5483
0         train   26600/102467 167:12/476:54      1.414         {"mlm": 1.3828223989636854, "mse": 0.0}  0.6496
0         train   26700/102467 167:49/476:13      1.414         {"mlm": 1.3870197310509265, "mse": 0.0}  0.5449
0         train   26800/102467 168:25/475:32      1.414         {"mlm": 1.3868398898219225, "mse": 0.0}  4.8118
0         train   26900/102467 169:02/474:51      1.414         {"mlm": 1.3867122118954143, "mse": 0.0}  0.5534
0         train   27000/102467 169:38/474:10      1.414         {"mlm": 1.3852003290875625, "mse": 0.0}  0.5561
0         train   27100/102467 170:15/473:28      1.414         {"mlm": 1.385317367809734, "mse": 0.0}  0.5502
0         train   27200/102467 170:51/472:47      1.414         {"mlm": 1.3846513426791855, "mse": 0.0}  0.8915
0         train   27300/102467 171:27/472:06      1.414         {"mlm": 1.3822056863431114, "mse": 0.0}  0.5329
0         train   27400/102467 172:04/471:25      1.413         {"mlm": 1.3824647146629792, "mse": 0.0}  0.555
0         train   27500/102467 172:40/470:43      1.414         {"mlm": 1.3855863805205804, "mse": 0.0}  0.7413
0         train   27600/102467 173:17/470:02      1.414         {"mlm": 1.3882205960671454, "mse": 0.0}  0.532
0         train   27700/102467 173:53/469:21      1.414         {"mlm": 1.389633576309393, "mse": 0.0}  0.6898
0         train   27800/102467 174:29/468:40      1.414         {"mlm": 1.390206411870639, "mse": 0.0}  0.5549
0         train   27900/102467 175:06/467:59      1.414         {"mlm": 1.3909820535651747, "mse": 0.0}  0.5558
0         train   28000/102467 175:42/467:19      1.413         {"mlm": 1.3917230839244592, "mse": 0.0}  0.5525
0         train   28100/102467 176:19/466:38      1.413         {"mlm": 1.390676522006591, "mse": 0.0}  0.5551
0         train   28200/102467 176:55/465:57      1.413         {"mlm": 1.3938859786306108, "mse": 0.0}  0.5323
0         train   28300/102467 177:32/465:16      1.413         {"mlm": 1.384194688418427, "mse": 0.0}  0.5467
0         train   28400/102467 178:08/464:35      1.413         {"mlm": 1.3832097955123344, "mse": 0.0}  0.5477
0         train   28500/102467 178:44/463:54      1.413         {"mlm": 1.3887976784138911, "mse": 0.0}  0.5455
0         train   28600/102467 179:21/463:13      1.413         {"mlm": 1.3917099396454409, "mse": 0.0}  0.5413
0         train   28700/102467 179:57/462:33      1.413         {"mlm": 1.3858525750452075, "mse": 0.0}  0.5495
0         train   28800/102467 180:34/461:52      1.413         {"mlm": 1.3863687107161662, "mse": 0.0}  0.685
0         train   28900/102467 181:10/461:11      1.413         {"mlm": 1.386582131431039, "mse": 0.0}  0.551
0         train   29000/102467 181:47/460:31      1.413         {"mlm": 1.389489491840443, "mse": 0.0}  0.5392
0         train   29100/102467 182:23/459:50      1.413         {"mlm": 1.3894940724011755, "mse": 0.0}  1.3147
0         train   29200/102467 182:59/459:10      1.413         {"mlm": 1.3905956945989444, "mse": 0.0}  0.5345
0         train   29300/102467 183:36/458:29      1.413         {"mlm": 1.3916110270829112, "mse": 0.0}  0.5806
0         train   29400/102467 184:12/457:49      1.412         {"mlm": 1.389771474082354, "mse": 0.0}  0.551
0         train   29500/102467 184:49/457:08      1.412         {"mlm": 1.3899594273637323, "mse": 0.0}  0.5143
0         train   29600/102467 185:25/456:28      1.412         {"mlm": 1.391444176510163, "mse": 0.0}  0.5313
0         train   29700/102467 186:02/455:47      1.412         {"mlm": 1.3906111497924012, "mse": 0.0}  0.5081
0         train   29800/102467 186:38/455:07      1.412         {"mlm": 1.3898838230788308, "mse": 0.0}  0.5429
0         train   29900/102467 187:14/454:26      1.412         {"mlm": 1.3901844433339838, "mse": 0.0}  0.5741
0         train   30000/102467 187:51/453:46      1.412         {"mlm": 1.391184162072285, "mse": 0.0}  0.5241

09/24/2022 10:15:51 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step30000.pkl
0         valid   1/781        0:34/453:16        1.426           None
0         valid   101/781      0:50/ 5:38         1.267           None
0         valid   201/781      1:05/ 3:10         1.288           None
0         valid   301/781      1:21/ 2:09         1.278           None
0         valid   401/781      1:36/ 1:31         1.277           None
0         valid   501/781      1:52/ 1:02         1.282           None
0         valid   601/781      2:08/ 0:38         1.284           None
0         valid   701/781      2:23/ 0:16         1.287           None
0         valid   781/781      2:37/ 0:00         1.285         {"mlm": 1.2850512163892445, "mse": 0.0, "train": 0.0}  None
0         train   30100/102467 191:05/459:25      1.412         {"mlm": 1.393285557627678, "mse": 0.0}  1.0136
0         train   30200/102467 191:41/458:42      1.412         {"mlm": 1.3941105851531028, "mse": 0.0}  0.5232
0         train   30300/102467 192:17/457:59      1.412         {"mlm": 1.4003238036235173, "mse": 0.0}  0.5007
0         train   30400/102467 192:53/457:16      1.412         {"mlm": 1.3971885760128497, "mse": 0.0}  0.5666
0         train   30500/102467 193:29/456:34      1.412         {"mlm": 1.3955230783224106, "mse": 0.0}  0.5799
0         train   30600/102467 194:05/455:51      1.412         {"mlm": 1.3982027106483776, "mse": 0.0}  0.719
0         train   30700/102467 194:42/455:09      1.412         {"mlm": 1.400662699341774, "mse": 0.0}  0.502
0         train   30800/102467 195:18/454:26      1.412         {"mlm": 1.3991067137569189, "mse": 0.0}  0.5086
0         train   30900/102467 195:54/453:44      1.412         {"mlm": 1.398239180445671, "mse": 0.0}  0.5705
0         train   31000/102467 196:30/453:02      1.411         {"mlm": 1.3942470051646232, "mse": 0.0}  0.5157
0         train   31100/102467 197:07/452:20      1.411         {"mlm": 1.393756741231138, "mse": 0.0}  0.9269
0         train   31200/102467 197:43/451:39      1.411         {"mlm": 1.3929581738015016, "mse": 0.0}  0.484
0         train   31300/102467 198:20/450:57      1.411         {"mlm": 1.3919902620407252, "mse": 0.0}  0.5197
0         train   31400/102467 198:56/450:15      1.411         {"mlm": 1.3919937448416437, "mse": 0.0}  0.6588
0         train   31500/102467 199:33/449:34      1.411         {"mlm": 1.389026514728864, "mse": 0.0}  3.6784
0         train   31600/102467 200:09/448:52      1.411         {"mlm": 1.39135640244931, "mse": 0.0}  0.5562
0         train   31700/102467 200:46/448:11      1.411         {"mlm": 1.38922685714329, "mse": 0.0}  0.5182
0         train   31800/102467 201:22/447:30      1.411         {"mlm": 1.3892067396640777, "mse": 0.0}  0.5251
0         train   31900/102467 201:58/446:48      1.411         {"mlm": 1.3884587412758878, "mse": 0.0}  0.5596
0         train   32000/102467 202:35/446:07      1.410         {"mlm": 1.3864118157029153, "mse": 0.0}  0.4874
0         train   32100/102467 203:12/445:26      1.410         {"mlm": 1.4089614357611147, "mse": 0.0}  0.5125
0         train   32200/102467 203:48/444:45      1.410         {"mlm": 1.4003529941017305, "mse": 0.0}  0.5071
0         train   32300/102467 204:25/444:03      1.410         {"mlm": 1.3966004754787305, "mse": 0.0}  0.5071
0         train   32400/102467 205:01/443:22      1.410         {"mlm": 1.3910470449536068, "mse": 0.0}  0.5051
0         train   32500/102467 205:38/442:41      1.410         {"mlm": 1.3881473757460983, "mse": 0.0}  0.4872
0         train   32600/102467 206:14/442:00      1.410         {"mlm": 1.3863468521425442, "mse": 0.0}  0.5338
0         train   32700/102467 206:51/441:19      1.410         {"mlm": 1.3833709574905417, "mse": 0.0}  0.4728
0         train   32800/102467 207:27/440:38      1.410         {"mlm": 1.3828708222870236, "mse": 0.0}  0.5066
0         train   32900/102467 208:03/439:57      1.410         {"mlm": 1.3827853501837564, "mse": 0.0}  0.5064
0         train   33000/102467 208:40/439:16      1.410         {"mlm": 1.3841079674564205, "mse": 0.0}  0.4906
0         train   33100/102467 209:16/438:34      1.410         {"mlm": 1.3865668142894054, "mse": 0.0}  0.5134
0         train   33200/102467 209:53/437:54      1.410         {"mlm": 1.3867122672616292, "mse": 0.0}  0.4767
0         train   33300/102467 210:29/437:13      1.409         {"mlm": 1.3862919165502612, "mse": 0.0}  0.4592
0         train   33400/102467 211:06/436:31      1.409         {"mlm": 1.386508864875859, "mse": 0.0}  0.4819
0         train   33500/102467 211:42/435:51      1.409         {"mlm": 1.386914416262911, "mse": 0.0}  0.475
0         train   33600/102467 212:19/435:10      1.409         {"mlm": 1.3854990068862705, "mse": 0.0}  0.5438
0         train   33700/102467 212:55/434:29      1.409         {"mlm": 1.3849528185615405, "mse": 0.0}  0.4863
0         train   33800/102467 213:31/433:48      1.409         {"mlm": 1.3855931619394481, "mse": 0.0}  0.5518
0         train   33900/102467 214:08/433:07      1.409         {"mlm": 1.3860166953889366, "mse": 0.0}  0.5014
0         train   34000/102467 214:44/432:26      1.409         {"mlm": 1.3851033965309243, "mse": 0.0}  0.6832
0         train   34100/102467 215:21/431:45      1.409         {"mlm": 1.3584950505470743, "mse": 0.0}  0.504
0         train   34200/102467 215:57/431:04      1.409         {"mlm": 1.37890830425301, "mse": 0.0}  0.4602
0         train   34300/102467 216:33/430:23      1.409         {"mlm": 1.3812663923173942, "mse": 0.0}  0.5277
0         train   34400/102467 217:10/429:42      1.409         {"mlm": 1.3850634253803809, "mse": 0.0}  0.5197
0         train   34500/102467 217:46/429:02      1.409         {"mlm": 1.38505333973222, "mse": 0.0}  0.4921
0         train   34600/102467 218:23/428:21      1.408         {"mlm": 1.3802707329840964, "mse": 0.0}  0.6055
0         train   34700/102467 218:59/427:40      1.408         {"mlm": 1.379314270180071, "mse": 0.0}  0.4991
0         train   34800/102467 219:35/426:59      1.408         {"mlm": 1.3772365790710115, "mse": 0.0}  0.5247
0         train   34900/102467 220:12/426:19      1.408         {"mlm": 1.3782424648647054, "mse": 0.0}  0.4782
0         train   35000/102467 220:48/425:38      1.408         {"mlm": 1.377557747648331, "mse": 0.0}  0.6149
0         train   35100/102467 221:25/424:58      1.408         {"mlm": 1.3774478401316972, "mse": 0.0}  0.5923
0         train   35200/102467 222:01/424:17      1.408         {"mlm": 1.3776191860586653, "mse": 0.0}  0.4649
0         train   35300/102467 222:38/423:37      1.408         {"mlm": 1.3782455426610674, "mse": 0.0}  0.5271
0         train   35400/102467 223:14/422:56      1.408         {"mlm": 1.377791382383038, "mse": 0.0}  0.4756
0         train   35500/102467 223:51/422:16      1.408         {"mlm": 1.3774565514798796, "mse": 0.0}  0.5223
0         train   35600/102467 224:27/421:35      1.407         {"mlm": 1.3764627906721136, "mse": 0.0}  0.6155
0         train   35700/102467 225:03/420:55      1.407         {"mlm": 1.375580712889614, "mse": 0.0}  0.4456
0         train   35800/102467 225:40/420:14      1.407         {"mlm": 1.3769111807672545, "mse": 0.0}  0.4977
0         train   35900/102467 226:16/419:34      1.407         {"mlm": 1.377265747216529, "mse": 0.0}  0.4521
0         train   36000/102467 226:53/418:54      1.407         {"mlm": 1.378240711278505, "mse": 0.0}  0.476
0         train   36100/102467 227:29/418:14      1.407         {"mlm": 1.3791690327457546, "mse": 0.0}  0.4955
0         train   36200/102467 228:06/417:34      1.407         {"mlm": 1.3721871061373483, "mse": 0.0}  0.4571
0         train   36300/102467 228:42/416:53      1.407         {"mlm": 1.3735961432408805, "mse": 0.0}  0.459
0         train   36400/102467 229:19/416:13      1.407         {"mlm": 1.3782895801950161, "mse": 0.0}  0.4852
0         train   36500/102467 229:55/415:33      1.407         {"mlm": 1.3771446368823828, "mse": 0.0}  0.461
0         train   36600/102467 230:32/414:53      1.407         {"mlm": 1.3812728406396544, "mse": 0.0}  0.5149
0         train   36700/102467 231:08/414:13      1.407         {"mlm": 1.3813479228033396, "mse": 0.0}  0.457
0         train   36800/102467 231:45/413:32      1.407         {"mlm": 1.3800611007617438, "mse": 0.0}  0.457
0         train   36900/102467 232:21/412:52      1.406         {"mlm": 1.377038380440528, "mse": 0.0}  0.4646
0         train   37000/102467 232:58/412:12      1.406         {"mlm": 1.3755693402787748, "mse": 0.0}  0.5362
0         train   37100/102467 233:34/411:32      1.406         {"mlm": 1.3765404374358212, "mse": 0.0}  0.5045
0         train   37200/102467 234:10/410:52      1.406         {"mlm": 1.3784534290818842, "mse": 0.0}  0.5908
0         train   37300/102467 234:47/410:11      1.406         {"mlm": 1.3767604867862386, "mse": 0.0}  0.5912
0         train   37400/102467 235:23/409:31      1.406         {"mlm": 1.378707764570254, "mse": 0.0}  0.5758
0         train   37500/102467 236:00/408:51      1.406         {"mlm": 1.3779821441264335, "mse": 0.0}  0.4578
0         train   37600/102467 236:36/408:11      1.406         {"mlm": 1.377906777370551, "mse": 0.0}  0.4205
0         train   37700/102467 237:12/407:31      1.406         {"mlm": 1.3768903751828492, "mse": 0.0}  0.4662
0         train   37800/102467 237:49/406:51      1.406         {"mlm": 1.3762792214859043, "mse": 0.0}  0.4352
0         train   37900/102467 238:25/406:11      1.406         {"mlm": 1.3779361726236017, "mse": 0.0}  0.4555
0         train   38000/102467 239:02/405:31      1.406         {"mlm": 1.3776224675510427, "mse": 0.0}  0.49
0         train   38100/102467 239:38/404:51      1.406         {"mlm": 1.3676664717495441, "mse": 0.0}  0.4532
0         train   38200/102467 240:14/404:11      1.406         {"mlm": 1.3794032061586574, "mse": 0.0}  0.4585
0         train   38300/102467 240:51/403:31      1.405         {"mlm": 1.3802943998897397, "mse": 0.0}  0.4639
0         train   38400/102467 241:27/402:51      1.405         {"mlm": 1.3757612362052456, "mse": 0.0}  0.4554
0         train   38500/102467 242:04/402:11      1.405         {"mlm": 1.3742871197961992, "mse": 0.0}  0.4637
0         train   38600/102467 242:40/401:31      1.405         {"mlm": 1.3734474075120569, "mse": 0.0}  1.3591
0         train   38700/102467 243:17/400:52      1.405         {"mlm": 1.3788273892012135, "mse": 0.0}  0.5352
0         train   38800/102467 243:53/400:12      1.405         {"mlm": 1.3784610998241147, "mse": 0.0}  0.4679
0         train   38900/102467 244:29/399:32      1.405         {"mlm": 1.3778629579049135, "mse": 0.0}  0.4255
0         train   39000/102467 245:06/398:52      1.405         {"mlm": 1.3815559725086373, "mse": 0.0}  0.4373
0         train   39100/102467 245:42/398:12      1.405         {"mlm": 1.3801391558708065, "mse": 0.0}  0.4207
0         train   39200/102467 246:19/397:32      1.405         {"mlm": 1.379826058312802, "mse": 0.0}  0.4596
0         train   39300/102467 246:55/396:53      1.405         {"mlm": 1.3784988779620624, "mse": 0.0}  0.5016
0         train   39400/102467 247:32/396:13      1.405         {"mlm": 1.3773603859466945, "mse": 0.0}  0.46
0         train   39500/102467 248:08/395:33      1.405         {"mlm": 1.3757958916180275, "mse": 0.0}  0.4705
0         train   39600/102467 248:44/394:53      1.404         {"mlm": 1.3748248171313364, "mse": 0.0}  0.5899
0         train   39700/102467 249:21/394:13      1.404         {"mlm": 1.3748759036215972, "mse": 0.0}  0.4474
0         train   39800/102467 249:57/393:34      1.404         {"mlm": 1.3747111197834823, "mse": 0.0}  0.4199
0         train   39900/102467 250:34/392:54      1.404         {"mlm": 1.373761322647962, "mse": 0.0}  0.5859
0         train   40000/102467 251:10/392:15      1.404         {"mlm": 1.3742599671852134, "mse": 0.0}  0.5698

09/24/2022 11:19:10 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step40000.pkl
0         valid   1/781        0:39/508:32        1.319           None
0         valid   101/781      0:54/ 6:07         1.304           None
0         valid   201/781      1:10/ 3:22         1.312           None
0         valid   301/781      1:25/ 2:16         1.306           None
0         valid   401/781      1:41/ 1:35         1.302           None
0         valid   501/781      1:56/ 1:05         1.306           None
0         valid   601/781      2:12/ 0:39         1.302           None
0         valid   701/781      2:27/ 0:16         1.307           None
0         valid   781/781      2:42/ 0:00         1.314         {"mlm": 1.3143405889884763, "mse": 0.0, "train": 0.0}  None
0         train   40100/102467 254:28/395:47      1.404         {"mlm": 1.406137467622757, "mse": 0.0}  1.6927
0         train   40200/102467 255:04/395:05      1.404         {"mlm": 1.4133274421095847, "mse": 0.0}  0.5608
0         train   40300/102467 255:40/394:24      1.404         {"mlm": 1.4018948727846146, "mse": 0.0}  0.5266
0         train   40400/102467 256:16/393:43      1.404         {"mlm": 1.3994022044539451, "mse": 0.0}  0.5037
0         train   40500/102467 256:52/393:02      1.404         {"mlm": 1.39064085149765, "mse": 0.0}  0.4516
0         train   40600/102467 257:29/392:21      1.404         {"mlm": 1.3875678084294, "mse": 0.0}  0.81
0         train   40700/102467 258:05/391:40      1.404         {"mlm": 1.3827171931096485, "mse": 0.0}  0.4814
0         train   40800/102467 258:41/390:59      1.404         {"mlm": 1.3823220569640398, "mse": 0.0}  0.4471
0         train   40900/102467 259:17/390:19      1.404         {"mlm": 1.3792804035213257, "mse": 0.0}  0.4317
0         train   41000/102467 259:54/389:38      1.403         {"mlm": 1.3778504975438117, "mse": 0.0}  0.9738
0         train   41100/102467 260:30/388:58      1.403         {"mlm": 1.3765203666687011, "mse": 0.0}  0.4948
0         train   41200/102467 261:06/388:17      1.403         {"mlm": 1.3747573234140873, "mse": 0.0}  1.8706
0         train   41300/102467 261:43/387:37      1.403         {"mlm": 1.3741488693769162, "mse": 0.0}  0.4771
0         train   41400/102467 262:19/386:56      1.403         {"mlm": 1.3727318557245392, "mse": 0.0}  0.8154
0         train   41500/102467 262:56/386:16      1.403         {"mlm": 1.3732485264539718, "mse": 0.0}  0.4487
0         train   41600/102467 263:32/385:36      1.403         {"mlm": 1.372918046414852, "mse": 0.0}  0.4729
0         train   41700/102467 264:08/384:55      1.403         {"mlm": 1.372265847079894, "mse": 0.0}  0.4319
0         train   41800/102467 264:45/384:15      1.403         {"mlm": 1.372128643989563, "mse": 0.0}  0.4432
0         train   41900/102467 265:21/383:35      1.403         {"mlm": 1.371210206088267, "mse": 0.0}  0.4303
0         train   42000/102467 265:58/382:54      1.403         {"mlm": 1.3711090771257877, "mse": 0.0}  0.683
0         train   42100/102467 266:34/382:14      1.402         {"mlm": 1.355921152866248, "mse": 0.0}  0.6382
0         train   42200/102467 267:11/381:34      1.402         {"mlm": 1.3792590933229456, "mse": 0.0}  0.4401
0         train   42300/102467 267:47/380:54      1.402         {"mlm": 1.3724449159309617, "mse": 0.0}  0.4304
0         train   42400/102467 268:23/380:13      1.402         {"mlm": 1.3766396944982964, "mse": 0.0}  0.5845
0         train   42500/102467 269:00/379:33      1.402         {"mlm": 1.3727781779780417, "mse": 0.0}  0.4658
0         train   42600/102467 269:36/378:53      1.402         {"mlm": 1.3657268111614234, "mse": 0.0}  0.439
0         train   42700/102467 270:12/378:13      1.402         {"mlm": 1.3703083438593602, "mse": 0.0}  0.4802
0         train   42800/102467 270:49/377:33      1.402         {"mlm": 1.3723103868797217, "mse": 0.0}  0.4461
0         train   42900/102467 271:25/376:53      1.402         {"mlm": 1.3717610852604316, "mse": 0.0}  0.4367
0         train   43000/102467 272:02/376:12      1.402         {"mlm": 1.3722443517979916, "mse": 0.0}  0.4289
0         train   43100/102467 272:38/375:32      1.402         {"mlm": 1.3715078438597446, "mse": 0.0}  0.5837
0         train   43200/102467 273:15/374:52      1.402         {"mlm": 1.3697274629526879, "mse": 0.0}  1.176
0         train   43300/102467 273:51/374:12      1.402         {"mlm": 1.3698982009986806, "mse": 0.0}  0.4112
0         train   43400/102467 274:27/373:32      1.402         {"mlm": 1.3703727603725573, "mse": 0.0}  0.6114
0         train   43500/102467 275:04/372:52      1.401         {"mlm": 1.3711550872352936, "mse": 0.0}  2.1426
0         train   43600/102467 275:40/372:12      1.401         {"mlm": 1.371599783779607, "mse": 0.0}  1.4997
0         train   43700/102467 276:17/371:32      1.401         {"mlm": 1.3703783228511035, "mse": 0.0}  0.3892
0         train   43800/102467 276:53/370:52      1.401         {"mlm": 1.370636441861344, "mse": 0.0}  0.5294
0         train   43900/102467 277:30/370:12      1.401         {"mlm": 1.3696764319303352, "mse": 0.0}  0.4189
0         train   44000/102467 278:06/369:32      1.401         {"mlm": 1.36927724966948, "mse": 0.0}  0.4453
0         train   44100/102467 278:42/368:52      1.401         {"mlm": 1.3704531764497563, "mse": 0.0}  0.4811
0         train   44200/102467 279:19/368:12      1.401         {"mlm": 1.37991124241039, "mse": 0.0}  0.4388
0         train   44300/102467 279:55/367:32      1.401         {"mlm": 1.3733507560803586, "mse": 0.0}  0.4048
0         train   44400/102467 280:31/366:52      1.401         {"mlm": 1.3749958751489169, "mse": 0.0}  0.4358
0         train   44500/102467 281:08/366:13      1.401         {"mlm": 1.3746373560055192, "mse": 0.0}  0.5011
0         train   44600/102467 281:44/365:33      1.401         {"mlm": 1.37581521053378, "mse": 0.0}  0.4536
0         train   44700/102467 282:20/364:53      1.401         {"mlm": 1.3728342587421822, "mse": 0.0}  0.401
0         train   44800/102467 282:57/364:13      1.401         {"mlm": 1.37408687506702, "mse": 0.0}  0.4191
0         train   44900/102467 283:33/363:33      1.400         {"mlm": 1.369790751711032, "mse": 0.0}  0.3935
0         train   45000/102467 284:10/362:54      1.400         {"mlm": 1.368076869445239, "mse": 0.0}  0.5816
0         train   45100/102467 284:46/362:14      1.400         {"mlm": 1.3674160324050646, "mse": 0.0}  0.6791
0         train   45200/102467 285:23/361:34      1.400         {"mlm": 1.3660801371966858, "mse": 0.0}  0.4003
0         train   45300/102467 285:59/360:54      1.400         {"mlm": 1.3674150773758882, "mse": 0.0}  0.5135
0         train   45400/102467 286:35/360:14      1.400         {"mlm": 1.367107837369002, "mse": 0.0}  2.8561
0         train   45500/102467 287:12/359:35      1.400         {"mlm": 1.3650259556216455, "mse": 0.0}  0.4957
0         train   45600/102467 287:48/358:55      1.400         {"mlm": 1.3663974314070166, "mse": 0.0}  1.5748
0         train   45700/102467 288:25/358:15      1.400         {"mlm": 1.3668896097808338, "mse": 0.0}  0.4275
0         train   45800/102467 289:01/357:36      1.400         {"mlm": 1.3659241980188284, "mse": 0.0}  0.4355
0         train   45900/102467 289:38/356:56      1.400         {"mlm": 1.3651020649047245, "mse": 0.0}  0.4435
0         train   46000/102467 290:14/356:17      1.399         {"mlm": 1.3645035912981023, "mse": 0.0}  0.7026
0         train   46100/102467 290:50/355:37      1.399         {"mlm": 1.3379252079835873, "mse": 0.0}  1.32
0         train   46200/102467 291:27/354:57      1.399         {"mlm": 1.3368687018525176, "mse": 0.0}  0.4121
0         train   46300/102467 292:03/354:18      1.399         {"mlm": 1.3413871751489863, "mse": 0.0}  0.6224
0         train   46400/102467 292:40/353:38      1.399         {"mlm": 1.3441762539841966, "mse": 0.0}  0.5828
0         train   46500/102467 293:16/352:59      1.399         {"mlm": 1.3438545392312755, "mse": 0.0}  0.3939
0         train   46600/102467 293:53/352:19      1.399         {"mlm": 1.3442194695448755, "mse": 0.0}  0.4014
0         train   46700/102467 294:29/351:40      1.399         {"mlm": 1.3474499408791705, "mse": 0.0}  0.4571
0         train   46800/102467 295:06/351:01      1.399         {"mlm": 1.3484610387193066, "mse": 0.0}  0.4169
0         train   46900/102467 295:42/350:21      1.398         {"mlm": 1.347448561303193, "mse": 0.0}  0.4049
0         train   47000/102467 296:19/349:42      1.398         {"mlm": 1.346810743167862, "mse": 0.0}  0.3994
0         train   47100/102467 296:56/349:03      1.398         {"mlm": 1.3442224459204766, "mse": 0.0}  0.3788
0         train   47200/102467 297:32/348:23      1.398         {"mlm": 1.3433489825890874, "mse": 0.0}  0.3855
0         train   47300/102467 298:09/347:44      1.398         {"mlm": 1.3421645678734908, "mse": 0.0}  0.4135
0         train   47400/102467 298:45/347:05      1.398         {"mlm": 1.3424288614272388, "mse": 0.0}  0.482
0         train   47500/102467 299:22/346:26      1.398         {"mlm": 1.3429868669611817, "mse": 0.0}  0.466
0         train   47600/102467 299:59/345:46      1.398         {"mlm": 1.3435730685572662, "mse": 0.0}  0.3751
0         train   47700/102467 300:35/345:07      1.397         {"mlm": 1.34455435888586, "mse": 0.0}  0.4311
0         train   47800/102467 301:12/344:28      1.397         {"mlm": 1.344422995944652, "mse": 0.0}  0.3563
0         train   47900/102467 301:48/343:49      1.397         {"mlm": 1.3458735059296765, "mse": 0.0}  0.5066
0         train   48000/102467 302:24/343:09      1.397         {"mlm": 1.3450885391796477, "mse": 0.0}  1.1355
0         train   48100/102467 303:01/342:30      1.397         {"mlm": 1.350344682733218, "mse": 0.0}  0.4676
0         train   48200/102467 303:37/341:50      1.397         {"mlm": 1.3503624431940975, "mse": 0.0}  0.3963
0         train   48300/102467 304:14/341:11      1.397         {"mlm": 1.3536647316571828, "mse": 0.0}  0.3924
0         train   48400/102467 304:50/340:32      1.397         {"mlm": 1.353364041658363, "mse": 0.0}  0.3782
0         train   48500/102467 305:27/339:52      1.397         {"mlm": 1.3519811264930233, "mse": 0.0}  0.3976
0         train   48600/102467 306:03/339:13      1.397         {"mlm": 1.3540742625326119, "mse": 0.0}  0.5746
0         train   48700/102467 306:39/338:34      1.397         {"mlm": 1.3548814595259469, "mse": 0.0}  0.3739
0         train   48800/102467 307:16/337:54      1.396         {"mlm": 1.3518821048227387, "mse": 0.0}  0.7822
0         train   48900/102467 307:52/337:15      1.396         {"mlm": 1.352487489514585, "mse": 0.0}  0.3868
0         train   49000/102467 308:29/336:36      1.396         {"mlm": 1.3530053670506879, "mse": 0.0}  0.3864
0         train   49100/102467 309:05/335:57      1.396         {"mlm": 1.3487376169349155, "mse": 0.0}  0.6871
0         train   49200/102467 309:41/335:17      1.396         {"mlm": 1.3504411329493475, "mse": 0.0}  0.5208
0         train   49300/102467 310:18/334:38      1.396         {"mlm": 1.3493847118943563, "mse": 0.0}  0.5898
0         train   49400/102467 310:54/333:59      1.396         {"mlm": 1.349581862290131, "mse": 0.0}  0.4341
0         train   49500/102467 311:31/333:20      1.396         {"mlm": 1.348502926130027, "mse": 0.0}  0.3943
0         train   49600/102467 312:07/332:41      1.396         {"mlm": 1.3486565428792983, "mse": 0.0}  0.3919
0         train   49700/102467 312:43/332:01      1.396         {"mlm": 1.3496535228178748, "mse": 0.0}  0.3746
0         train   49800/102467 313:20/331:22      1.395         {"mlm": 1.3493964355812307, "mse": 0.0}  8.8288
0         train   49900/102467 313:56/330:43      1.395         {"mlm": 1.3506423601181194, "mse": 0.0}  0.5363
0         train   50000/102467 314:33/330:04      1.395         {"mlm": 1.3505996610215287, "mse": 0.0}  0.412

09/24/2022 12:22:33 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step50000.pkl
0         valid   1/781        0:25/327:30        1.155           None
0         valid   101/781      0:40/ 4:34         1.256           None
0         valid   201/781      0:56/ 2:42         1.262           None
0         valid   301/781      1:11/ 1:54         1.257           None
0         valid   401/781      1:27/ 1:22         1.253           None
0         valid   501/781      1:43/ 0:57         1.253           None
0         valid   601/781      1:58/ 0:35         1.253           None
0         valid   701/781      2:14/ 0:15         1.258           None
0         valid   781/781      2:28/ 0:00         1.252         {"mlm": 1.2516005121638925, "mse": 0.0, "train": 0.0}  None
0         train   50100/102467 317:37/332:00      1.395         {"mlm": 1.3095068836212158, "mse": 0.0}  0.5367
0         train   50200/102467 318:13/331:19      1.395         {"mlm": 1.3431138116121293, "mse": 0.0}  0.4887
0         train   50300/102467 318:49/330:39      1.395         {"mlm": 1.3704318225383758, "mse": 0.0}  4.3113
0         train   50400/102467 319:26/329:59      1.395         {"mlm": 1.3798755103349685, "mse": 0.0}  81.0893
0         train   50500/102467 320:02/329:19      1.395         {"mlm": 1.3826827214956283, "mse": 0.0}  0.5627
0         train   50600/102467 320:38/328:39      1.395         {"mlm": 1.3809347173571587, "mse": 0.0}  0.4356
0         train   50700/102467 321:14/328:00      1.395         {"mlm": 1.3766827286141259, "mse": 0.0}  0.9295
0         train   50800/102467 321:50/327:20      1.395         {"mlm": 1.3755542259663343, "mse": 0.0}  0.4142
0         train   50900/102467 322:26/326:40      1.395         {"mlm": 1.3720609428485235, "mse": 0.0}  0.4137
0         train   51000/102467 323:03/326:00      1.395         {"mlm": 1.3693733399510384, "mse": 0.0}  0.3932
0         train   51100/102467 323:39/325:21      1.395         {"mlm": 1.368124769817699, "mse": 0.0}  1.8376
0         train   51200/102467 324:16/324:41      1.395         {"mlm": 1.3679270699620247, "mse": 0.0}  0.4727
0         train   51300/102467 324:52/324:01      1.395         {"mlm": 1.3671779146561256, "mse": 0.0}  0.9725
0         train   51400/102467 325:28/323:22      1.395         {"mlm": 1.3670725680674825, "mse": 0.0}  9.497
0         train   51500/102467 326:04/322:42      1.394         {"mlm": 1.365301165898641, "mse": 0.0}  0.3767
0         train   51600/102467 326:41/322:03      1.394         {"mlm": 1.3636153671890496, "mse": 0.0}  0.3926
0         train   51700/102467 327:17/321:23      1.394         {"mlm": 1.361708579975016, "mse": 0.0}  0.3838
0         train   51800/102467 327:54/320:43      1.394         {"mlm": 1.360596141119798, "mse": 0.0}  0.4326
0         train   51900/102467 328:30/320:04      1.394         {"mlm": 1.3627911063871885, "mse": 0.0}  38.6183
0         train   52000/102467 329:07/319:25      1.394         {"mlm": 1.3671424270272254, "mse": 0.0}  16.5197
0         train   52100/102467 329:43/318:45      1.394         {"mlm": 1.48985783861141, "mse": 0.0}  0.7855
0         train   52200/102467 330:19/318:06      1.397         {"mlm": 1.9959450259280564, "mse": 0.0}  1.5906
0         train   52300/102467 330:56/317:26      1.397         {"mlm": 1.8045659276554018, "mse": 0.0}  3.0105
0         train   52400/102467 331:32/316:46      1.397         {"mlm": 1.7161086020911844, "mse": 0.0}  3.9199
0         train   52500/102467 332:09/316:07      1.397         {"mlm": 1.655871358089791, "mse": 0.0}  1.6847
0         train   52600/102467 332:45/315:28      1.397         {"mlm": 1.6322214820349157, "mse": 0.0}  0.4776
0         train   52700/102467 333:21/314:48      1.397         {"mlm": 1.596796075673574, "mse": 0.0}  2.5918
0         train   52800/102467 333:58/314:09      1.397         {"mlm": 1.5782082847719348, "mse": 0.0}  0.7755
0         train   52900/102467 334:34/313:29      1.397         {"mlm": 1.5611742770711625, "mse": 0.0}  0.5027
0         train   53000/102467 335:11/312:50      1.397         {"mlm": 1.5435520405764576, "mse": 0.0}  0.3753
0         train   53100/102467 335:47/312:10      1.397         {"mlm": 1.5274334062328112, "mse": 0.0}  0.3677
0         train   53200/102467 336:23/311:31      1.397         {"mlm": 1.516276489704027, "mse": 0.0}  0.3745
0         train   53300/102467 337:00/310:52      1.397         {"mlm": 1.5045902125646373, "mse": 0.0}  0.8003
0         train   53400/102467 337:36/310:12      1.397         {"mlm": 1.494982688191791, "mse": 0.0}  1.0856
0         train   53500/102467 338:12/309:33      1.397         {"mlm": 1.4869880222574403, "mse": 0.0}  0.8144
0         train   53600/102467 338:49/308:54      1.397         {"mlm": 1.4791000814569077, "mse": 0.0}  0.369
0         train   53700/102467 339:25/308:14      1.397         {"mlm": 1.473131486217438, "mse": 0.0}  0.3325
0         train   53800/102467 340:01/307:35      1.397         {"mlm": 1.4676440947581955, "mse": 0.0}  0.406
0         train   53900/102467 340:38/306:56      1.397         {"mlm": 1.460508062595942, "mse": 0.0}  0.3831
0         train   54000/102467 341:14/306:16      1.396         {"mlm": 1.4550375391865682, "mse": 0.0}  0.3602
0         train   54100/102467 341:51/305:37      1.397         {"mlm": 1.4551373610691147, "mse": 0.0}  0.3581
0         train   54200/102467 342:27/304:58      1.396         {"mlm": 1.3950409762787097, "mse": 0.0}  0.4377
0         train   54300/102467 343:04/304:19      1.396         {"mlm": 1.3767336052936197, "mse": 0.0}  0.4396
0         train   54400/102467 343:40/303:40      1.396         {"mlm": 1.3669443182909309, "mse": 0.0}  0.3595
0         train   54500/102467 344:16/303:00      1.396         {"mlm": 1.3664682903682372, "mse": 0.0}  0.5435
0         train   54600/102467 344:53/302:21      1.396         {"mlm": 1.36103627394673, "mse": 0.0}  0.3364
0         train   54700/102467 345:29/301:42      1.396         {"mlm": 1.3611660381101265, "mse": 0.0}  0.3897
0         train   54800/102467 346:06/301:03      1.396         {"mlm": 1.3608504045278507, "mse": 0.0}  0.4111
0         train   54900/102467 346:42/300:24      1.396         {"mlm": 1.362436394317113, "mse": 0.0}  0.3969
0         train   55000/102467 347:19/299:44      1.396         {"mlm": 1.3608026746399178, "mse": 0.0}  0.3884
0         train   55100/102467 347:55/299:05      1.396         {"mlm": 1.36164073502214, "mse": 0.0}  0.4638
0         train   55200/102467 348:31/298:26      1.396         {"mlm": 1.361596982845281, "mse": 0.0}  0.3876
0         train   55300/102467 349:08/297:47      1.396         {"mlm": 1.3596577984618112, "mse": 0.0}  0.8288
0         train   55400/102467 349:44/297:08      1.395         {"mlm": 1.358518891708022, "mse": 0.0}  1.7716
0         train   55500/102467 350:21/296:29      1.395         {"mlm": 1.3572121614687274, "mse": 0.0}  1.5932
0         train   55600/102467 350:57/295:50      1.395         {"mlm": 1.3568800255488394, "mse": 0.0}  0.3589
0         train   55700/102467 351:33/295:10      1.395         {"mlm": 1.3562004552502513, "mse": 0.0}  0.3319
0         train   55800/102467 352:10/294:31      1.395         {"mlm": 1.355455601507087, "mse": 0.0}  0.4887
0         train   55900/102467 352:46/293:52      1.395         {"mlm": 1.3562072557882967, "mse": 0.0}  0.4701
0         train   56000/102467 353:22/293:13      1.395         {"mlm": 1.3547912758212906, "mse": 0.0}  1.1824
0         train   56100/102467 353:59/292:34      1.395         {"mlm": 1.348905325550394, "mse": 0.0}  1.1757
0         train   56200/102467 354:35/291:55      1.395         {"mlm": 1.3396428212296538, "mse": 0.0}  0.8682
0         train   56300/102467 355:12/291:16      1.395         {"mlm": 1.3392074156289149, "mse": 0.0}  0.3961
0         train   56400/102467 355:48/290:37      1.395         {"mlm": 1.348189438019952, "mse": 0.0}  1.1456
0         train   56500/102467 356:24/289:58      1.395         {"mlm": 1.37535071672808, "mse": 0.0}  3.8847
0         train   56600/102467 357:01/289:19      1.395         {"mlm": 1.4122683322010328, "mse": 0.0}  2.0142
0         train   56700/102467 357:37/288:40      1.395         {"mlm": 1.4108164947549444, "mse": 0.0}  1.8172
0         train   56800/102467 358:14/288:01      1.395         {"mlm": 1.4039279976333847, "mse": 0.0}  2.1683
0         train   56900/102467 358:50/287:22      1.395         {"mlm": 1.399532020291357, "mse": 0.0}  1.1302
0         train   57000/102467 359:26/286:42      1.395         {"mlm": 1.396833008964179, "mse": 0.0}  0.7654
0         train   57100/102467 360:03/286:04      1.395         {"mlm": 1.392696024508724, "mse": 0.0}  3.0292
0         train   57200/102467 360:39/285:24      1.395         {"mlm": 1.3889490824096284, "mse": 0.0}  10.58
0         train   57300/102467 361:15/284:46      1.395         {"mlm": 1.384585133449978, "mse": 0.0}  8.7417
0         train   57400/102467 361:52/284:07      1.395         {"mlm": 1.3791246172694027, "mse": 0.0}  2.0582
0         train   57500/102467 362:28/283:28      1.394         {"mlm": 1.3752739362582893, "mse": 0.0}  0.8785
0         train   57600/102467 363:05/282:49      1.394         {"mlm": 1.3761514557400718, "mse": 0.0}  1.4273
0         train   57700/102467 363:41/282:10      1.394         {"mlm": 1.3782782791359114, "mse": 0.0}  1.9393
0         train   57800/102467 364:18/281:31      1.394         {"mlm": 1.3796828404093824, "mse": 0.0}  0.8194
0         train   57900/102467 364:54/280:52      1.394         {"mlm": 1.3776362035610077, "mse": 0.0}  1.5927
0         train   58000/102467 365:31/280:13      1.399         {"mlm": 1.5076334677938825, "mse": 0.0}  269.4418
0         train   58100/102467 366:07/279:35      1.408         {"mlm": 6.919558987021446, "mse": 0.0}  1210.4453
0         train   58200/102467 366:43/278:56      1.413         {"mlm": 5.543496214613622, "mse": 0.0}  974.2385
0         train   58300/102467 367:20/278:17      1.415         {"mlm": 4.48697591432043, "mse": 0.0}  1014.7715
0         train   58400/102467 367:56/277:38      1.417         {"mlm": 3.9904799578767833, "mse": 0.0}  22.7185
0         train   58500/102467 368:33/276:59      1.418         {"mlm": 3.5922722643421543, "mse": 0.0}  1.91
0         train   58600/102467 369:09/276:21      1.419         {"mlm": 3.3399160428335204, "mse": 0.0}  3.193
0         train   58700/102467 369:46/275:42      1.420         {"mlm": 3.1639721153796403, "mse": 0.0}  24.6235
0         train   58800/102467 370:22/275:03      1.421         {"mlm": 3.025358865908043, "mse": 0.0}  1.9262
0         train   58900/102467 370:59/274:24      1.429         {"mlm": 3.366286356999938, "mse": 0.0}  0.2218
0         train   59000/102467 371:35/273:45      1.440         {"mlm": 3.808383340277825, "mse": 0.0}  0.3073
0         train   59100/102467 372:12/273:07      1.451         {"mlm": 4.167262897384862, "mse": 0.0}  0.1785
0         train   59200/102467 372:48/272:28      1.461         {"mlm": 4.466774406989282, "mse": 0.0}  0.1115
0         train   59300/102467 373:24/271:49      1.472         {"mlm": 4.717967803417532, "mse": 0.0}  0.1199
0         train   59400/102467 374:01/271:10      1.482         {"mlm": 4.933774629388976, "mse": 0.0}  0.1153
0         train   59500/102467 374:37/270:31      1.493         {"mlm": 5.121481241030808, "mse": 0.0}  0.1121
0         train   59600/102467 375:14/269:53      1.503         {"mlm": 5.284793056902432, "mse": 0.0}  0.9451
0         train   59700/102467 375:50/269:14      1.514         {"mlm": 5.429540969536833, "mse": 0.0}  12.7351
0         train   59800/102467 376:26/268:35      1.524         {"mlm": 5.557069145989312, "mse": 0.0}  0.0976
0         train   59900/102467 377:03/267:56      1.535         {"mlm": 5.673235832858941, "mse": 0.0}  0.1298
0         train   60000/102467 377:39/267:17      1.545         {"mlm": 5.775963229233612, "mse": 0.0}  0.0976

09/24/2022 13:25:39 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step60000.pkl
0         valid   1/781        0:21/284:32        7.649           None
0         valid   101/781      0:37/ 4:10         7.704           None
0         valid   201/781      0:52/ 2:31         7.724           None
0         valid   301/781      1:07/ 1:48         7.718           None
0         valid   401/781      1:23/ 1:18         7.723           None
0         valid   501/781      1:38/ 0:55         7.724           None
0         valid   601/781      1:53/ 0:34         7.726           None
0         valid   701/781      2:09/ 0:14         7.726           None
0         valid   781/781      2:23/ 0:00         7.724         {"mlm": 7.723751600512164, "mse": 0.0, "train": 0.0}  None
0         train   60100/102467 380:38/268:19      1.555         {"mlm": 7.704024939537049, "mse": 0.0}  0.0921
0         train   60200/102467 381:14/267:40      1.565         {"mlm": 7.707031190395355, "mse": 0.0}  0.0926
0         train   60300/102467 381:50/267:01      1.576         {"mlm": 7.712883543968201, "mse": 0.0}  0.1046
0         train   60400/102467 382:26/266:21      1.586         {"mlm": 7.717504037618637, "mse": 0.0}  0.0993
0         train   60500/102467 383:02/265:42      1.596         {"mlm": 7.717533229827881, "mse": 0.0}  0.0966
0         train   60600/102467 383:38/265:03      1.606         {"mlm": 7.717328727245331, "mse": 0.0}  0.1075
0         train   60700/102467 384:15/264:23      1.616         {"mlm": 7.7190997580119545, "mse": 0.0}  0.1213
0         train   60800/102467 384:51/263:44      1.626         {"mlm": 7.720089811682701, "mse": 0.0}  0.0864
0         train   60900/102467 385:27/263:05      1.636         {"mlm": 7.719729797575209, "mse": 0.0}  0.0987
0         train   61000/102467 386:04/262:26      1.646         {"mlm": 7.720646098136902, "mse": 0.0}  0.1065
0         train   61100/102467 386:40/261:47      1.656         {"mlm": 7.718894400596619, "mse": 0.0}  0.0861
0         train   61200/102467 387:16/261:08      1.666         {"mlm": 7.72027853012085, "mse": 0.0}  0.0962
0         train   61300/102467 387:52/260:29      1.676         {"mlm": 7.720905134861286, "mse": 0.0}  0.0975
0         train   61400/102467 388:29/259:50      1.686         {"mlm": 7.721533399990626, "mse": 0.0}  0.0902
0         train   61500/102467 389:05/259:11      1.695         {"mlm": 7.721665635426839, "mse": 0.0}  0.0966
0         train   61600/102467 389:42/258:32      1.705         {"mlm": 7.7216070023179055, "mse": 0.0}  0.0752
0         train   61700/102467 390:18/257:53      1.715         {"mlm": 7.721315207761877, "mse": 0.0}  0.1072
0         train   61800/102467 390:54/257:14      1.725         {"mlm": 7.720821213722229, "mse": 0.0}  0.0902
0         train   61900/102467 391:31/256:35      1.734         {"mlm": 7.720509578303287, "mse": 0.0}  0.1051
0         train   62000/102467 392:07/255:56      1.744         {"mlm": 7.7200194272994995, "mse": 0.0}  0.0864
0         train   62100/102467 392:43/255:17      1.754         {"mlm": 7.71342894525239, "mse": 0.0}  0.0851
0         train   62200/102467 393:20/254:38      1.763         {"mlm": 7.721845713093053, "mse": 0.0}  0.112
0         train   62300/102467 393:56/253:59      1.773         {"mlm": 7.716495724426065, "mse": 0.0}  0.0832
0         train   62400/102467 394:33/253:20      1.782         {"mlm": 7.715659118833996, "mse": 0.0}  0.0873
0         train   62500/102467 395:09/252:41      1.792         {"mlm": 7.712001193740325, "mse": 0.0}  0.0873
0         train   62600/102467 395:45/252:02      1.801         {"mlm": 7.71024520249916, "mse": 0.0}  0.0795
0         train   62700/102467 396:22/251:23      1.811         {"mlm": 7.709832774723036, "mse": 0.0}  0.0924
0         train   62800/102467 396:58/250:44      1.820         {"mlm": 7.712367035718973, "mse": 0.0}  0.0895
0         train   62900/102467 397:34/250:05      1.829         {"mlm": 7.714875637623041, "mse": 0.0}  0.1203
0         train   63000/102467 398:11/249:26      1.839         {"mlm": 7.71467582408611, "mse": 0.0}  0.0827
0         train   63100/102467 398:47/248:47      1.848         {"mlm": 7.7153789335429614, "mse": 0.0}  0.0874
0         train   63200/102467 399:23/248:08      1.857         {"mlm": 7.715626089447633, "mse": 0.0}  0.0775
0         train   63300/102467 399:59/247:29      1.867         {"mlm": 7.717899414279811, "mse": 0.0}  0.0956
0         train   63400/102467 400:36/246:51      1.876         {"mlm": 7.717224843336736, "mse": 0.0}  0.0729
0         train   63500/102467 401:12/246:12      1.885         {"mlm": 7.7158389791319415, "mse": 0.0}  0.0817
0         train   63600/102467 401:48/245:33      1.894         {"mlm": 7.71650417451936, "mse": 0.0}  0.0707
0         train   63700/102467 402:24/244:54      1.903         {"mlm": 7.715044843371719, "mse": 0.0}  0.0791
0         train   63800/102467 403:01/244:15      1.912         {"mlm": 7.715561114529095, "mse": 0.0}  0.0824
0         train   63900/102467 403:37/243:36      1.922         {"mlm": 7.715016189281911, "mse": 0.0}  0.0687
0         train   64000/102467 404:14/242:57      1.931         {"mlm": 7.714630615955714, "mse": 0.0}  0.0686
0         train   64100/102467 404:50/242:19      1.940         {"mlm": 7.753853189701936, "mse": 0.0}  0.0825
0         train   64200/102467 405:26/241:40      1.949         {"mlm": 7.725105194130329, "mse": 0.0}  0.0743
0         train   64300/102467 406:03/241:01      1.958         {"mlm": 7.714901104869458, "mse": 0.0}  0.0813
0         train   64400/102467 406:39/240:22      1.967         {"mlm": 7.716225098125899, "mse": 0.0}  0.0777
0         train   64500/102467 407:16/239:44      1.975         {"mlm": 7.714643459243468, "mse": 0.0}  0.0736
0         train   64600/102467 407:52/239:05      1.984         {"mlm": 7.718701093093209, "mse": 0.0}  0.0714
0         train   64700/102467 408:29/238:26      1.993         {"mlm": 7.7190333820004176, "mse": 0.0}  0.0735
0         train   64800/102467 409:05/237:47      2.002         {"mlm": 7.7193216375241, "mse": 0.0}  0.0695
0         train   64900/102467 409:41/237:09      2.011         {"mlm": 7.721804679369343, "mse": 0.0}  0.1019
0         train   65000/102467 410:18/236:30      2.020         {"mlm": 7.7239588334230715, "mse": 0.0}  0.0664
0         train   65100/102467 410:54/235:51      2.029         {"mlm": 7.725956307517158, "mse": 0.0}  0.0919
0         train   65200/102467 411:30/235:12      2.037         {"mlm": 7.725652617882806, "mse": 0.0}  0.0888
0         train   65300/102467 412:07/234:34      2.046         {"mlm": 7.725080414068167, "mse": 0.0}  0.068
0         train   65400/102467 412:43/233:55      2.055         {"mlm": 7.7252628421237715, "mse": 0.0}  0.0685
0         train   65500/102467 413:19/233:16      2.063         {"mlm": 7.72275297918689, "mse": 0.0}  0.0806
0         train   65600/102467 413:56/232:37      2.072         {"mlm": 7.722419702663589, "mse": 0.0}  0.0688
0         train   65700/102467 414:32/231:59      2.080         {"mlm": 7.722753868226589, "mse": 0.0}  0.0672
0         train   65800/102467 415:08/231:20      2.089         {"mlm": 7.721317143541024, "mse": 0.0}  0.0714
0         train   65900/102467 415:45/230:41      2.098         {"mlm": 7.720195936076382, "mse": 0.0}  0.0611
0         train   66000/102467 416:21/230:03      2.106         {"mlm": 7.720183408773458, "mse": 0.0}  0.0728
0         train   66100/102467 416:57/229:24      2.115         {"mlm": 7.7233092907777765, "mse": 0.0}  0.0754
0         train   66200/102467 417:34/228:45      2.123         {"mlm": 7.714841641750432, "mse": 0.0}  0.089
0         train   66300/102467 418:10/228:07      2.131         {"mlm": 7.722905322758838, "mse": 0.0}  0.0783
0         train   66400/102467 418:46/227:28      2.140         {"mlm": 7.725463521270368, "mse": 0.0}  0.0757
0         train   66500/102467 419:23/226:49      2.148         {"mlm": 7.728178271825165, "mse": 0.0}  0.071
0         train   66600/102467 419:59/226:10      2.157         {"mlm": 7.728372054683104, "mse": 0.0}  0.0662
0         train   66700/102467 420:35/225:32      2.165         {"mlm": 7.730044350562513, "mse": 0.0}  0.0688
0         train   66800/102467 421:11/224:53      2.173         {"mlm": 7.731904873435141, "mse": 0.0}  0.0654
0         train   66900/102467 421:48/224:14      2.182         {"mlm": 7.734170819073085, "mse": 0.0}  0.063
0         train   67000/102467 422:24/223:36      2.190         {"mlm": 7.731136441589478, "mse": 0.0}  0.0626
0         train   67100/102467 423:00/222:57      2.198         {"mlm": 7.730840148334625, "mse": 0.0}  0.0718
0         train   67200/102467 423:37/222:19      2.206         {"mlm": 7.729338845911879, "mse": 0.0}  0.0679
0         train   67300/102467 424:13/221:40      2.215         {"mlm": 7.72917401505693, "mse": 0.0}  0.0611
0         train   67400/102467 424:49/221:01      2.223         {"mlm": 7.729827239160121, "mse": 0.0}  0.0672
0         train   67500/102467 425:26/220:23      2.231         {"mlm": 7.73153277086909, "mse": 0.0}  0.072
0         train   67600/102467 426:02/219:44      2.239         {"mlm": 7.732366850619474, "mse": 0.0}  0.0692
0         train   67700/102467 426:38/219:06      2.247         {"mlm": 7.730114811788816, "mse": 0.0}  0.0654
0         train   67800/102467 427:15/218:27      2.255         {"mlm": 7.728977140480768, "mse": 0.0}  0.0621
0         train   67900/102467 427:51/217:49      2.263         {"mlm": 7.727750225700576, "mse": 0.0}  0.0631
0         train   68000/102467 428:27/217:10      2.271         {"mlm": 7.727402645764377, "mse": 0.0}  0.0833
0         train   68100/102467 429:04/216:31      2.279         {"mlm": 7.744103853901227, "mse": 0.0}  0.0574
0         train   68200/102467 429:40/215:53      2.287         {"mlm": 7.73540457414121, "mse": 0.0}  0.0684
0         train   68300/102467 430:17/215:14      2.295         {"mlm": 7.735972296547246, "mse": 0.0}  0.0662
0         train   68400/102467 430:53/214:36      2.303         {"mlm": 7.730417376816875, "mse": 0.0}  0.0861
0         train   68500/102467 431:29/213:57      2.311         {"mlm": 7.726654272887014, "mse": 0.0}  0.0727
0         train   68600/102467 432:06/213:19      2.319         {"mlm": 7.71949286428874, "mse": 0.0}  0.0716
0         train   68700/102467 432:42/212:40      2.327         {"mlm": 7.72418048669552, "mse": 0.0}  0.0918
0         train   68800/102467 433:18/212:02      2.335         {"mlm": 7.7237186330047685, "mse": 0.0}  0.0734
0         train   68900/102467 433:54/211:23      2.343         {"mlm": 7.723140798509121, "mse": 0.0}  0.0671
0         train   69000/102467 434:31/210:45      2.350         {"mlm": 7.7215897706617795, "mse": 0.0}  0.0691
0         train   69100/102467 435:07/210:06      2.358         {"mlm": 7.722582554295116, "mse": 0.0}  0.058
0         train   69200/102467 435:43/209:28      2.366         {"mlm": 7.7228749974515525, "mse": 0.0}  0.0763
0         train   69300/102467 436:20/208:49      2.374         {"mlm": 7.721995977707851, "mse": 0.0}  0.0665
0         train   69400/102467 436:56/208:11      2.381         {"mlm": 7.721945199378924, "mse": 0.0}  0.0665
0         train   69500/102467 437:32/207:32      2.389         {"mlm": 7.721896257311265, "mse": 0.0}  0.078
0         train   69600/102467 438:08/206:54      2.397         {"mlm": 7.723821311367484, "mse": 0.0}  0.0716
0         train   69700/102467 438:45/206:15      2.404         {"mlm": 7.724881025136642, "mse": 0.0}  0.0773
0         train   69800/102467 439:21/205:37      2.412         {"mlm": 7.724132867857715, "mse": 0.0}  0.0692
0         train   69900/102467 439:57/204:58      2.420         {"mlm": 7.724542193774935, "mse": 0.0}  0.0633
0         train   70000/102467 440:33/204:20      2.427         {"mlm": 7.724660741781185, "mse": 0.0}  0.0615

09/24/2022 14:28:34 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step70000.pkl
0         valid   1/781        0:21/283:00        7.808           None
0         valid   101/781      0:37/ 4:09         7.707           None
0         valid   201/781      0:52/ 2:31         7.712           None
0         valid   301/781      1:07/ 1:48         7.713           None
0         valid   401/781      1:23/ 1:18         7.721           None
0         valid   501/781      1:38/ 0:54         7.721           None
0         valid   601/781      1:53/ 0:34         7.723           None
0         valid   701/781      2:09/ 0:14         7.724           None
0         valid   781/781      2:23/ 0:00         7.721         {"mlm": 7.721030729833546, "mse": 0.0, "train": 0.0}  None
0         train   70100/102467 443:34/204:48      2.435         {"mlm": 7.734846277236938, "mse": 0.0}  0.0669
0         train   70200/102467 444:10/204:09      2.442         {"mlm": 7.735200743675232, "mse": 0.0}  0.0619
0         train   70300/102467 444:46/203:30      2.450         {"mlm": 7.7342975123723345, "mse": 0.0}  0.073
0         train   70400/102467 445:22/202:52      2.457         {"mlm": 7.734613883495331, "mse": 0.0}  0.0719
0         train   70500/102467 445:59/202:13      2.465         {"mlm": 7.734315557479858, "mse": 0.0}  0.0667
0         train   70600/102467 446:35/201:34      2.472         {"mlm": 7.734570947488149, "mse": 0.0}  0.0591
0         train   70700/102467 447:11/200:55      2.480         {"mlm": 7.733769699505397, "mse": 0.0}  0.0723
0         train   70800/102467 447:47/200:17      2.487         {"mlm": 7.72962377011776, "mse": 0.0}  0.06
0         train   70900/102467 448:24/199:38      2.494         {"mlm": 7.726801430384318, "mse": 0.0}  0.0654
0         train   71000/102467 449:00/198:59      2.502         {"mlm": 7.725696469306945, "mse": 0.0}  0.0775
0         train   71100/102467 449:36/198:21      2.509         {"mlm": 7.723636184605685, "mse": 0.0}  0.0687
0         train   71200/102467 450:13/197:42      2.516         {"mlm": 7.724065678517023, "mse": 0.0}  0.0682
0         train   71300/102467 450:49/197:04      2.524         {"mlm": 7.723614576779879, "mse": 0.0}  0.0596
0         train   71400/102467 451:25/196:25      2.531         {"mlm": 7.725128357069833, "mse": 0.0}  0.0688
0         train   71500/102467 452:02/195:46      2.538         {"mlm": 7.725234890619913, "mse": 0.0}  0.0606
0         train   71600/102467 452:38/195:08      2.546         {"mlm": 7.725535074770451, "mse": 0.0}  0.0686
0         train   71700/102467 453:14/194:29      2.553         {"mlm": 7.724605531692505, "mse": 0.0}  0.0659
0         train   71800/102467 453:51/193:50      2.560         {"mlm": 7.7243591676818, "mse": 0.0}  0.0778
0         train   71900/102467 454:27/193:12      2.567         {"mlm": 7.724315133345755, "mse": 0.0}  0.0642
0         train   72000/102467 455:03/192:33      2.574         {"mlm": 7.723873568058014, "mse": 0.0}  0.0604
0         train   72100/102467 455:40/191:55      2.581         {"mlm": 7.729169012320162, "mse": 0.0}  0.0545
0         train   72200/102467 456:16/191:16      2.589         {"mlm": 7.729774307365992, "mse": 0.0}  0.0637
0         train   72300/102467 456:52/190:37      2.596         {"mlm": 7.729012122521033, "mse": 0.0}  0.0681
0         train   72400/102467 457:29/189:59      2.603         {"mlm": 7.729234426541436, "mse": 0.0}  0.0759
0         train   72500/102467 458:05/189:20      2.610         {"mlm": 7.725348554775567, "mse": 0.0}  0.058
0         train   72600/102467 458:41/188:42      2.617         {"mlm": 7.723639547924367, "mse": 0.0}  0.0661
0         train   72700/102467 459:17/188:03      2.624         {"mlm": 7.723517447241045, "mse": 0.0}  0.0635
0         train   72800/102467 459:54/187:25      2.631         {"mlm": 7.724230652905823, "mse": 0.0}  0.0553
0         train   72900/102467 460:30/186:46      2.638         {"mlm": 7.724724176595686, "mse": 0.0}  0.0609
0         train   73000/102467 461:06/186:07      2.645         {"mlm": 7.723225407891564, "mse": 0.0}  0.0718
0         train   73100/102467 461:43/185:29      2.652         {"mlm": 7.722917996285935, "mse": 0.0}  0.0564
0         train   73200/102467 462:19/184:50      2.659         {"mlm": 7.722464963135071, "mse": 0.0}  0.0749
0         train   73300/102467 462:56/184:12      2.666         {"mlm": 7.721896413108585, "mse": 0.0}  0.0521
0         train   73400/102467 463:32/183:33      2.673         {"mlm": 7.722830472118604, "mse": 0.0}  0.0608
0         train   73500/102467 464:08/182:55      2.679         {"mlm": 7.723182498812278, "mse": 0.0}  0.0571
0         train   73600/102467 464:45/182:17      2.686         {"mlm": 7.722031148394024, "mse": 0.0}  0.0664
0         train   73700/102467 465:21/181:38      2.693         {"mlm": 7.722257597857605, "mse": 0.0}  0.0606
0         train   73800/102467 465:58/181:00      2.700         {"mlm": 7.722675949815514, "mse": 0.0}  0.0669
0         train   73900/102467 466:34/180:21      2.707         {"mlm": 7.72316241603327, "mse": 0.0}  0.056
0         train   74000/102467 467:10/179:43      2.713         {"mlm": 7.721820342773316, "mse": 0.0}  0.0635
0         train   74100/102467 467:47/179:04      2.720         {"mlm": 7.7102779077023875, "mse": 0.0}  0.0543
0         train   74200/102467 468:23/178:26      2.727         {"mlm": 7.7171673630223125, "mse": 0.0}  0.0715
0         train   74300/102467 468:59/177:47      2.734         {"mlm": 7.724813326893237, "mse": 0.0}  0.0633
0         train   74400/102467 469:36/177:09      2.740         {"mlm": 7.720475097397464, "mse": 0.0}  0.0729
0         train   74500/102467 470:12/176:30      2.747         {"mlm": 7.721060308586641, "mse": 0.0}  0.0525
0         train   74600/102467 470:49/175:52      2.754         {"mlm": 7.722118368914295, "mse": 0.0}  0.0598
0         train   74700/102467 471:25/175:14      2.760         {"mlm": 7.720654134422455, "mse": 0.0}  0.0555
0         train   74800/102467 472:01/174:35      2.767         {"mlm": 7.72068086483126, "mse": 0.0}  0.0577
0         train   74900/102467 472:37/173:57      2.774         {"mlm": 7.7212825033871795, "mse": 0.0}  0.0595
0         train   75000/102467 473:14/173:18      2.780         {"mlm": 7.720874773476549, "mse": 0.0}  0.0638
0         train   75100/102467 473:50/172:40      2.787         {"mlm": 7.719420418278984, "mse": 0.0}  0.0574
0         train   75200/102467 474:26/172:01      2.793         {"mlm": 7.718367071501998, "mse": 0.0}  0.0544
0         train   75300/102467 475:03/171:23      2.800         {"mlm": 7.718417164357307, "mse": 0.0}  0.0529
0         train   75400/102467 475:39/170:45      2.806         {"mlm": 7.717963950657879, "mse": 0.0}  0.0644
0         train   75500/102467 476:15/170:06      2.813         {"mlm": 7.718511858992328, "mse": 0.0}  0.0786
0         train   75600/102467 476:52/169:28      2.819         {"mlm": 7.718291667286535, "mse": 0.0}  0.0719
0         train   75700/102467 477:28/168:49      2.826         {"mlm": 7.718772941820753, "mse": 0.0}  0.0602
0         train   75800/102467 478:04/168:11      2.832         {"mlm": 7.7187544198932585, "mse": 0.0}  0.0753
0         train   75900/102467 478:40/167:33      2.839         {"mlm": 7.719406806756875, "mse": 0.0}  0.0561
0         train   76000/102467 479:17/166:54      2.845         {"mlm": 7.719825780188835, "mse": 0.0}  0.0595
0         train   76100/102467 479:53/166:16      2.852         {"mlm": 7.72864570322725, "mse": 0.0}  0.0742
0         train   76200/102467 480:29/165:37      2.858         {"mlm": 7.712469013814394, "mse": 0.0}  0.4128
0         train   76300/102467 481:05/164:59      2.864         {"mlm": 7.713936442879314, "mse": 0.0}  39.7176
0         train   76400/102467 481:42/164:21      2.871         {"mlm": 7.717070118305966, "mse": 0.0}  0.0542
0         train   76500/102467 482:18/163:42      2.877         {"mlm": 7.7172702610852495, "mse": 0.0}  0.0628
0         train   76600/102467 482:54/163:04      2.883         {"mlm": 7.720144552202081, "mse": 0.0}  0.0545
0         train   76700/102467 483:31/162:26      2.890         {"mlm": 7.718710826835468, "mse": 0.0}  0.0557
0         train   76800/102467 484:07/161:47      2.896         {"mlm": 7.717762232724217, "mse": 0.0}  0.0563
0         train   76900/102467 484:43/161:09      2.902         {"mlm": 7.71780333099025, "mse": 0.0}  0.0592
0         train   77000/102467 485:20/160:31      2.908         {"mlm": 7.717438543811366, "mse": 0.0}  0.0584
0         train   77100/102467 485:56/159:52      2.915         {"mlm": 7.720210403557137, "mse": 0.0}  0.052
0         train   77200/102467 486:32/159:14      2.921         {"mlm": 7.721281201020817, "mse": 0.0}  0.0693
0         train   77300/102467 487:09/158:36      2.927         {"mlm": 7.723297166199342, "mse": 0.0}  0.0676
0         train   77400/102467 487:45/157:58      2.933         {"mlm": 7.722842739066996, "mse": 0.0}  0.0554
0         train   77500/102467 488:21/157:19      2.940         {"mlm": 7.7252672054644975, "mse": 0.0}  0.0535
0         train   77600/102467 488:58/156:41      2.946         {"mlm": 7.7258686386352045, "mse": 0.0}  0.057
0         train   77700/102467 489:34/156:03      2.952         {"mlm": 7.7251221629262465, "mse": 0.0}  0.066
0         train   77800/102467 490:10/155:24      2.958         {"mlm": 7.72465236490809, "mse": 0.0}  0.0535
0         train   77900/102467 490:47/154:46      2.964         {"mlm": 7.724273905354421, "mse": 0.0}  0.0626
0         train   78000/102467 491:23/154:08      2.970         {"mlm": 7.725036844588782, "mse": 0.0}  0.4178
0         train   78100/102467 491:59/153:30      2.976         {"mlm": 7.716900999347369, "mse": 0.0}  0.0576
0         train   78200/102467 492:36/152:51      2.982         {"mlm": 7.723894647189549, "mse": 0.0}  0.0692
0         train   78300/102467 493:12/152:13      2.989         {"mlm": 7.724780881727064, "mse": 0.0}  0.0618
0         train   78400/102467 493:48/151:35      2.995         {"mlm": 7.72065658641584, "mse": 0.0}  0.058
0         train   78500/102467 494:24/150:57      3.001         {"mlm": 7.723396676201975, "mse": 0.0}  0.0573
0         train   78600/102467 495:00/150:18      3.007         {"mlm": 7.723230768370148, "mse": 0.0}  0.0544
0         train   78700/102467 495:37/149:40      3.013         {"mlm": 7.7259387723330795, "mse": 0.0}  0.0726
0         train   78800/102467 496:13/149:02      3.019         {"mlm": 7.726914463330753, "mse": 0.0}  0.0576
0         train   78900/102467 496:49/148:23      3.025         {"mlm": 7.725550901144743, "mse": 0.0}  0.0527
0         train   79000/102467 497:25/147:45      3.030         {"mlm": 7.72130273863015, "mse": 0.0}  0.062
0         train   79100/102467 498:02/147:07      3.036         {"mlm": 7.723732732508305, "mse": 0.0}  0.0674
0         train   79200/102467 498:38/146:29      3.042         {"mlm": 7.724512615331439, "mse": 0.0}  0.0932
0         train   79300/102467 499:14/145:51      3.048         {"mlm": 7.723090255701983, "mse": 0.0}  0.0639
0         train   79400/102467 499:50/145:12      3.054         {"mlm": 7.723425202519983, "mse": 0.0}  0.0581
0         train   79500/102467 500:27/144:34      3.060         {"mlm": 7.72310292338305, "mse": 0.0}  0.054
0         train   79600/102467 501:03/143:56      3.066         {"mlm": 7.724145571988328, "mse": 0.0}  0.0554
0         train   79700/102467 501:39/143:18      3.072         {"mlm": 7.723783077777557, "mse": 0.0}  0.0585
0         train   79800/102467 502:16/142:40      3.078         {"mlm": 7.723481861413985, "mse": 0.0}  0.0609
0         train   79900/102467 502:52/142:01      3.083         {"mlm": 7.723859712292876, "mse": 0.0}  0.0535
0         train   80000/102467 503:29/141:23      3.089         {"mlm": 7.723662285862083, "mse": 0.0}  0.0529

09/24/2022 15:31:29 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step80000.pkl
0         valid   1/781        0:23/306:08        7.749           None
0         valid   101/781      0:38/ 4:21         7.707           None
0         valid   201/781      0:54/ 2:36         7.728           None
0         valid   301/781      1:09/ 1:50         7.726           None
0         valid   401/781      1:24/ 1:20         7.726           None
0         valid   501/781      1:40/ 0:56         7.722           None
0         valid   601/781      1:55/ 0:34         7.721           None
0         valid   701/781      2:10/ 0:14         7.719           None
0         valid   781/781      2:25/ 0:00         7.721         {"mlm": 7.72039052496799, "mse": 0.0, "train": 0.0}  None
0         train   80100/102467 506:30/141:26      3.095         {"mlm": 7.731011610031128, "mse": 0.0}  0.0589
0         train   80200/102467 507:06/140:47      3.101         {"mlm": 7.717841296195984, "mse": 0.0}  0.0681
0         train   80300/102467 507:42/140:09      3.106         {"mlm": 7.72150446732839, "mse": 0.0}  0.0575
0         train   80400/102467 508:18/139:30      3.112         {"mlm": 7.719249560832977, "mse": 0.0}  0.0533
0         train   80500/102467 508:54/138:52      3.118         {"mlm": 7.716880393981934, "mse": 0.0}  0.0566
0         train   80600/102467 509:30/138:13      3.124         {"mlm": 7.717242416540782, "mse": 0.0}  0.0536
0         train   80700/102467 510:07/137:35      3.129         {"mlm": 7.718173280443464, "mse": 0.0}  0.0539
0         train   80800/102467 510:43/136:57      3.135         {"mlm": 7.718382332921028, "mse": 0.0}  0.059
0         train   80900/102467 511:19/136:18      3.141         {"mlm": 7.718247633510166, "mse": 0.0}  0.0555
0         train   81000/102467 511:55/135:40      3.146         {"mlm": 7.717269000053406, "mse": 0.0}  0.0594
0         train   81100/102467 512:31/135:01      3.152         {"mlm": 7.719200456792658, "mse": 0.0}  0.0579
0         train   81200/102467 513:07/134:23      3.158         {"mlm": 7.720500967502594, "mse": 0.0}  0.0582
0         train   81300/102467 513:43/133:45      3.163         {"mlm": 7.7214125119722805, "mse": 0.0}  0.0592
0         train   81400/102467 514:19/133:06      3.169         {"mlm": 7.72176901783262, "mse": 0.0}  0.0549
0         train   81500/102467 514:56/132:28      3.174         {"mlm": 7.722847785631815, "mse": 0.0}  0.0667
0         train   81600/102467 515:32/131:50      3.180         {"mlm": 7.722746322154999, "mse": 0.0}  0.052
0         train   81700/102467 516:08/131:11      3.185         {"mlm": 7.720875457034391, "mse": 0.0}  0.0537
0         train   81800/102467 516:44/130:33      3.191         {"mlm": 7.720235015816159, "mse": 0.0}  0.0556
0         train   81900/102467 517:21/129:55      3.197         {"mlm": 7.721327517660041, "mse": 0.0}  0.0488
0         train   82000/102467 517:57/129:16      3.202         {"mlm": 7.7209592261314395, "mse": 0.0}  0.0551
0         train   82100/102467 518:33/128:38      3.208         {"mlm": 7.718069081354623, "mse": 0.0}  0.0532
0         train   82200/102467 519:10/128:00      3.213         {"mlm": 7.726364478394014, "mse": 0.0}  0.0513
0         train   82300/102467 519:46/127:22      3.219         {"mlm": 7.718574198592068, "mse": 0.0}  0.0499
0         train   82400/102467 520:22/126:43      3.224         {"mlm": 7.717782389848752, "mse": 0.0}  0.0537
0         train   82500/102467 520:59/126:05      3.229         {"mlm": 7.716285932040167, "mse": 0.0}  0.0569
0         train   82600/102467 521:35/125:27      3.235         {"mlm": 7.719272346050791, "mse": 0.0}  0.0667
0         train   82700/102467 522:12/124:49      3.240         {"mlm": 7.717593665116164, "mse": 0.0}  0.0527
0         train   82800/102467 522:48/124:10      3.246         {"mlm": 7.7155285955818185, "mse": 0.0}  0.0493
0         train   82900/102467 523:24/123:32      3.251         {"mlm": 7.716470107353304, "mse": 0.0}  0.0672
0         train   83000/102467 524:01/122:54      3.256         {"mlm": 7.715998691600841, "mse": 0.0}  0.0538
0         train   83100/102467 524:37/122:16      3.262         {"mlm": 7.716410326675679, "mse": 0.0}  0.0547
0         train   83200/102467 525:13/121:37      3.267         {"mlm": 7.717158258309257, "mse": 0.0}  0.0522
0         train   83300/102467 525:49/120:59      3.273         {"mlm": 7.716669430266535, "mse": 0.0}  0.0542
0         train   83400/102467 526:26/120:21      3.278         {"mlm": 7.716103865982721, "mse": 0.0}  0.0663
0         train   83500/102467 527:02/119:43      3.283         {"mlm": 7.718102968876325, "mse": 0.0}  0.0576
0         train   83600/102467 527:38/119:04      3.289         {"mlm": 7.71717258093132, "mse": 0.0}  0.0542
0         train   83700/102467 528:15/118:26      3.294         {"mlm": 7.717220111620152, "mse": 0.0}  0.0547
0         train   83800/102467 528:51/117:48      3.299         {"mlm": 7.716986526841254, "mse": 0.0}  0.0607
0         train   83900/102467 529:27/117:10      3.304         {"mlm": 7.717271770911194, "mse": 0.0}  0.0544
0         train   84000/102467 530:03/116:31      3.310         {"mlm": 7.7154692112654075, "mse": 0.0}  0.0529
0         train   84100/102467 530:40/115:53      3.315         {"mlm": 7.731223797311588, "mse": 0.0}  0.0557
0         train   84200/102467 531:16/115:15      3.320         {"mlm": 7.722267928749624, "mse": 0.0}  0.0515
0         train   84300/102467 531:52/114:37      3.325         {"mlm": 7.718322067452757, "mse": 0.0}  0.0533
0         train   84400/102467 532:28/113:59      3.330         {"mlm": 7.717456101173132, "mse": 0.0}  0.0508
0         train   84500/102467 533:04/113:20      3.336         {"mlm": 7.718986147378822, "mse": 0.0}  0.0574
0         train   84600/102467 533:41/112:42      3.341         {"mlm": 7.71841524675937, "mse": 0.0}  0.0497
0         train   84700/102467 534:17/112:04      3.346         {"mlm": 7.72183929131844, "mse": 0.0}  0.0523
0         train   84800/102467 534:53/111:26      3.351         {"mlm": 7.722550697493971, "mse": 0.0}  0.0556
0         train   84900/102467 535:29/110:48      3.356         {"mlm": 7.720440356396885, "mse": 0.0}  0.0504
0         train   85000/102467 536:05/110:09      3.361         {"mlm": 7.721012407887675, "mse": 0.0}  0.0521
0         train   85100/102467 536:42/109:31      3.367         {"mlm": 7.721056685421636, "mse": 0.0}  0.0514
0         train   85200/102467 537:18/108:53      3.372         {"mlm": 7.721376677785374, "mse": 0.0}  0.0541
0         train   85300/102467 537:54/108:15      3.377         {"mlm": 7.7215955404360965, "mse": 0.0}  0.059
0         train   85400/102467 538:30/107:37      3.382         {"mlm": 7.721246551546416, "mse": 0.0}  0.0571
0         train   85500/102467 539:07/106:59      3.387         {"mlm": 7.720762018208829, "mse": 0.0}  0.0604
0         train   85600/102467 539:43/106:20      3.392         {"mlm": 7.721157481583845, "mse": 0.0}  0.0516
0         train   85700/102467 540:19/105:42      3.397         {"mlm": 7.720455539802219, "mse": 0.0}  0.0745
0         train   85800/102467 540:56/105:04      3.402         {"mlm": 7.719594326645169, "mse": 0.0}  0.0567
0         train   85900/102467 541:32/104:26      3.407         {"mlm": 7.719627615523665, "mse": 0.0}  0.0503
0         train   86000/102467 542:08/103:48      3.412         {"mlm": 7.7216142429126515, "mse": 0.0}  0.0785
0         train   86100/102467 542:44/103:10      3.417         {"mlm": 7.737036297001789, "mse": 0.0}  0.0484
0         train   86200/102467 543:21/102:32      3.422         {"mlm": 7.71976776413506, "mse": 0.0}  0.0582
0         train   86300/102467 543:57/101:54      3.427         {"mlm": 7.7258095548610495, "mse": 0.0}  0.0476
0         train   86400/102467 544:33/101:16      3.432         {"mlm": 7.71854481108543, "mse": 0.0}  0.0521
0         train   86500/102467 545:10/100:37      3.437         {"mlm": 7.718398163256271, "mse": 0.0}  0.0514
0         train   86600/102467 545:46/99:59       3.442         {"mlm": 7.717915053343653, "mse": 0.0}  0.0439
0         train   86700/102467 546:22/99:21       3.447         {"mlm": 7.715900132440597, "mse": 0.0}  0.0538
0         train   86800/102467 546:58/98:43       3.452         {"mlm": 7.717637424636515, "mse": 0.0}  0.0542
0         train   86900/102467 547:35/98:05       3.457         {"mlm": 7.717553085573806, "mse": 0.0}  0.0547
0         train   87000/102467 548:11/97:27       3.462         {"mlm": 7.717782222399621, "mse": 0.0}  0.0499
0         train   87100/102467 548:47/96:49       3.467         {"mlm": 7.72095224307468, "mse": 0.0}  0.0473
0         train   87200/102467 549:23/96:11       3.471         {"mlm": 7.721164788618223, "mse": 0.0}  0.0481
0         train   87300/102467 550:00/95:33       3.476         {"mlm": 7.721536943704786, "mse": 0.0}  0.0516
0         train   87400/102467 550:36/94:55       3.481         {"mlm": 7.721458277705745, "mse": 0.0}  0.0443
0         train   87500/102467 551:12/94:17       3.486         {"mlm": 7.7210894863686725, "mse": 0.0}  0.0533
0         train   87600/102467 551:48/93:39       3.491         {"mlm": 7.720509142150115, "mse": 0.0}  0.0502
0         train   87700/102467 552:24/93:00       3.496         {"mlm": 7.720249952677073, "mse": 0.0}  0.05
0         train   87800/102467 553:01/92:22       3.500         {"mlm": 7.720407245287845, "mse": 0.0}  0.0435
0         train   87900/102467 553:37/91:44       3.505         {"mlm": 7.721293086683869, "mse": 0.0}  0.0527
0         train   88000/102467 554:13/91:06       3.510         {"mlm": 7.720668192917428, "mse": 0.0}  0.0574
0         train   88100/102467 554:49/90:28       3.515         {"mlm": 7.713103224833806, "mse": 0.0}  0.0463
0         train   88200/102467 555:25/89:50       3.520         {"mlm": 7.718096137046814, "mse": 0.0}  0.0535
0         train   88300/102467 556:01/89:12       3.524         {"mlm": 7.717727131134755, "mse": 0.0}  0.0485
0         train   88400/102467 556:38/88:34       3.529         {"mlm": 7.714826239479913, "mse": 0.0}  0.0512
0         train   88500/102467 557:14/87:56       3.534         {"mlm": 7.713397784579184, "mse": 0.0}  0.0496
0         train   88600/102467 557:50/87:18       3.539         {"mlm": 7.713481160618315, "mse": 0.0}  0.0592
0         train   88700/102467 558:26/86:40       3.543         {"mlm": 7.716448957207559, "mse": 0.0}  0.0454
0         train   88800/102467 559:03/86:02       3.548         {"mlm": 7.717503063642799, "mse": 0.0}  0.0828
0         train   88900/102467 559:39/85:24       3.553         {"mlm": 7.716209984783616, "mse": 0.0}  0.0584
0         train   89000/102467 560:15/84:46       3.557         {"mlm": 7.715776109312433, "mse": 0.0}  0.0516
0         train   89100/102467 560:51/84:08       3.562         {"mlm": 7.715527512731343, "mse": 0.0}  0.0456
0         train   89200/102467 561:28/83:30       3.567         {"mlm": 7.7150930158270645, "mse": 0.0}  0.0565
0         train   89300/102467 562:04/82:52       3.571         {"mlm": 7.716741296980116, "mse": 0.0}  0.0483
0         train   89400/102467 562:40/82:14       3.576         {"mlm": 7.71756595970908, "mse": 0.0}  0.0497
0         train   89500/102467 563:16/81:36       3.581         {"mlm": 7.719260198547241, "mse": 0.0}  0.0462
0         train   89600/102467 563:53/80:58       3.585         {"mlm": 7.719261957888017, "mse": 0.0}  0.0462
0         train   89700/102467 564:29/80:20       3.590         {"mlm": 7.718034883152764, "mse": 0.0}  0.0556
0         train   89800/102467 565:05/79:42       3.594         {"mlm": 7.718239629454496, "mse": 0.0}  0.074
0         train   89900/102467 565:42/79:04       3.599         {"mlm": 7.716720593126515, "mse": 0.0}  0.067
0         train   90000/102467 566:18/78:26       3.604         {"mlm": 7.717092066346285, "mse": 0.0}  0.0449

09/24/2022 16:34:18 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step90000.pkl
0         valid   1/781        0:23/307:46        7.774           None
0         valid   101/781      0:39/ 4:22         7.721           None
0         valid   201/781      0:54/ 2:36         7.724           None
0         valid   301/781      1:09/ 1:51         7.726           None
0         valid   401/781      1:24/ 1:20         7.732           None
0         valid   501/781      1:40/ 0:56         7.729           None
0         valid   601/781      1:55/ 0:34         7.731           None
0         valid   701/781      2:11/ 0:14         7.733           None
0         valid   781/781      2:25/ 0:00         7.722         {"mlm": 7.7216709346991035, "mse": 0.0, "train": 0.0}  None
0         train   90100/102467 569:20/78:08       3.608         {"mlm": 7.72019859790802, "mse": 0.0}  0.0483
0         train   90200/102467 569:56/77:30       3.613         {"mlm": 7.723467111587524, "mse": 0.0}  0.0527
0         train   90300/102467 570:31/76:52       3.617         {"mlm": 7.721264581680298, "mse": 0.0}  0.0508
0         train   90400/102467 571:07/76:14       3.622         {"mlm": 7.7240964281558995, "mse": 0.0}  0.0465
0         train   90500/102467 571:44/75:36       3.626         {"mlm": 7.724507727622986, "mse": 0.0}  0.0466
0         train   90600/102467 572:20/74:57       3.631         {"mlm": 7.718225960731506, "mse": 0.0}  0.0503
0         train   90700/102467 572:56/74:19       3.635         {"mlm": 7.715199112210955, "mse": 0.0}  0.0479
0         train   90800/102467 573:32/73:41       3.640         {"mlm": 7.718736339211464, "mse": 0.0}  0.0492
0         train   90900/102467 574:08/73:03       3.644         {"mlm": 7.717140766249763, "mse": 0.0}  0.0502
0         train   91000/102467 574:44/72:25       3.649         {"mlm": 7.718532030582428, "mse": 0.0}  0.0488
0         train   91100/102467 575:20/71:47       3.653         {"mlm": 7.717358497272838, "mse": 0.0}  1.0759
0         train   91200/102467 575:56/71:09       3.658         {"mlm": 7.716585893630981, "mse": 0.0}  0.0484
0         train   91300/102467 576:33/70:31       3.662         {"mlm": 7.716001582512488, "mse": 0.0}  0.0617
0         train   91400/102467 577:09/69:53       3.667         {"mlm": 7.716144854341234, "mse": 0.0}  0.4392
0         train   91500/102467 577:45/69:14       3.671         {"mlm": 7.717101408322653, "mse": 0.0}  0.0507
0         train   91600/102467 578:21/68:36       3.675         {"mlm": 7.7171515259146695, "mse": 0.0}  0.0621
0         train   91700/102467 578:58/67:58       3.680         {"mlm": 7.715527884258943, "mse": 0.0}  0.0465
0         train   91800/102467 579:34/67:20       3.684         {"mlm": 7.71724200963974, "mse": 0.0}  0.0451
0         train   91900/102467 580:10/66:42       3.689         {"mlm": 7.718440688032853, "mse": 0.0}  0.0436
0         train   92000/102467 580:46/66:04       3.693         {"mlm": 7.718268710136414, "mse": 0.0}  0.0549
0         train   92100/102467 581:23/65:26       3.697         {"mlm": 7.713594296965936, "mse": 0.0}  0.0543
0         train   92200/102467 581:59/64:48       3.702         {"mlm": 7.710119968682677, "mse": 0.0}  0.0465
0         train   92300/102467 582:35/64:10       3.706         {"mlm": 7.715113960380937, "mse": 0.0}  0.0449
0         train   92400/102467 583:12/63:32       3.710         {"mlm": 7.719150928028843, "mse": 0.0}  0.0447
0         train   92500/102467 583:48/62:54       3.715         {"mlm": 7.724710164423696, "mse": 0.0}  0.0547
0         train   92600/102467 584:24/62:16       3.719         {"mlm": 7.72037558324747, "mse": 0.0}  0.0466
0         train   92700/102467 585:01/61:38       3.723         {"mlm": 7.719317693396529, "mse": 0.0}  0.0595
0         train   92800/102467 585:37/61:00       3.728         {"mlm": 7.716886189165939, "mse": 0.0}  0.0502
0         train   92900/102467 586:13/60:22       3.732         {"mlm": 7.716833777634533, "mse": 0.0}  0.0587
0         train   93000/102467 586:50/59:44       3.736         {"mlm": 7.718974913443412, "mse": 0.0}  0.0455
0         train   93100/102467 587:26/59:06       3.741         {"mlm": 7.721052036597796, "mse": 0.0}  0.0569
0         train   93200/102467 588:02/58:28       3.745         {"mlm": 7.719628684415332, "mse": 0.0}  0.0529
0         train   93300/102467 588:39/57:50       3.749         {"mlm": 7.719302740530201, "mse": 0.0}  0.053
0         train   93400/102467 589:15/57:12       3.753         {"mlm": 7.718594723211347, "mse": 0.0}  0.0468
0         train   93500/102467 589:51/56:34       3.758         {"mlm": 7.717785214328066, "mse": 0.0}  0.0518
0         train   93600/102467 590:27/55:56       3.762         {"mlm": 7.718876595046835, "mse": 0.0}  0.0509
0         train   93700/102467 591:03/55:18       3.766         {"mlm": 7.7186851004701, "mse": 0.0}  0.048
0         train   93800/102467 591:40/54:40       3.770         {"mlm": 7.719227580112374, "mse": 0.0}  0.0539
0         train   93900/102467 592:16/54:02       3.775         {"mlm": 7.72015981488381, "mse": 0.0}  0.0411
0         train   94000/102467 592:52/53:24       3.779         {"mlm": 7.720324523929598, "mse": 0.0}  0.0434
0         train   94100/102467 593:28/52:46       3.783         {"mlm": 7.717029162815639, "mse": 0.0}  0.05
0         train   94200/102467 594:04/52:08       3.787         {"mlm": 7.7169872462147415, "mse": 0.0}  0.0514
0         train   94300/102467 594:41/51:30       3.791         {"mlm": 7.718237830488474, "mse": 0.0}  0.044
0         train   94400/102467 595:17/50:52       3.795         {"mlm": 7.721227412247778, "mse": 0.0}  0.0475
0         train   94500/102467 595:53/50:14       3.800         {"mlm": 7.7192994418393175, "mse": 0.0}  0.0553
0         train   94600/102467 596:29/49:36       3.804         {"mlm": 7.717420818813668, "mse": 0.0}  0.0479
0         train   94700/102467 597:05/48:58       3.808         {"mlm": 7.716309677905545, "mse": 0.0}  0.0418
0         train   94800/102467 597:42/48:20       3.812         {"mlm": 7.716320757877857, "mse": 0.0}  0.0514
0         train   94900/102467 598:18/47:42       3.816         {"mlm": 7.717512376058871, "mse": 0.0}  0.0434
0         train   95000/102467 598:54/47:04       3.820         {"mlm": 7.7166944156906645, "mse": 0.0}  0.0529
0         train   95100/102467 599:31/46:26       3.824         {"mlm": 7.713133068032604, "mse": 0.0}  0.0474
0         train   95200/102467 600:07/45:48       3.828         {"mlm": 7.711470900075463, "mse": 0.0}  0.0494
0         train   95300/102467 600:43/45:10       3.832         {"mlm": 7.712383025967285, "mse": 0.0}  0.0508
0         train   95400/102467 601:20/44:32       3.836         {"mlm": 7.712898233588332, "mse": 0.0}  0.0418
0         train   95500/102467 601:56/43:54       3.841         {"mlm": 7.7133243879743505, "mse": 0.0}  0.042
0         train   95600/102467 602:32/43:16       3.845         {"mlm": 7.713610494539645, "mse": 0.0}  0.0551
0         train   95700/102467 603:08/42:38       3.849         {"mlm": 7.71245104601021, "mse": 0.0}  0.0462
0         train   95800/102467 603:45/42:01       3.853         {"mlm": 7.713712554354556, "mse": 0.0}  0.0596
0         train   95900/102467 604:21/41:23       3.857         {"mlm": 7.71580368650977, "mse": 0.0}  0.0481
0         train   96000/102467 604:57/40:45       3.861         {"mlm": 7.716152833866047, "mse": 0.0}  0.0448
0         train   96100/102467 605:34/40:07       3.865         {"mlm": 7.723496992563464, "mse": 0.0}  0.0514
0         train   96200/102467 606:10/39:29       3.869         {"mlm": 7.711465160253689, "mse": 0.0}  0.0511
0         train   96300/102467 606:46/38:51       3.873         {"mlm": 7.716747556872641, "mse": 0.0}  0.0531
0         train   96400/102467 607:23/38:13       3.877         {"mlm": 7.717431476794802, "mse": 0.0}  0.0429
0         train   96500/102467 607:59/37:35       3.881         {"mlm": 7.714942418833374, "mse": 0.0}  0.0486
0         train   96600/102467 608:35/36:57       3.885         {"mlm": 7.712002456288042, "mse": 0.0}  0.0577
0         train   96700/102467 609:11/36:19       3.889         {"mlm": 7.708693321671342, "mse": 0.0}  0.0492
0         train   96800/102467 609:47/35:41       3.893         {"mlm": 7.708003023786556, "mse": 0.0}  0.0509
0         train   96900/102467 610:23/35:04       3.896         {"mlm": 7.708251906344989, "mse": 0.0}  0.0416
0         train   97000/102467 611:00/34:26       3.900         {"mlm": 7.709716568739268, "mse": 0.0}  0.0463
0         train   97100/102467 611:36/33:48       3.904         {"mlm": 7.708262721733712, "mse": 0.0}  0.0493
0         train   97200/102467 612:12/33:10       3.908         {"mlm": 7.710365806904652, "mse": 0.0}  0.0496
0         train   97300/102467 612:48/32:32       3.912         {"mlm": 7.709602105222303, "mse": 0.0}  0.0531
0         train   97400/102467 613:24/31:54       3.916         {"mlm": 7.709521238003446, "mse": 0.0}  0.0576
0         train   97500/102467 614:00/31:16       3.920         {"mlm": 7.709398730564054, "mse": 0.0}  0.0453
0         train   97600/102467 614:37/30:38       3.924         {"mlm": 7.7092852302842685, "mse": 0.0}  0.0467
0         train   97700/102467 615:13/30:01       3.928         {"mlm": 7.709930228287849, "mse": 0.0}  0.0518
0         train   97800/102467 615:49/29:23       3.932         {"mlm": 7.710922348147707, "mse": 0.0}  0.0467
0         train   97900/102467 616:25/28:45       3.935         {"mlm": 7.711717128753662, "mse": 0.0}  0.0452
0         train   98000/102467 617:02/28:07       3.939         {"mlm": 7.712890012537651, "mse": 0.0}  0.045
0         train   98100/102467 617:38/27:29       3.943         {"mlm": 7.72247909506162, "mse": 0.0}  0.0464
0         train   98200/102467 618:14/26:51       3.947         {"mlm": 7.722188667375214, "mse": 0.0}  0.0461
0         train   98300/102467 618:50/26:14       3.951         {"mlm": 7.724592891899315, "mse": 0.0}  0.0483
0         train   98400/102467 619:27/25:36       3.955         {"mlm": 7.720791229093917, "mse": 0.0}  0.0531
0         train   98500/102467 620:03/24:58       3.959         {"mlm": 7.717802477459753, "mse": 0.0}  0.0454
0         train   98600/102467 620:39/24:20       3.962         {"mlm": 7.715220094527174, "mse": 0.0}  0.0433
0         train   98700/102467 621:16/23:42       3.966         {"mlm": 7.71614143834717, "mse": 0.0}  0.0468
0         train   98800/102467 621:52/23:04       3.970         {"mlm": 7.7152287474828745, "mse": 0.0}  0.0523
0         train   98900/102467 622:28/22:27       3.974         {"mlm": 7.715521282383373, "mse": 0.0}  0.0421
0         train   99000/102467 623:04/21:49       3.978         {"mlm": 7.7162084095928085, "mse": 0.0}  0.0582
0         train   99100/102467 623:41/21:11       3.981         {"mlm": 7.716764883403361, "mse": 0.0}  0.0515
0         train   99200/102467 624:17/20:33       3.985         {"mlm": 7.716211578917743, "mse": 0.0}  0.0435
0         train   99300/102467 624:53/19:55       3.989         {"mlm": 7.71639805001977, "mse": 0.0}  0.0504
0         train   99400/102467 625:30/19:17       3.993         {"mlm": 7.714836213513569, "mse": 0.0}  0.0458
0         train   99500/102467 626:06/18:40       3.996         {"mlm": 7.7151227612546425, "mse": 0.0}  0.0424
0         train   99600/102467 626:45/18:02       4.000         {"mlm": 7.715307024487278, "mse": 0.0}  0.0494
0         train   99700/102467 627:21/17:24       4.004         {"mlm": 7.715378416034411, "mse": 0.0}  0.0427
0         train   99800/102467 627:57/16:46       4.007         {"mlm": 7.716412783731066, "mse": 0.0}  0.0464
0         train   99900/102467 628:34/16:09       4.011         {"mlm": 7.717456806309616, "mse": 0.0}  0.0452
0         train   100000/102467 629:10/15:31      4.015         {"mlm": 7.718256482619322, "mse": 0.0}  0.045

09/24/2022 17:37:10 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step100000.pkl
0         valid   1/781        0:21/285:00        7.641           None
0         valid   101/781      0:37/ 4:10         7.702           None
0         valid   201/781      0:52/ 2:31         7.712           None
0         valid   301/781      1:07/ 1:48         7.715           None
0         valid   401/781      1:23/ 1:18         7.718           None
0         valid   501/781      1:38/ 0:54         7.718           None
0         valid   601/781      1:53/ 0:34         7.721           None
0         valid   701/781      2:08/ 0:14         7.720           None
0         valid   781/781      2:23/ 0:00         7.721         {"mlm": 7.721190781049936, "mse": 0.0, "train": 0.0}  None
0         train   100100/102467 632:09/14:56      4.019         {"mlm": 7.6983336210250854, "mse": 0.0}  0.0446
0         train   100200/102467 632:45/14:18      4.022         {"mlm": 7.715162839889526, "mse": 0.0}  0.0434
0         train   100300/102467 633:21/13:41      4.026         {"mlm": 7.7072204367319745, "mse": 0.0}  0.0452
0         train   100400/102467 633:57/13:03      4.030         {"mlm": 7.708691598176956, "mse": 0.0}  0.046
0         train   100500/102467 634:33/12:25      4.033         {"mlm": 7.716034602165222, "mse": 0.0}  0.0525
0         train   100600/102467 635:09/11:47      4.037         {"mlm": 7.718629732926686, "mse": 0.0}  0.0508
0         train   100700/102467 635:45/11:09      4.041         {"mlm": 7.715283772604806, "mse": 0.0}  0.0482
0         train   100800/102467 636:21/10:31      4.044         {"mlm": 7.7164716750383375, "mse": 0.0}  0.0465
0         train   100900/102467 636:58/ 9:53      4.048         {"mlm": 7.717998721864489, "mse": 0.0}  0.043
0         train   101000/102467 637:34/ 9:15      4.052         {"mlm": 7.721122149467468, "mse": 0.0}  0.0417
0         train   101100/102467 638:10/ 8:37      4.055         {"mlm": 7.719561420353976, "mse": 0.0}  0.0429
0         train   101200/102467 638:46/ 7:59      4.059         {"mlm": 7.720416148106257, "mse": 0.0}  0.0486
0         train   101300/102467 639:22/ 7:21      4.062         {"mlm": 7.720659862298231, "mse": 0.0}  0.0548
0         train   101400/102467 639:59/ 6:44      4.066         {"mlm": 7.719930498940604, "mse": 0.0}  0.0443
0         train   101500/102467 640:35/ 6:06      4.070         {"mlm": 7.719465732574463, "mse": 0.0}  0.0475
0         train   101600/102467 641:11/ 5:28      4.073         {"mlm": 7.7200673857331275, "mse": 0.0}  0.0483
0         train   101700/102467 641:48/ 4:50      4.077         {"mlm": 7.72001322073095, "mse": 0.0}  0.0453
0         train   101800/102467 642:24/ 4:12      4.080         {"mlm": 7.720369723637899, "mse": 0.0}  0.0443
0         train   101900/102467 643:00/ 3:34      4.084         {"mlm": 7.72097073529896, "mse": 0.0}  0.0446
0         train   102000/102467 643:37/ 2:56      4.088         {"mlm": 7.721249491214752, "mse": 0.0}  0.0599
0         train   102100/102467 644:13/ 2:18      4.091         {"mlm": 7.740577355779783, "mse": 0.0}  0.0407
0         train   102200/102467 644:49/ 1:41      4.095         {"mlm": 7.727481377184691, "mse": 0.0}  0.0459
0         train   102300/102467 645:26/ 1:03      4.098         {"mlm": 7.726917223786829, "mse": 0.0}  0.0416
0         train   102400/102467 646:02/ 0:25      4.102         {"mlm": 7.72719725391321, "mse": 0.0}  0.0517
True False
skip validation
1         train   100/102467   0:53/918:37        7.716         {"mlm": 7.716244373321533, "mse": 0.0}  0.0554
1         train   200/102467   1:29/766:08        7.723         {"mlm": 7.72298024892807, "mse": 0.0}  0.0444
1         train   300/102467   2:06/715:29        7.726         {"mlm": 7.725698118209839, "mse": 0.0}  0.0435
1         train   400/102467   2:42/689:29        7.723         {"mlm": 7.722775942087173, "mse": 0.0}  0.0578
1         train   500/102467   3:18/673:38        7.720         {"mlm": 7.720488951683045, "mse": 0.0}  0.0407
1         train   600/102467   3:54/663:03        7.715         {"mlm": 7.71499392747879, "mse": 0.0}  0.0411
1         train   700/102467   4:30/655:18        7.716         {"mlm": 7.715519680976867, "mse": 0.0}  0.0451
1         train   800/102467   5:06/649:23        7.716         {"mlm": 7.715834721326828, "mse": 0.0}  0.0534
1         train   900/102467   5:42/644:45        7.715         {"mlm": 7.7148204019334585, "mse": 0.0}  0.0433
1         train   1000/102467  6:18/640:50        7.715         {"mlm": 7.715027089118958, "mse": 0.0}  0.0428
1         train   1100/102467  6:55/637:27        7.717         {"mlm": 7.716621136231856, "mse": 0.0}  0.0439
1         train   1200/102467  7:31/634:34        7.719         {"mlm": 7.718501558303833, "mse": 0.0}  0.0427
1         train   1300/102467  8:07/632:03        7.719         {"mlm": 7.719024651967562, "mse": 0.0}  0.0504
1         train   1400/102467  8:43/629:49        7.719         {"mlm": 7.7194420259339465, "mse": 0.0}  0.0524
1         train   1500/102467  9:19/627:52        7.718         {"mlm": 7.71769927183787, "mse": 0.0}  0.057
1         train   1600/102467  9:55/625:57        7.719         {"mlm": 7.7191079559922215, "mse": 0.0}  0.0414
1         train   1700/102467 10:31/624:17        7.720         {"mlm": 7.720133986753576, "mse": 0.0}  0.0423
1         train   1800/102467 11:08/622:40        7.720         {"mlm": 7.720363657209608, "mse": 0.0}  0.05
1         train   1900/102467 11:44/621:12        7.721         {"mlm": 7.721306812888698, "mse": 0.0}  0.0472
1         train   2000/102467 12:20/619:54        7.722         {"mlm": 7.722200978994369, "mse": 0.0}  0.0516
1         train   2100/102467 12:56/618:41        7.722         {"mlm": 7.726822154690521, "mse": 0.0}  0.0437
1         train   2200/102467 13:32/617:30        7.721         {"mlm": 7.713186537201081, "mse": 0.0}  0.0408
1         train   2300/102467 14:09/616:25        7.722         {"mlm": 7.720349973659451, "mse": 0.0}  0.0489
1         train   2400/102467 14:45/615:26        7.722         {"mlm": 7.72067740208523, "mse": 0.0}  0.0452
1         train   2500/102467 15:21/614:22        7.721         {"mlm": 7.717738559585296, "mse": 0.0}  0.0434
1         train   2600/102467 15:59/614:13        7.722         {"mlm": 7.719776725928254, "mse": 0.0}  0.0464
1         train   2700/102467 16:35/613:12        7.721         {"mlm": 7.716237855400991, "mse": 0.0}  0.0458
1         train   2800/102467 17:12/612:21        7.720         {"mlm": 7.71586446678534, "mse": 0.0}  0.0437
1         train   2900/102467 17:48/611:22        7.719         {"mlm": 7.712222234558343, "mse": 0.0}  0.041
1         train   3000/102467 18:24/610:30        7.718         {"mlm": 7.710728507380824, "mse": 0.0}  0.0448
1         train   3100/102467 19:01/609:33        7.718         {"mlm": 7.7110709866358, "mse": 0.0}  0.0485
1         train   3200/102467 19:37/608:38        7.719         {"mlm": 7.712552417806032, "mse": 0.0}  0.0486
1         train   3300/102467 20:13/607:47        7.719         {"mlm": 7.712942749284799, "mse": 0.0}  0.0457
1         train   3400/102467 20:49/606:58        7.718         {"mlm": 7.711874498649526, "mse": 0.0}  0.0449
1         train   3500/102467 21:26/606:10        7.718         {"mlm": 7.712113835638249, "mse": 0.0}  0.0403
1         train   3600/102467 22:02/605:19        7.717         {"mlm": 7.711518714993055, "mse": 0.0}  0.0487
1         train   3700/102467 22:38/604:31        7.717         {"mlm": 7.711688865416888, "mse": 0.0}  0.0486
1         train   3800/102467 23:15/603:44        7.717         {"mlm": 7.712217088405658, "mse": 0.0}  0.0527
1         train   3900/102467 23:51/602:58        7.717         {"mlm": 7.711160537253686, "mse": 0.0}  0.0423
1         train   4000/102467 24:27/602:10        7.717         {"mlm": 7.7114444728372336, "mse": 0.0}  0.0427
1         train   4100/102467 25:03/601:22        7.716         {"mlm": 7.700969554940048, "mse": 0.0}  0.0405
1         train   4200/102467 25:40/600:34        7.716         {"mlm": 7.7098257878814085, "mse": 0.0}  0.0612
1         train   4300/102467 26:16/599:48        7.716         {"mlm": 7.702464335716811, "mse": 0.0}  0.0513
1         train   4400/102467 26:52/599:00        7.716         {"mlm": 7.707140692514391, "mse": 0.0}  0.0454
1         train   4500/102467 27:28/598:14        7.716         {"mlm": 7.712655201494455, "mse": 0.0}  0.0436
1         train   4600/102467 28:05/597:29        7.717         {"mlm": 7.7166169032604, "mse": 0.0}  0.044
1         train   4700/102467 28:41/596:45        7.717         {"mlm": 7.72004256890633, "mse": 0.0}  0.0402
1         train   4800/102467 29:17/596:00        7.717         {"mlm": 7.717058754206302, "mse": 0.0}  0.046
1         train   4900/102467 29:53/595:13        7.717         {"mlm": 7.717151862741313, "mse": 0.0}  0.0523
1         train   5000/102467 30:29/594:28        7.717         {"mlm": 7.71891844439841, "mse": 0.0}  0.043
1         train   5100/102467 31:05/593:44        7.717         {"mlm": 7.719300602303177, "mse": 0.0}  0.0434
1         train   5200/102467 31:42/593:00        7.718         {"mlm": 7.720066375445842, "mse": 0.0}  0.0461
1         train   5300/102467 32:18/592:16        7.718         {"mlm": 7.72083864263467, "mse": 0.0}  0.0414
1         train   5400/102467 32:54/591:34        7.718         {"mlm": 7.720076396912123, "mse": 0.0}  0.0501
1         train   5500/102467 33:30/590:52        7.718         {"mlm": 7.7212488899562, "mse": 0.0}  0.0436
1         train   5600/102467 34:07/590:09        7.719         {"mlm": 7.722876950110004, "mse": 0.0}  0.0416
1         train   5700/102467 34:43/589:27        7.719         {"mlm": 7.724030516874945, "mse": 0.0}  0.0458
1         train   5800/102467 35:19/588:46        7.719         {"mlm": 7.7236908418318055, "mse": 0.0}  0.0442
1         train   5900/102467 35:55/588:04        7.719         {"mlm": 7.723489099106623, "mse": 0.0}  0.043
1         train   6000/102467 36:32/587:23        7.719         {"mlm": 7.7247333832092595, "mse": 0.0}  0.0424
1         train   6100/102467 37:08/586:41        7.720         {"mlm": 7.737766118393731, "mse": 0.0}  0.0569
1         train   6200/102467 37:44/586:00        7.720         {"mlm": 7.725340516434103, "mse": 0.0}  0.046
1         train   6300/102467 38:20/585:20        7.719         {"mlm": 7.717862697562786, "mse": 0.0}  0.0442
1         train   6400/102467 38:57/584:40        7.719         {"mlm": 7.719298222203099, "mse": 0.0}  0.0465
1         train   6500/102467 39:33/583:59        7.719         {"mlm": 7.7194751973603095, "mse": 0.0}  0.0474
1         train   6600/102467 40:09/583:20        7.719         {"mlm": 7.720367923653505, "mse": 0.0}  0.0473
1         train   6700/102467 40:45/582:39        7.720         {"mlm": 7.723988400983331, "mse": 0.0}  0.0428
1         train   6800/102467 41:22/581:58        7.720         {"mlm": 7.720557533513049, "mse": 0.0}  0.0425
1         train   6900/102467 41:58/581:20        7.719         {"mlm": 7.719027755252494, "mse": 0.0}  0.0398
1         train   7000/102467 42:34/580:39        7.720         {"mlm": 7.7212171970662045, "mse": 0.0}  0.0406
1         train   7100/102467 43:10/579:59        7.720         {"mlm": 7.72034343938558, "mse": 0.0}  0.0472
1         train   7200/102467 43:47/579:20        7.719         {"mlm": 7.718083139051471, "mse": 0.0}  0.0509
1         train   7300/102467 44:23/578:42        7.719         {"mlm": 7.717221201247404, "mse": 0.0}  0.0452
1         train   7400/102467 44:59/578:03        7.719         {"mlm": 7.716627239412978, "mse": 0.0}  0.0478
1         train   7500/102467 45:36/577:24        7.720         {"mlm": 7.720046496343517, "mse": 0.0}  0.0363
1         train   7600/102467 46:12/576:44        7.720         {"mlm": 7.719912114558402, "mse": 0.0}  0.0635
1         train   7700/102467 46:48/576:05        7.719         {"mlm": 7.719058364717554, "mse": 0.0}  0.0426
1         train   7800/102467 47:24/575:25        7.719         {"mlm": 7.718386127873136, "mse": 0.0}  0.0467
1         train   7900/102467 48:01/574:47        7.719         {"mlm": 7.718187205968685, "mse": 0.0}  0.0433
1         train   8000/102467 48:37/574:08        7.719         {"mlm": 7.718427651156529, "mse": 0.0}  0.0589
1         train   8100/102467 49:13/573:29        7.719         {"mlm": 7.7284650057554245, "mse": 0.0}  0.0472
1         train   8200/102467 49:49/572:50        7.719         {"mlm": 7.72314052922385, "mse": 0.0}  0.0388
1         train   8300/102467 50:25/572:09        7.719         {"mlm": 7.719481305496113, "mse": 0.0}  0.0384
1         train   8400/102467 51:02/571:30        7.719         {"mlm": 7.714613440060856, "mse": 0.0}  0.046
1         train   8500/102467 51:38/570:50        7.719         {"mlm": 7.716868894715463, "mse": 0.0}  0.0506
1         train   8600/102467 52:14/570:10        7.719         {"mlm": 7.71270863801841, "mse": 0.0}  0.0446
1         train   8700/102467 52:50/569:31        7.719         {"mlm": 7.7121535245029404, "mse": 0.0}  0.0534
1         train   8800/102467 53:26/568:51        7.719         {"mlm": 7.715248751280895, "mse": 0.0}  0.0653
1         train   8900/102467 54:02/568:12        7.719         {"mlm": 7.71384896710515, "mse": 0.0}  0.0488
1         train   9000/102467 54:39/567:33        7.719         {"mlm": 7.715887698303743, "mse": 0.0}  0.0401
1         train   9100/102467 55:15/566:53        7.719         {"mlm": 7.7157346797685555, "mse": 0.0}  0.0457
1         train   9200/102467 55:51/566:15        7.719         {"mlm": 7.715200218069913, "mse": 0.0}  0.0361
1         train   9300/102467 56:27/565:37        7.719         {"mlm": 7.716909386861471, "mse": 0.0}  0.0489
1         train   9400/102467 57:03/564:59        7.719         {"mlm": 7.716463426464267, "mse": 0.0}  0.0379
1         train   9500/102467 57:40/564:20        7.719         {"mlm": 7.7163800635439825, "mse": 0.0}  0.0423
1         train   9600/102467 58:16/563:42        7.719         {"mlm": 7.717591661558414, "mse": 0.0}  0.0476
1         train   9700/102467 58:52/563:04        7.719         {"mlm": 7.719161715147631, "mse": 0.0}  0.0473
1         train   9800/102467 59:29/562:27        7.719         {"mlm": 7.719280664002179, "mse": 0.0}  0.0461
1         train   9900/102467 60:05/561:50        7.719         {"mlm": 7.718859052356286, "mse": 0.0}  0.0514
1         train   10000/102467 60:41/561:12       7.719         {"mlm": 7.718447950655569, "mse": 0.0}  0.0399

09/24/2022 18:55:10 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step10000.pkl
1         valid   1/781        0:20/269:31        7.651           None
1         valid   101/781      0:35/ 4:02         7.702           None
1         valid   201/781      0:51/ 2:28         7.712           None
1         valid   301/781      1:06/ 1:46         7.714           None
1         valid   401/781      1:21/ 1:17         7.718           None
1         valid   501/781      1:37/ 0:54         7.720           None
1         valid   601/781      1:52/ 0:33         7.720           None
1         valid   701/781      2:08/ 0:14         7.719           None
1         valid   781/781      2:22/ 0:00         7.720         {"mlm": 7.720070422535211, "mse": 0.0, "train": 0.0}  None
1         train   10100/102467 63:40/582:16       7.719         {"mlm": 7.7326328086853025, "mse": 0.0}  0.0453
1         train   10200/102467 64:16/581:22       7.719         {"mlm": 7.714017469882965, "mse": 0.0}  0.0524
1         train   10300/102467 64:52/580:28       7.719         {"mlm": 7.71343812306722, "mse": 0.0}  0.0385
1         train   10400/102467 65:28/579:35       7.719         {"mlm": 7.716585924625397, "mse": 0.0}  0.051
1         train   10500/102467 66:04/578:43       7.719         {"mlm": 7.718962953567505, "mse": 0.0}  0.0391
1         train   10600/102467 66:40/577:51       7.719         {"mlm": 7.717776528994242, "mse": 0.0}  0.0401
1         train   10700/102467 67:16/577:00       7.719         {"mlm": 7.718702861240932, "mse": 0.0}  0.0373
1         train   10800/102467 67:52/576:08       7.719         {"mlm": 7.719533842206001, "mse": 0.0}  0.0414
1         train   10900/102467 68:29/575:18       7.719         {"mlm": 7.719490721490648, "mse": 0.0}  0.0423
1         train   11000/102467 69:05/574:29       7.719         {"mlm": 7.7197468199729915, "mse": 0.0}  0.0447
1         train   11100/102467 69:41/573:39       7.719         {"mlm": 7.718470688299699, "mse": 0.0}  0.0411
1         train   11200/102467 70:17/572:49       7.719         {"mlm": 7.718708215554555, "mse": 0.0}  0.0425
1         train   11300/102467 70:53/571:59       7.719         {"mlm": 7.71764051253979, "mse": 0.0}  0.0391
1         train   11400/102467 71:30/571:10       7.719         {"mlm": 7.71857844046184, "mse": 0.0}  0.0383
1         train   11500/102467 72:06/570:21       7.719         {"mlm": 7.717720527013143, "mse": 0.0}  0.0417
1         train   11600/102467 72:42/569:32       7.719         {"mlm": 7.716388941407204, "mse": 0.0}  0.0434
1         train   11700/102467 73:18/568:43       7.719         {"mlm": 7.717970731118146, "mse": 0.0}  0.041
1         train   11800/102467 73:54/567:54       7.719         {"mlm": 7.716870110034943, "mse": 0.0}  0.0423
1         train   11900/102467 74:30/567:06       7.719         {"mlm": 7.7172347425159655, "mse": 0.0}  0.0471
1         train   12000/102467 75:07/566:19       7.718         {"mlm": 7.71596555185318, "mse": 0.0}  0.0388
1         train   12100/102467 75:43/565:32       7.718         {"mlm": 7.714210043049822, "mse": 0.0}  0.0385
1         train   12200/102467 76:19/564:45       7.718         {"mlm": 7.71600171429428, "mse": 0.0}  0.0438
1         train   12300/102467 76:56/563:58       7.718         {"mlm": 7.714815933967514, "mse": 0.0}  0.0402
1         train   12400/102467 77:32/563:12       7.719         {"mlm": 7.721122231399804, "mse": 0.0}  0.0482
1         train   12500/102467 78:08/562:25       7.719         {"mlm": 7.722738740917198, "mse": 0.0}  0.0412
1         train   12600/102467 78:44/561:39       7.718         {"mlm": 7.717296815276743, "mse": 0.0}  0.0381
1         train   12700/102467 79:21/560:53       7.719         {"mlm": 7.7194061074645735, "mse": 0.0}  0.0397
1         train   12800/102467 79:57/560:08       7.718         {"mlm": 7.717278097985832, "mse": 0.0}  0.0395
1         train   12900/102467 80:33/559:22       7.718         {"mlm": 7.715567598883912, "mse": 0.0}  0.0508
1         train   13000/102467 81:10/558:37       7.718         {"mlm": 7.715812523205121, "mse": 0.0}  0.0415
1         train   13100/102467 81:46/557:52       7.718         {"mlm": 7.7176660779826305, "mse": 0.0}  0.0404
1         train   13200/102467 82:22/557:07       7.718         {"mlm": 7.715467753263192, "mse": 0.0}  0.0433
1         train   13300/102467 82:59/556:22       7.718         {"mlm": 7.716323592646293, "mse": 0.0}  0.0451
1         train   13400/102467 83:35/555:37       7.718         {"mlm": 7.715737871479528, "mse": 0.0}  0.0383
1         train   13500/102467 84:11/554:52       7.718         {"mlm": 7.717855413092066, "mse": 0.0}  0.0468
1         train   13600/102467 84:48/554:08       7.718         {"mlm": 7.716834006568952, "mse": 0.0}  0.0566
1         train   13700/102467 85:24/553:24       7.718         {"mlm": 7.717810163503538, "mse": 0.0}  0.0434
1         train   13800/102467 86:00/552:39       7.718         {"mlm": 7.718387112609541, "mse": 0.0}  0.0425
1         train   13900/102467 86:37/551:54       7.719         {"mlm": 7.718918602236828, "mse": 0.0}  0.0415
1         train   14000/102467 87:13/551:10       7.718         {"mlm": 7.718412278114766, "mse": 0.0}  0.0418
1         train   14100/102467 87:49/550:26       7.719         {"mlm": 7.724240619309095, "mse": 0.0}  0.0426
1         train   14200/102467 88:26/549:42       7.718         {"mlm": 7.709716214074029, "mse": 0.0}  0.0428
1         train   14300/102467 89:02/548:58       7.718         {"mlm": 7.716080521577156, "mse": 0.0}  0.0524
1         train   14400/102467 89:38/548:14       7.718         {"mlm": 7.714793721635138, "mse": 0.0}  0.047
1         train   14500/102467 90:14/547:30       7.718         {"mlm": 7.717109320154152, "mse": 0.0}  0.039
1         train   14600/102467 90:51/546:46       7.718         {"mlm": 7.715485540919479, "mse": 0.0}  0.0431
1         train   14700/102467 91:27/546:02       7.718         {"mlm": 7.718578526488689, "mse": 0.0}  0.054
1         train   14800/102467 92:03/545:19       7.718         {"mlm": 7.717918045837479, "mse": 0.0}  0.042
1         train   14900/102467 92:39/544:35       7.718         {"mlm": 7.718847760113417, "mse": 0.0}  0.037
1         train   15000/102467 93:16/543:51       7.718         {"mlm": 7.716780323303773, "mse": 0.0}  0.0402
1         train   15100/102467 93:52/543:08       7.718         {"mlm": 7.717647513839499, "mse": 0.0}  0.0394
1         train   15200/102467 94:28/542:24       7.718         {"mlm": 7.718452182556432, "mse": 0.0}  0.0422
1         train   15300/102467 95:04/541:40       7.719         {"mlm": 7.718925041116441, "mse": 0.0}  0.043
1         train   15400/102467 95:40/540:57       7.718         {"mlm": 7.718578467553265, "mse": 0.0}  0.0431
1         train   15500/102467 96:17/540:14       7.718         {"mlm": 7.718028389723183, "mse": 0.0}  0.0334
1         train   15600/102467 96:53/539:31       7.718         {"mlm": 7.718549711981763, "mse": 0.0}  0.0433
1         train   15700/102467 97:29/538:48       7.718         {"mlm": 7.7186521374575525, "mse": 0.0}  0.0494
1         train   15800/102467 98:05/538:05       7.719         {"mlm": 7.719309505816959, "mse": 0.0}  0.0412
1         train   15900/102467 98:42/537:23       7.719         {"mlm": 7.719913661040295, "mse": 0.0}  0.0403
1         train   16000/102467 99:18/536:40       7.719         {"mlm": 7.719987850886088, "mse": 0.0}  0.0377
1         train   16100/102467 99:54/535:58       7.719         {"mlm": 7.713488013473983, "mse": 0.0}  0.0403
1         train   16200/102467 100:31/535:16      7.719         {"mlm": 7.713435642610347, "mse": 0.0}  0.0428
1         train   16300/102467 101:07/534:34      7.718         {"mlm": 7.709679532934119, "mse": 0.0}  0.0418
1         train   16400/102467 101:43/533:52      7.718         {"mlm": 7.709367651182698, "mse": 0.0}  0.0427
1         train   16500/102467 102:20/533:10      7.719         {"mlm": 7.714402089416381, "mse": 0.0}  0.04
1         train   16600/102467 102:56/532:28      7.719         {"mlm": 7.715620145526164, "mse": 0.0}  0.0408
1         train   16700/102467 103:32/531:46      7.719         {"mlm": 7.715970275391808, "mse": 0.0}  0.0403
1         train   16800/102467 104:08/531:04      7.719         {"mlm": 7.716135900921026, "mse": 0.0}  0.0411
1         train   16900/102467 104:45/530:23      7.718         {"mlm": 7.715590521643393, "mse": 0.0}  0.0378
1         train   17000/102467 105:21/529:41      7.719         {"mlm": 7.718142662029209, "mse": 0.0}  0.0407
1         train   17100/102467 105:57/528:59      7.719         {"mlm": 7.718625135169642, "mse": 0.0}  0.0437
1         train   17200/102467 106:34/528:18      7.719         {"mlm": 7.719939343413414, "mse": 0.0}  0.0406
1         train   17300/102467 107:10/527:36      7.719         {"mlm": 7.719474550569251, "mse": 0.0}  0.0379
1         train   17400/102467 107:46/526:54      7.719         {"mlm": 7.719639155893729, "mse": 0.0}  0.0392
1         train   17500/102467 108:22/526:13      7.719         {"mlm": 7.719788746588534, "mse": 0.0}  0.0418
1         train   17600/102467 108:59/525:31      7.719         {"mlm": 7.721367307207923, "mse": 0.0}  0.0412
1         train   17700/102467 109:35/524:50      7.719         {"mlm": 7.721408155291798, "mse": 0.0}  0.0395
1         train   17800/102467 110:11/524:09      7.719         {"mlm": 7.721426883935265, "mse": 0.0}  0.0404
1         train   17900/102467 110:47/523:27      7.719         {"mlm": 7.7209792159768735, "mse": 0.0}  0.0402
1         train   18000/102467 111:24/522:45      7.719         {"mlm": 7.720151893365484, "mse": 0.0}  0.0394
1         train   18100/102467 112:00/522:04      7.719         {"mlm": 7.711668436725934, "mse": 0.0}  0.0434
1         train   18200/102467 112:36/521:23      7.719         {"mlm": 7.718200897683903, "mse": 0.0}  0.0453
1         train   18300/102467 113:12/520:41      7.719         {"mlm": 7.729879783617483, "mse": 0.0}  0.0465
1         train   18400/102467 113:48/520:00      7.719         {"mlm": 7.727868000666301, "mse": 0.0}  0.0395
1         train   18500/102467 114:25/519:19      7.719         {"mlm": 7.726690101046716, "mse": 0.0}  0.0403
1         train   18600/102467 115:01/518:38      7.719         {"mlm": 7.721150075029207, "mse": 0.0}  0.0521
1         train   18700/102467 115:37/517:56      7.719         {"mlm": 7.71995304713304, "mse": 0.0}  0.0378
1         train   18800/102467 116:13/517:15      7.719         {"mlm": 7.717103939559592, "mse": 0.0}  0.0442
1         train   18900/102467 116:49/516:34      7.719         {"mlm": 7.717778893985918, "mse": 0.0}  0.051
1         train   19000/102467 117:26/515:53      7.719         {"mlm": 7.717034361448633, "mse": 0.0}  0.042
1         train   19100/102467 118:02/515:11      7.719         {"mlm": 7.71736981616403, "mse": 0.0}  0.0449
1         train   19200/102467 118:38/514:30      7.719         {"mlm": 7.719409004501674, "mse": 0.0}  0.0408
1         train   19300/102467 119:14/513:50      7.719         {"mlm": 7.719741534671666, "mse": 0.0}  0.0393
1         train   19400/102467 119:50/513:09      7.719         {"mlm": 7.72105423871289, "mse": 0.0}  0.0414
1         train   19500/102467 120:27/512:29      7.719         {"mlm": 7.7212842055820525, "mse": 0.0}  0.0383
1         train   19600/102467 121:03/511:49      7.719         {"mlm": 7.721474794516886, "mse": 0.0}  0.0465
1         train   19700/102467 121:39/511:08      7.719         {"mlm": 7.720869434329699, "mse": 0.0}  0.0463
1         train   19800/102467 122:16/510:29      7.719         {"mlm": 7.720280695067748, "mse": 0.0}  0.0453
1         train   19900/102467 122:52/509:49      7.719         {"mlm": 7.719494577953081, "mse": 0.0}  0.0363
1         train   20000/102467 123:28/509:09      7.719         {"mlm": 7.720502271919786, "mse": 0.0}  0.038

09/24/2022 19:57:57 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step20000.pkl
1         valid   1/781        0:21/274:02        7.631           None
1         valid   101/781      0:36/ 4:04         7.701           None
1         valid   201/781      0:51/ 2:29         7.717           None
1         valid   301/781      1:07/ 1:47         7.718           None
1         valid   401/781      1:22/ 1:18         7.720           None
1         valid   501/781      1:37/ 0:54         7.716           None
1         valid   601/781      1:53/ 0:33         7.718           None
1         valid   701/781      2:08/ 0:14         7.720           None
1         valid   781/781      2:22/ 0:00         7.719         {"mlm": 7.719110115236876, "mse": 0.0, "train": 0.0}  None
1         train   20100/102467 126:28/518:14      7.719         {"mlm": 7.7134491109848025, "mse": 0.0}  0.037
1         train   20200/102467 127:04/517:29      7.719         {"mlm": 7.709420478343963, "mse": 0.0}  0.0435
1         train   20300/102467 127:40/516:44      7.719         {"mlm": 7.710479491551717, "mse": 0.0}  0.0386
1         train   20400/102467 128:16/516:00      7.719         {"mlm": 7.708333220481872, "mse": 0.0}  0.0372
1         train   20500/102467 128:52/515:16      7.719         {"mlm": 7.709865580558777, "mse": 0.0}  0.0369
1         train   20600/102467 129:28/514:32      7.719         {"mlm": 7.712284450531006, "mse": 0.0}  0.0452
1         train   20700/102467 130:04/513:48      7.719         {"mlm": 7.710769410133362, "mse": 0.0}  0.0441
1         train   20800/102467 130:40/513:05      7.719         {"mlm": 7.71078796505928, "mse": 0.0}  0.0419
1         train   20900/102467 131:16/512:21      7.718         {"mlm": 7.705931839413113, "mse": 0.0}  0.0446
1         train   21000/102467 131:53/511:38      7.719         {"mlm": 7.709697020053864, "mse": 0.0}  0.0373
1         train   21100/102467 132:29/510:54      7.718         {"mlm": 7.7078101179816505, "mse": 0.0}  0.0412
1         train   21200/102467 133:05/510:11      7.718         {"mlm": 7.710251003901163, "mse": 0.0}  0.0388
1         train   21300/102467 133:41/509:27      7.718         {"mlm": 7.710517100187448, "mse": 0.0}  0.036
1         train   21400/102467 134:18/508:45      7.718         {"mlm": 7.710610711574555, "mse": 0.0}  0.0351
1         train   21500/102467 134:54/508:02      7.718         {"mlm": 7.710893874486287, "mse": 0.0}  0.0397
1         train   21600/102467 135:30/507:19      7.718         {"mlm": 7.712417818903923, "mse": 0.0}  0.0366
1         train   21700/102467 136:06/506:36      7.718         {"mlm": 7.7128609688141765, "mse": 0.0}  0.0437
1         train   21800/102467 136:43/505:53      7.718         {"mlm": 7.712980703777737, "mse": 0.0}  0.0454
1         train   21900/102467 137:19/505:11      7.718         {"mlm": 7.713277458140724, "mse": 0.0}  0.0386
1         train   22000/102467 137:55/504:28      7.719         {"mlm": 7.714159793615341, "mse": 0.0}  0.0385
1         train   22100/102467 138:31/503:46      7.719         {"mlm": 7.72022470320114, "mse": 0.0}  0.0371
1         train   22200/102467 139:08/503:03      7.718         {"mlm": 7.714323793823396, "mse": 0.0}  0.0379
1         train   22300/102467 139:44/502:21      7.719         {"mlm": 7.717454422277751, "mse": 0.0}  0.0342
1         train   22400/102467 140:20/501:38      7.719         {"mlm": 7.7207254646416, "mse": 0.0}  0.0374
1         train   22500/102467 140:56/500:56      7.719         {"mlm": 7.720565177634628, "mse": 0.0}  0.0511
1         train   22600/102467 141:32/500:13      7.719         {"mlm": 7.72064484101106, "mse": 0.0}  0.0427
1         train   22700/102467 142:09/499:31      7.719         {"mlm": 7.721052208683521, "mse": 0.0}  0.0408
1         train   22800/102467 142:45/498:49      7.719         {"mlm": 7.720550896378423, "mse": 0.0}  0.0482
1         train   22900/102467 143:21/498:07      7.719         {"mlm": 7.718637253206484, "mse": 0.0}  0.0448
1         train   23000/102467 143:58/497:25      7.719         {"mlm": 7.719190448612064, "mse": 0.0}  0.0557
1         train   23100/102467 144:34/496:43      7.719         {"mlm": 7.719733853465975, "mse": 0.0}  0.0433
1         train   23200/102467 145:10/496:01      7.719         {"mlm": 7.720392324210605, "mse": 0.0}  0.0448
1         train   23300/102467 145:47/495:20      7.719         {"mlm": 7.719504238551538, "mse": 0.0}  0.0409
1         train   23400/102467 146:23/494:38      7.719         {"mlm": 7.719506273958153, "mse": 0.0}  0.0386
1         train   23500/102467 146:59/493:56      7.719         {"mlm": 7.719412482047256, "mse": 0.0}  0.0385
1         train   23600/102467 147:35/493:14      7.719         {"mlm": 7.719779014587402, "mse": 0.0}  0.0422
1         train   23700/102467 148:12/492:33      7.719         {"mlm": 7.719240782750642, "mse": 0.0}  0.0376
1         train   23800/102467 148:48/491:51      7.719         {"mlm": 7.718367409348289, "mse": 0.0}  0.0335
1         train   23900/102467 149:24/491:09      7.719         {"mlm": 7.719311982346183, "mse": 0.0}  0.0396
1         train   24000/102467 150:00/490:28      7.719         {"mlm": 7.719087029648399, "mse": 0.0}  0.0395
1         train   24100/102467 150:37/489:46      7.719         {"mlm": 7.723324298858643, "mse": 0.0}  0.039
1         train   24200/102467 151:13/489:05      7.719         {"mlm": 7.7171408263119785, "mse": 0.0}  0.0398
1         train   24300/102467 151:49/488:24      7.719         {"mlm": 7.714530952824842, "mse": 0.0}  0.0389
1         train   24400/102467 152:26/487:43      7.718         {"mlm": 7.712338677602797, "mse": 0.0}  0.0488
1         train   24500/102467 153:02/487:02      7.718         {"mlm": 7.7136477476142975, "mse": 0.0}  0.0394
1         train   24600/102467 153:38/486:20      7.719         {"mlm": 7.717387242460729, "mse": 0.0}  0.0476
1         train   24700/102467 154:15/485:39      7.719         {"mlm": 7.716250578106987, "mse": 0.0}  0.0376
1         train   24800/102467 154:51/484:58      7.718         {"mlm": 7.715522398028457, "mse": 0.0}  0.0364
1         train   24900/102467 155:27/484:17      7.718         {"mlm": 7.7166075016183155, "mse": 0.0}  0.0403
1         train   25000/102467 156:03/483:35      7.718         {"mlm": 7.715819278556503, "mse": 0.0}  0.0398
1         train   25100/102467 156:40/482:55      7.718         {"mlm": 7.715192506004986, "mse": 0.0}  0.0456
1         train   25200/102467 157:16/482:14      7.718         {"mlm": 7.714980336381916, "mse": 0.0}  0.0415
1         train   25300/102467 157:53/481:33      7.718         {"mlm": 7.716376655825481, "mse": 0.0}  0.0451
1         train   25400/102467 158:29/480:52      7.718         {"mlm": 7.716627012506575, "mse": 0.0}  0.0431
1         train   25500/102467 159:05/480:11      7.718         {"mlm": 7.71714964759684, "mse": 0.0}  0.041
1         train   25600/102467 159:41/479:30      7.719         {"mlm": 7.718493565450771, "mse": 0.0}  0.0439
1         train   25700/102467 160:18/478:49      7.719         {"mlm": 7.7186626899368775, "mse": 0.0}  0.0382
1         train   25800/102467 160:54/478:08      7.719         {"mlm": 7.717843932755399, "mse": 0.0}  0.0371
1         train   25900/102467 161:30/477:27      7.719         {"mlm": 7.718078804719561, "mse": 0.0}  0.0443
1         train   26000/102467 162:06/476:47      7.719         {"mlm": 7.7183373704686895, "mse": 0.0}  0.0442
1         train   26100/102467 162:43/476:06      7.718         {"mlm": 7.692722045269209, "mse": 0.0}  0.0342
1         train   26200/102467 163:19/475:25      7.718         {"mlm": 7.705537382116172, "mse": 0.0}  0.0357
1         train   26300/102467 163:55/474:44      7.718         {"mlm": 7.7105490190011485, "mse": 0.0}  0.0383
1         train   26400/102467 164:31/474:03      7.718         {"mlm": 7.713602644970796, "mse": 0.0}  0.0384
1         train   26500/102467 165:07/473:22      7.719         {"mlm": 7.716712388474217, "mse": 0.0}  0.0373
1         train   26600/102467 165:44/472:41      7.718         {"mlm": 7.71487146246573, "mse": 0.0}  0.0454
1         train   26700/102467 166:20/472:01      7.719         {"mlm": 7.717151070598892, "mse": 0.0}  0.0409
1         train   26800/102467 166:56/471:20      7.719         {"mlm": 7.720074154053309, "mse": 0.0}  0.0372
1         train   26900/102467 167:32/470:39      7.719         {"mlm": 7.719898740051848, "mse": 0.0}  0.0349
1         train   27000/102467 168:08/469:59      7.719         {"mlm": 7.7208944113108675, "mse": 0.0}  0.0339
1         train   27100/102467 168:45/469:18      7.719         {"mlm": 7.721459099674833, "mse": 0.0}  0.0407
1         train   27200/102467 169:21/468:38      7.719         {"mlm": 7.72209151984257, "mse": 0.0}  0.0402
1         train   27300/102467 169:57/467:58      7.719         {"mlm": 7.722068160887213, "mse": 0.0}  0.0571
1         train   27400/102467 170:34/467:17      7.719         {"mlm": 7.721869899446656, "mse": 0.0}  0.0384
1         train   27500/102467 171:10/466:37      7.719         {"mlm": 7.721017561678737, "mse": 0.0}  0.0388
1         train   27600/102467 171:46/465:57      7.719         {"mlm": 7.721295040849603, "mse": 0.0}  0.0357
1         train   27700/102467 172:23/465:17      7.719         {"mlm": 7.7191601926323665, "mse": 0.0}  0.0422
1         train   27800/102467 172:59/464:37      7.719         {"mlm": 7.7190720688719585, "mse": 0.0}  0.0377
1         train   27900/102467 173:35/463:56      7.719         {"mlm": 7.718824646255501, "mse": 0.0}  0.0434
1         train   28000/102467 174:11/463:16      7.719         {"mlm": 7.71935570329562, "mse": 0.0}  0.0386
1         train   28100/102467 174:47/462:36      7.719         {"mlm": 7.732821350296338, "mse": 0.0}  0.0343
1         train   28200/102467 175:24/461:56      7.719         {"mlm": 7.731395728734075, "mse": 0.0}  0.0421
1         train   28300/102467 176:00/461:16      7.719         {"mlm": 7.72396084263518, "mse": 0.0}  0.0371
1         train   28400/102467 176:36/460:36      7.719         {"mlm": 7.723911532247909, "mse": 0.0}  0.0426
1         train   28500/102467 177:13/459:56      7.719         {"mlm": 7.720487837829897, "mse": 0.0}  0.0446
1         train   28600/102467 177:49/459:16      7.719         {"mlm": 7.723154808850897, "mse": 0.0}  0.0369
1         train   28700/102467 178:25/458:36      7.719         {"mlm": 7.723718198551529, "mse": 0.0}  0.0419
1         train   28800/102467 179:02/457:56      7.719         {"mlm": 7.721432827225882, "mse": 0.0}  0.0358
1         train   28900/102467 179:38/457:16      7.719         {"mlm": 7.722663019384656, "mse": 0.0}  0.0454
1         train   29000/102467 180:14/456:36      7.719         {"mlm": 7.7221540863734175, "mse": 0.0}  0.0443
1         train   29100/102467 180:50/455:56      7.719         {"mlm": 7.720851772458014, "mse": 0.0}  0.0414
1         train   29200/102467 181:26/455:16      7.719         {"mlm": 7.721753088526901, "mse": 0.0}  0.0351
1         train   29300/102467 182:04/454:40      7.719         {"mlm": 7.723043433310073, "mse": 0.0}  0.0397
1         train   29400/102467 182:40/454:00      7.719         {"mlm": 7.723570203712813, "mse": 0.0}  0.0412
1         train   29500/102467 183:17/453:20      7.719         {"mlm": 7.723062703634966, "mse": 0.0}  0.0363
1         train   29600/102467 183:53/452:40      7.719         {"mlm": 7.721749312298041, "mse": 0.0}  0.0334
1         train   29700/102467 184:29/452:01      7.719         {"mlm": 7.720375468146126, "mse": 0.0}  0.0377
1         train   29800/102467 185:05/451:21      7.719         {"mlm": 7.720389524387623, "mse": 0.0}  0.0358
1         train   29900/102467 185:42/450:41      7.719         {"mlm": 7.7195774434991025, "mse": 0.0}  0.044
1         train   30000/102467 186:18/450:02      7.719         {"mlm": 7.719081338517412, "mse": 0.0}  0.0418

09/24/2022 21:00:46 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step30000.pkl
1         valid   1/781        0:19/250:07        7.889           None
1         valid   101/781      0:34/ 3:52         7.703           None
1         valid   201/781      0:49/ 2:23         7.717           None
1         valid   301/781      1:05/ 1:43         7.714           None
1         valid   401/781      1:20/ 1:16         7.718           None
1         valid   501/781      1:35/ 0:53         7.716           None
1         valid   601/781      1:51/ 0:33         7.714           None
1         valid   701/781      2:06/ 0:14         7.718           None
1         valid   781/781      2:20/ 0:00         7.718         {"mlm": 7.71830985915493, "mse": 0.0, "train": 0.0}  None
1         train   30100/102467 189:15/455:01      7.719         {"mlm": 7.72951374053955, "mse": 0.0}  0.042
1         train   30200/102467 189:51/454:19      7.719         {"mlm": 7.726344244480133, "mse": 0.0}  0.0424
1         train   30300/102467 190:27/453:37      7.719         {"mlm": 7.728834015528361, "mse": 0.0}  0.051
1         train   30400/102467 191:03/452:55      7.719         {"mlm": 7.724701821804047, "mse": 0.0}  0.0394
1         train   30500/102467 191:39/452:14      7.719         {"mlm": 7.720481692314148, "mse": 0.0}  0.0376
1         train   30600/102467 192:15/451:32      7.719         {"mlm": 7.714842848777771, "mse": 0.0}  0.0398
1         train   30700/102467 192:51/450:51      7.719         {"mlm": 7.718485349246434, "mse": 0.0}  0.035
1         train   30800/102467 193:27/450:09      7.719         {"mlm": 7.717183932065963, "mse": 0.0}  0.0374
1         train   30900/102467 194:04/449:28      7.719         {"mlm": 7.716720192697313, "mse": 0.0}  0.0455
1         train   31000/102467 194:40/448:47      7.719         {"mlm": 7.715418583393097, "mse": 0.0}  0.044
1         train   31100/102467 195:16/448:06      7.719         {"mlm": 7.7159902858734135, "mse": 0.0}  0.0485
1         train   31200/102467 195:52/447:25      7.719         {"mlm": 7.71810360232989, "mse": 0.0}  0.039
1         train   31300/102467 196:29/446:44      7.719         {"mlm": 7.718044075232286, "mse": 0.0}  0.037
1         train   31400/102467 197:05/446:04      7.719         {"mlm": 7.719075688294002, "mse": 0.0}  0.0449
1         train   31500/102467 197:41/445:23      7.719         {"mlm": 7.718490071614584, "mse": 0.0}  0.0381
1         train   31600/102467 198:17/444:42      7.719         {"mlm": 7.718443853855133, "mse": 0.0}  0.04
1         train   31700/102467 198:54/444:02      7.719         {"mlm": 7.719228001482347, "mse": 0.0}  0.0361
1         train   31800/102467 199:30/443:21      7.719         {"mlm": 7.718189178572761, "mse": 0.0}  0.0381
1         train   31900/102467 200:06/442:40      7.719         {"mlm": 7.717391180490193, "mse": 0.0}  0.0359
1         train   32000/102467 200:43/442:00      7.719         {"mlm": 7.717161605596543, "mse": 0.0}  0.0385
1         train   32100/102467 201:19/441:19      7.719         {"mlm": 7.746958352098561, "mse": 0.0}  0.0369
1         train   32200/102467 201:55/440:39      7.719         {"mlm": 7.736511264015083, "mse": 0.0}  0.0393
1         train   32300/102467 202:32/439:58      7.719         {"mlm": 7.726107879625913, "mse": 0.0}  0.0449
1         train   32400/102467 203:08/439:18      7.719         {"mlm": 7.725566571218925, "mse": 0.0}  0.0395
1         train   32500/102467 203:44/438:37      7.719         {"mlm": 7.726627047888502, "mse": 0.0}  0.0358
1         train   32600/102467 204:21/437:57      7.719         {"mlm": 7.724724861934707, "mse": 0.0}  0.0417
1         train   32700/102467 204:57/437:16      7.719         {"mlm": 7.725708836649621, "mse": 0.0}  0.0359
1         train   32800/102467 205:33/436:36      7.719         {"mlm": 7.72519957825299, "mse": 0.0}  0.035
1         train   32900/102467 206:09/435:56      7.719         {"mlm": 7.72661197225297, "mse": 0.0}  0.0422
1         train   33000/102467 206:46/435:16      7.719         {"mlm": 7.725306593500696, "mse": 0.0}  0.0463
1         train   33100/102467 207:22/434:35      7.719         {"mlm": 7.725342281955931, "mse": 0.0}  0.0327
1         train   33200/102467 207:58/433:55      7.719         {"mlm": 7.725885856141638, "mse": 0.0}  0.0375
1         train   33300/102467 208:35/433:15      7.719         {"mlm": 7.722999133358927, "mse": 0.0}  0.0378
1         train   33400/102467 209:11/432:34      7.719         {"mlm": 7.722044313184017, "mse": 0.0}  0.0336
1         train   33500/102467 209:47/431:54      7.719         {"mlm": 7.721307377563945, "mse": 0.0}  0.0441
1         train   33600/102467 210:24/431:14      7.719         {"mlm": 7.721238096629627, "mse": 0.0}  0.0373
1         train   33700/102467 211:00/430:34      7.719         {"mlm": 7.72215953302636, "mse": 0.0}  0.0368
1         train   33800/102467 211:36/429:54      7.719         {"mlm": 7.721272237702434, "mse": 0.0}  0.0501
1         train   33900/102467 212:12/429:13      7.719         {"mlm": 7.720764964677711, "mse": 0.0}  0.0429
1         train   34000/102467 212:49/428:33      7.719         {"mlm": 7.7202804113639, "mse": 0.0}  0.0369
1         train   34100/102467 213:25/427:52      7.719         {"mlm": 7.722345206202293, "mse": 0.0}  0.0413
1         train   34200/102467 214:01/427:12      7.719         {"mlm": 7.723659939236111, "mse": 0.0}  0.041
1         train   34300/102467 214:37/426:32      7.719         {"mlm": 7.727747528344993, "mse": 0.0}  0.0425
1         train   34400/102467 215:13/425:52      7.719         {"mlm": 7.7274220313259105, "mse": 0.0}  0.0353
1         train   34500/102467 215:49/425:11      7.719         {"mlm": 7.726588848604256, "mse": 0.0}  0.0358
1         train   34600/102467 216:26/424:31      7.719         {"mlm": 7.725792997258164, "mse": 0.0}  0.0371
1         train   34700/102467 217:02/423:51      7.719         {"mlm": 7.723682682288752, "mse": 0.0}  0.0362
1         train   34800/102467 217:38/423:11      7.719         {"mlm": 7.727864579748092, "mse": 0.0}  0.0347
1         train   34900/102467 218:14/422:31      7.719         {"mlm": 7.7266786555139415, "mse": 0.0}  0.0326
1         train   35000/102467 218:51/421:51      7.719         {"mlm": 7.72604447185157, "mse": 0.0}  0.0385
1         train   35100/102467 219:27/421:11      7.719         {"mlm": 7.726458029668839, "mse": 0.0}  0.0451
1         train   35200/102467 220:03/420:31      7.719         {"mlm": 7.724586405220732, "mse": 0.0}  0.0393
1         train   35300/102467 220:39/419:51      7.719         {"mlm": 7.723784421000899, "mse": 0.0}  0.0482
1         train   35400/102467 221:15/419:11      7.719         {"mlm": 7.722918285661842, "mse": 0.0}  0.0334
1         train   35500/102467 221:52/418:32      7.719         {"mlm": 7.722582005053878, "mse": 0.0}  0.0377
1         train   35600/102467 222:28/417:52      7.719         {"mlm": 7.721772667761887, "mse": 0.0}  0.0394
1         train   35700/102467 223:04/417:12      7.719         {"mlm": 7.722402549042719, "mse": 0.0}  0.047
1         train   35800/102467 223:41/416:32      7.719         {"mlm": 7.721838671585609, "mse": 0.0}  0.0388
1         train   35900/102467 224:17/415:53      7.719         {"mlm": 7.720936627232237, "mse": 0.0}  0.0366
1         train   36000/102467 224:53/415:13      7.719         {"mlm": 7.720388081696656, "mse": 0.0}  0.0378
1         train   36100/102467 225:30/414:33      7.719         {"mlm": 7.728671300042536, "mse": 0.0}  0.038
1         train   36200/102467 226:06/413:54      7.719         {"mlm": 7.721656634722869, "mse": 0.0}  0.0402
1         train   36300/102467 226:42/413:14      7.719         {"mlm": 7.714254393722072, "mse": 0.0}  0.0384
1         train   36400/102467 227:18/412:34      7.719         {"mlm": 7.720772888558337, "mse": 0.0}  0.048
1         train   36500/102467 227:55/411:55      7.719         {"mlm": 7.722386250793334, "mse": 0.0}  0.0378
1         train   36600/102467 228:31/411:15      7.719         {"mlm": 7.721306403877348, "mse": 0.0}  0.0355
1         train   36700/102467 229:07/410:36      7.719         {"mlm": 7.72335072329944, "mse": 0.0}  0.0344
1         train   36800/102467 229:43/409:56      7.719         {"mlm": 7.721584769384176, "mse": 0.0}  0.0463
1         train   36900/102467 230:20/409:17      7.719         {"mlm": 7.722224530567693, "mse": 0.0}  0.0523
1         train   37000/102467 230:56/408:37      7.719         {"mlm": 7.721784201883146, "mse": 0.0}  0.0353
1         train   37100/102467 231:32/407:58      7.719         {"mlm": 7.717905718734727, "mse": 0.0}  0.0328
1         train   37200/102467 232:09/407:18      7.719         {"mlm": 7.718477892497229, "mse": 0.0}  0.0396
1         train   37300/102467 232:45/406:38      7.719         {"mlm": 7.716878669300535, "mse": 0.0}  0.0492
1         train   37400/102467 233:21/405:59      7.719         {"mlm": 7.7161399463115625, "mse": 0.0}  0.0409
1         train   37500/102467 233:57/405:19      7.719         {"mlm": 7.7179201931338675, "mse": 0.0}  0.0357
1         train   37600/102467 234:34/404:40      7.719         {"mlm": 7.718066838060832, "mse": 0.0}  0.037
1         train   37700/102467 235:10/404:00      7.719         {"mlm": 7.718015260533438, "mse": 0.0}  0.0368
1         train   37800/102467 235:46/403:21      7.719         {"mlm": 7.717752673457448, "mse": 0.0}  0.0334
1         train   37900/102467 236:22/402:41      7.719         {"mlm": 7.718738414641739, "mse": 0.0}  0.0363
1         train   38000/102467 236:58/402:02      7.719         {"mlm": 7.718981454416579, "mse": 0.0}  0.0329
1         train   38100/102467 237:34/401:22      7.719         {"mlm": 7.724401851495107, "mse": 0.0}  0.0345
1         train   38200/102467 238:10/400:42      7.719         {"mlm": 7.716952258226823, "mse": 0.0}  0.0353
1         train   38300/102467 238:47/400:03      7.719         {"mlm": 7.7114798990455835, "mse": 0.0}  0.0447
1         train   38400/102467 239:23/399:24      7.719         {"mlm": 7.715050912866689, "mse": 0.0}  0.0384
1         train   38500/102467 239:59/398:44      7.719         {"mlm": 7.7138961447823435, "mse": 0.0}  0.0351
1         train   38600/102467 240:35/398:05      7.719         {"mlm": 7.711120325446929, "mse": 0.0}  0.0329
1         train   38700/102467 241:12/397:26      7.719         {"mlm": 7.710226285046544, "mse": 0.0}  0.0331
1         train   38800/102467 241:48/396:46      7.719         {"mlm": 7.710480061607744, "mse": 0.0}  0.0371
1         train   38900/102467 242:24/396:07      7.719         {"mlm": 7.714701085218361, "mse": 0.0}  0.0457
1         train   39000/102467 243:00/395:28      7.719         {"mlm": 7.715175559721797, "mse": 0.0}  0.0435
1         train   39100/102467 243:36/394:48      7.719         {"mlm": 7.713070402615262, "mse": 0.0}  0.0421
1         train   39200/102467 244:13/394:09      7.719         {"mlm": 7.714849244790731, "mse": 0.0}  0.0339
1         train   39300/102467 244:49/393:30      7.719         {"mlm": 7.716091287724765, "mse": 0.0}  0.0332
1         train   39400/102467 245:25/392:51      7.719         {"mlm": 7.714509668186264, "mse": 0.0}  0.0425
1         train   39500/102467 246:02/392:12      7.719         {"mlm": 7.714307159026039, "mse": 0.0}  0.0373
1         train   39600/102467 246:38/391:33      7.719         {"mlm": 7.715148510789513, "mse": 0.0}  0.0415
1         train   39700/102467 247:14/390:54      7.719         {"mlm": 7.713903542397158, "mse": 0.0}  0.0379
1         train   39800/102467 247:50/390:15      7.719         {"mlm": 7.713680783199574, "mse": 0.0}  0.0411
1         train   39900/102467 248:27/389:36      7.718         {"mlm": 7.713140563874305, "mse": 0.0}  0.0383
1         train   40000/102467 249:03/388:57      7.718         {"mlm": 7.712890208364728, "mse": 0.0}  0.0403

09/24/2022 22:03:32 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step40000.pkl
1         valid   1/781        0:22/286:56        7.819           None
1         valid   101/781      0:37/ 4:11         7.707           None
1         valid   201/781      0:52/ 2:32         7.714           None
1         valid   301/781      1:08/ 1:48         7.720           None
1         valid   401/781      1:23/ 1:19         7.724           None
1         valid   501/781      1:38/ 0:55         7.726           None
1         valid   601/781      1:53/ 0:34         7.724           None
1         valid   701/781      2:09/ 0:14         7.725           None
1         valid   781/781      2:23/ 0:00         7.722         {"mlm": 7.721510883482715, "mse": 0.0, "train": 0.0}  None
1         train   40100/102467 252:03/392:01      7.718         {"mlm": 7.71739107131958, "mse": 0.0}  0.0444
1         train   40200/102467 252:39/391:20      7.718         {"mlm": 7.722028667926788, "mse": 0.0}  0.0367
1         train   40300/102467 253:15/390:40      7.718         {"mlm": 7.719889157613118, "mse": 0.0}  0.0376
1         train   40400/102467 253:51/390:00      7.718         {"mlm": 7.720202308893204, "mse": 0.0}  0.0368
1         train   40500/102467 254:27/389:20      7.718         {"mlm": 7.719678027153015, "mse": 0.0}  0.036
1         train   40600/102467 255:03/388:39      7.718         {"mlm": 7.7156280716260275, "mse": 0.0}  0.0362
1         train   40700/102467 255:39/388:00      7.718         {"mlm": 7.719634139197213, "mse": 0.0}  0.032
1         train   40800/102467 256:16/387:20      7.719         {"mlm": 7.720127074122429, "mse": 0.0}  0.0339
1         train   40900/102467 256:52/386:40      7.719         {"mlm": 7.72153947353363, "mse": 0.0}  0.0329
1         train   41000/102467 257:28/386:00      7.719         {"mlm": 7.721680075168609, "mse": 0.0}  0.0384
1         train   41100/102467 258:04/385:20      7.719         {"mlm": 7.719829613945701, "mse": 0.0}  0.036
1         train   41200/102467 258:40/384:40      7.719         {"mlm": 7.721398416360219, "mse": 0.0}  0.038
1         train   41300/102467 259:17/384:00      7.719         {"mlm": 7.721156265552227, "mse": 0.0}  0.0348
1         train   41400/102467 259:53/383:21      7.718         {"mlm": 7.71870336157935, "mse": 0.0}  0.0387
1         train   41500/102467 260:29/382:41      7.718         {"mlm": 7.717243440310161, "mse": 0.0}  0.0357
1         train   41600/102467 261:05/382:01      7.718         {"mlm": 7.718017429411411, "mse": 0.0}  0.0377
1         train   41700/102467 261:42/381:22      7.718         {"mlm": 7.718339970252093, "mse": 0.0}  0.0351
1         train   41800/102467 262:18/380:42      7.718         {"mlm": 7.7187629583146835, "mse": 0.0}  0.0387
1         train   41900/102467 262:55/380:02      7.718         {"mlm": 7.718114948272705, "mse": 0.0}  0.0375
1         train   42000/102467 263:31/379:23      7.718         {"mlm": 7.7176089181900025, "mse": 0.0}  0.0351
1         train   42100/102467 264:07/378:43      7.718         {"mlm": 7.718923722854768, "mse": 0.0}  0.0352
1         train   42200/102467 264:44/378:04      7.718         {"mlm": 7.723656199086252, "mse": 0.0}  0.0439
1         train   42300/102467 265:20/377:24      7.718         {"mlm": 7.718442392189766, "mse": 0.0}  0.0431
1         train   42400/102467 265:56/376:45      7.718         {"mlm": 7.721259892733772, "mse": 0.0}  0.0448
1         train   42500/102467 266:33/376:05      7.718         {"mlm": 7.7227767044174405, "mse": 0.0}  0.0396
1         train   42600/102467 267:09/375:26      7.718         {"mlm": 7.720876594218667, "mse": 0.0}  0.036
1         train   42700/102467 267:45/374:46      7.718         {"mlm": 7.71881250250493, "mse": 0.0}  0.0362
1         train   42800/102467 268:21/374:07      7.718         {"mlm": 7.720839316018383, "mse": 0.0}  0.0416
1         train   42900/102467 268:58/373:27      7.719         {"mlm": 7.7226582899507346, "mse": 0.0}  0.0423
1         train   43000/102467 269:34/372:48      7.718         {"mlm": 7.718317166463033, "mse": 0.0}  0.0338
1         train   43100/102467 270:10/372:08      7.718         {"mlm": 7.719333415252713, "mse": 0.0}  0.0412
1         train   43200/102467 270:46/371:29      7.718         {"mlm": 7.719584858347914, "mse": 0.0}  0.0375
1         train   43300/102467 271:22/370:49      7.718         {"mlm": 7.72041956877323, "mse": 0.0}  0.0368
1         train   43400/102467 271:58/370:09      7.718         {"mlm": 7.719981737866242, "mse": 0.0}  0.0321
1         train   43500/102467 272:35/369:30      7.719         {"mlm": 7.721212415714277, "mse": 0.0}  0.0339
1         train   43600/102467 273:11/368:50      7.719         {"mlm": 7.722375184167095, "mse": 0.0}  0.0413
1         train   43700/102467 273:47/368:11      7.719         {"mlm": 7.721654927611281, "mse": 0.0}  0.0361
1         train   43800/102467 274:23/367:31      7.719         {"mlm": 7.722027046538645, "mse": 0.0}  0.0526
1         train   43900/102467 274:59/366:52      7.719         {"mlm": 7.721919200620003, "mse": 0.0}  0.0452
1         train   44000/102467 275:36/366:13      7.719         {"mlm": 7.721724062218793, "mse": 0.0}  0.0347
1         train   44100/102467 276:12/365:33      7.719         {"mlm": 7.7192809776384, "mse": 0.0}  0.0403
1         train   44200/102467 276:48/364:54      7.719         {"mlm": 7.72077292866177, "mse": 0.0}  0.0353
1         train   44300/102467 277:24/364:15      7.719         {"mlm": 7.717550927360586, "mse": 0.0}  0.0424
1         train   44400/102467 278:01/363:35      7.719         {"mlm": 7.7125345330741535, "mse": 0.0}  0.0398
1         train   44500/102467 278:37/362:56      7.718         {"mlm": 7.707835177341139, "mse": 0.0}  0.0372
1         train   44600/102467 279:13/362:17      7.718         {"mlm": 7.708548254791311, "mse": 0.0}  0.0405
1         train   44700/102467 279:49/361:38      7.718         {"mlm": 7.708632657042888, "mse": 0.0}  0.038
1         train   44800/102467 280:26/360:58      7.718         {"mlm": 7.706826526718331, "mse": 0.0}  0.04
1         train   44900/102467 281:02/360:19      7.718         {"mlm": 7.707514134706527, "mse": 0.0}  0.0356
1         train   45000/102467 281:38/359:40      7.718         {"mlm": 7.709187054681873, "mse": 0.0}  0.0348
1         train   45100/102467 282:15/359:01      7.718         {"mlm": 7.711810618801847, "mse": 0.0}  0.0411
1         train   45200/102467 282:51/358:22      7.718         {"mlm": 7.715324314289379, "mse": 0.0}  0.0357
1         train   45300/102467 283:27/357:43      7.719         {"mlm": 7.716193377696127, "mse": 0.0}  0.0342
1         train   45400/102467 284:03/357:03      7.718         {"mlm": 7.71578635098426, "mse": 0.0}  0.037
1         train   45500/102467 284:40/356:24      7.719         {"mlm": 7.716402276972426, "mse": 0.0}  0.0416
1         train   45600/102467 285:16/355:45      7.719         {"mlm": 7.716670063469974, "mse": 0.0}  0.0337
1         train   45700/102467 285:52/355:06      7.719         {"mlm": 7.716798956458785, "mse": 0.0}  0.0358
1         train   45800/102467 286:28/354:27      7.719         {"mlm": 7.718497815201094, "mse": 0.0}  0.0362
1         train   45900/102467 287:04/353:48      7.719         {"mlm": 7.7178474015255, "mse": 0.0}  0.0365
1         train   46000/102467 287:41/353:08      7.719         {"mlm": 7.718165163521294, "mse": 0.0}  0.036
1         train   46100/102467 288:17/352:29      7.719         {"mlm": 7.717964462398254, "mse": 0.0}  0.0346
1         train   46200/102467 288:53/351:50      7.719         {"mlm": 7.717929031643165, "mse": 0.0}  0.0373
1         train   46300/102467 289:29/351:11      7.719         {"mlm": 7.7225100536539095, "mse": 0.0}  0.0352
1         train   46400/102467 290:05/350:32      7.719         {"mlm": 7.723451114721803, "mse": 0.0}  0.0453
1         train   46500/102467 290:41/349:53      7.719         {"mlm": 7.725249250170211, "mse": 0.0}  0.0323
1         train   46600/102467 291:18/349:13      7.719         {"mlm": 7.721028477302948, "mse": 0.0}  0.0446
1         train   46700/102467 291:54/348:34      7.719         {"mlm": 7.717585619758157, "mse": 0.0}  0.0368
1         train   46800/102467 292:30/347:55      7.719         {"mlm": 7.717719802593198, "mse": 0.0}  0.0333
1         train   46900/102467 293:06/347:16      7.719         {"mlm": 7.717872599960038, "mse": 0.0}  0.0375
1         train   47000/102467 293:42/346:37      7.719         {"mlm": 7.719438128150932, "mse": 0.0}  0.034
1         train   47100/102467 294:19/345:58      7.719         {"mlm": 7.718432252148443, "mse": 0.0}  0.0461
1         train   47200/102467 294:55/345:19      7.719         {"mlm": 7.719490122974367, "mse": 0.0}  0.0356
1         train   47300/102467 295:31/344:40      7.719         {"mlm": 7.719682978039626, "mse": 0.0}  0.0372
1         train   47400/102467 296:07/344:01      7.719         {"mlm": 7.717307182918213, "mse": 0.0}  0.0376
1         train   47500/102467 296:43/343:22      7.719         {"mlm": 7.718242173522969, "mse": 0.0}  0.0392
1         train   47600/102467 297:19/342:43      7.719         {"mlm": 7.719397019653225, "mse": 0.0}  0.038
1         train   47700/102467 297:56/342:04      7.719         {"mlm": 7.718332782659941, "mse": 0.0}  0.0376
1         train   47800/102467 298:32/341:25      7.719         {"mlm": 7.717887225652577, "mse": 0.0}  0.0419
1         train   47900/102467 299:08/340:47      7.719         {"mlm": 7.717306763734953, "mse": 0.0}  0.034
1         train   48000/102467 299:45/340:08      7.719         {"mlm": 7.717920687297731, "mse": 0.0}  0.0321
1         train   48100/102467 300:21/339:29      7.719         {"mlm": 7.719864989320437, "mse": 0.0}  0.0338
1         train   48200/102467 300:57/338:50      7.718         {"mlm": 7.708370503114194, "mse": 0.0}  0.0388
1         train   48300/102467 301:33/338:11      7.718         {"mlm": 7.710075410636696, "mse": 0.0}  0.0343
1         train   48400/102467 302:10/337:32      7.718         {"mlm": 7.71194067747906, "mse": 0.0}  0.0387
1         train   48500/102467 302:46/336:54      7.718         {"mlm": 7.707743848523786, "mse": 0.0}  0.0337
1         train   48600/102467 303:22/336:15      7.718         {"mlm": 7.7114259324617835, "mse": 0.0}  0.0366
1         train   48700/102467 303:59/335:36      7.718         {"mlm": 7.710215513048501, "mse": 0.0}  0.035
1         train   48800/102467 304:35/334:58      7.718         {"mlm": 7.709189819331145, "mse": 0.0}  0.0378
1         train   48900/102467 305:11/334:19      7.718         {"mlm": 7.711229805435453, "mse": 0.0}  0.0415
1         train   49000/102467 305:47/333:40      7.718         {"mlm": 7.711828262451662, "mse": 0.0}  0.0418
1         train   49100/102467 306:24/333:01      7.718         {"mlm": 7.710512345289662, "mse": 0.0}  0.0382
1         train   49200/102467 307:00/332:22      7.718         {"mlm": 7.7136554989129005, "mse": 0.0}  0.0319
1         train   49300/102467 307:36/331:44      7.718         {"mlm": 7.713866580783585, "mse": 0.0}  0.038
1         train   49400/102467 308:12/331:05      7.718         {"mlm": 7.713989215457337, "mse": 0.0}  0.0379
1         train   49500/102467 308:48/330:26      7.718         {"mlm": 7.714817539255887, "mse": 0.0}  0.0444
1         train   49600/102467 309:24/329:47      7.718         {"mlm": 7.714302673674466, "mse": 0.0}  0.0329
1         train   49700/102467 310:01/329:09      7.718         {"mlm": 7.716634447844523, "mse": 0.0}  0.0378
1         train   49800/102467 310:37/328:30      7.718         {"mlm": 7.716908210369951, "mse": 0.0}  0.0416
1         train   49900/102467 311:13/327:51      7.718         {"mlm": 7.717672821590166, "mse": 0.0}  0.0404
1         train   50000/102467 311:49/327:12      7.718         {"mlm": 7.716411371269302, "mse": 0.0}  0.0351

09/24/2022 23:06:18 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step50000.pkl
1         valid   1/781        0:19/249:27        7.530           None
1         valid   101/781      0:34/ 3:52         7.706           None
1         valid   201/781      0:49/ 2:23         7.721           None
1         valid   301/781      1:05/ 1:43         7.723           None
1         valid   401/781      1:20/ 1:16         7.733           None
1         valid   501/781      1:35/ 0:53         7.730           None
1         valid   601/781      1:51/ 0:33         7.730           None
1         valid   701/781      2:06/ 0:14         7.731           None
1         valid   781/781      2:21/ 0:00         7.721         {"mlm": 7.720870678617158, "mse": 0.0, "train": 0.0}  None
1         train   50100/102467 314:46/329:01      7.718         {"mlm": 7.695668730735779, "mse": 0.0}  0.0389
1         train   50200/102467 315:22/328:21      7.718         {"mlm": 7.702423117160797, "mse": 0.0}  0.0326
1         train   50300/102467 315:58/327:42      7.718         {"mlm": 7.708065245946249, "mse": 0.0}  0.0343
1         train   50400/102467 316:34/327:02      7.718         {"mlm": 7.705036848783493, "mse": 0.0}  0.0375
1         train   50500/102467 317:10/326:23      7.718         {"mlm": 7.710339826583862, "mse": 0.0}  0.0371
1         train   50600/102467 317:46/325:43      7.718         {"mlm": 7.71484246969223, "mse": 0.0}  0.0335
1         train   50700/102467 318:22/325:04      7.718         {"mlm": 7.716732928412301, "mse": 0.0}  0.039
1         train   50800/102467 318:58/324:25      7.718         {"mlm": 7.71697438120842, "mse": 0.0}  0.0429
1         train   50900/102467 319:34/323:45      7.718         {"mlm": 7.718894195026822, "mse": 0.0}  0.0385
1         train   51000/102467 320:10/323:06      7.718         {"mlm": 7.717810340881348, "mse": 0.0}  0.0339
1         train   51100/102467 320:46/322:27      7.718         {"mlm": 7.716734889203852, "mse": 0.0}  0.0364
1         train   51200/102467 321:23/321:48      7.718         {"mlm": 7.7177939828236894, "mse": 0.0}  0.0345
1         train   51300/102467 321:59/321:09      7.718         {"mlm": 7.718248045994685, "mse": 0.0}  0.0363
1         train   51400/102467 322:35/320:29      7.718         {"mlm": 7.719452211516244, "mse": 0.0}  0.0348
1         train   51500/102467 323:11/319:50      7.718         {"mlm": 7.719266320228576, "mse": 0.0}  0.0363
1         train   51600/102467 323:47/319:11      7.718         {"mlm": 7.719262042045593, "mse": 0.0}  0.0368
1         train   51700/102467 324:23/318:32      7.718         {"mlm": 7.719040159337661, "mse": 0.0}  0.0325
1         train   51800/102467 325:00/317:53      7.718         {"mlm": 7.719241775141822, "mse": 0.0}  0.0375
1         train   51900/102467 325:36/317:14      7.718         {"mlm": 7.718570101386622, "mse": 0.0}  0.0323
1         train   52000/102467 326:12/316:35      7.718         {"mlm": 7.718929609298706, "mse": 0.0}  0.0409
1         train   52100/102467 326:48/315:56      7.718         {"mlm": 7.679738039922232, "mse": 0.0}  0.0425
1         train   52200/102467 327:24/315:17      7.718         {"mlm": 7.71046538089388, "mse": 0.0}  0.0365
1         train   52300/102467 328:01/314:38      7.718         {"mlm": 7.715314482366759, "mse": 0.0}  0.0397
1         train   52400/102467 328:37/313:59      7.718         {"mlm": 7.719545421743751, "mse": 0.0}  0.0392
1         train   52500/102467 329:13/313:20      7.718         {"mlm": 7.719660512431112, "mse": 0.0}  0.0381
1         train   52600/102467 329:49/312:41      7.718         {"mlm": 7.718814107133869, "mse": 0.0}  0.0454
1         train   52700/102467 330:25/312:02      7.718         {"mlm": 7.716559685009914, "mse": 0.0}  0.0362
1         train   52800/102467 331:02/311:23      7.718         {"mlm": 7.715963141640674, "mse": 0.0}  0.0364
1         train   52900/102467 331:38/310:44      7.718         {"mlm": 7.7157881246658, "mse": 0.0}  0.0372
1         train   53000/102467 332:14/310:05      7.718         {"mlm": 7.716341242059931, "mse": 0.0}  0.0419
1         train   53100/102467 332:50/309:26      7.718         {"mlm": 7.715476683424862, "mse": 0.0}  0.0404
1         train   53200/102467 333:26/308:47      7.718         {"mlm": 7.716554284592884, "mse": 0.0}  0.033
1         train   53300/102467 334:03/308:08      7.718         {"mlm": 7.716555568601096, "mse": 0.0}  0.0367
1         train   53400/102467 334:39/307:30      7.718         {"mlm": 7.716738693027346, "mse": 0.0}  0.0403
1         train   53500/102467 335:15/306:51      7.718         {"mlm": 7.717284101100665, "mse": 0.0}  0.0327
1         train   53600/102467 335:51/306:12      7.718         {"mlm": 7.717502659302044, "mse": 0.0}  0.034
1         train   53700/102467 336:27/305:33      7.718         {"mlm": 7.717395835795635, "mse": 0.0}  0.0348
1         train   53800/102467 337:04/304:54      7.718         {"mlm": 7.716422859465433, "mse": 0.0}  0.0309
1         train   53900/102467 337:40/304:15      7.718         {"mlm": 7.716886587931396, "mse": 0.0}  0.0369
1         train   54000/102467 338:16/303:37      7.718         {"mlm": 7.716289275762378, "mse": 0.0}  0.0358
1         train   54100/102467 338:52/302:58      7.718         {"mlm": 7.726915670900929, "mse": 0.0}  0.0514
1         train   54200/102467 339:29/302:19      7.718         {"mlm": 7.713707803475736, "mse": 0.0}  0.0375
1         train   54300/102467 340:05/301:40      7.718         {"mlm": 7.7137402048046955, "mse": 0.0}  0.0522
1         train   54400/102467 340:41/301:01      7.718         {"mlm": 7.716441081396899, "mse": 0.0}  0.0375
1         train   54500/102467 341:17/300:23      7.718         {"mlm": 7.714087070710209, "mse": 0.0}  0.0396
1         train   54600/102467 341:54/299:44      7.718         {"mlm": 7.711840340923705, "mse": 0.0}  0.0404
1         train   54700/102467 342:34/299:09      7.718         {"mlm": 7.710825081199493, "mse": 0.0}  0.0358
1         train   54800/102467 343:10/298:30      7.718         {"mlm": 7.710431578165307, "mse": 0.0}  0.0397
1         train   54900/102467 343:46/297:51      7.718         {"mlm": 7.7122948036958485, "mse": 0.0}  0.0347
1         train   55000/102467 344:22/297:12      7.718         {"mlm": 7.710342837240032, "mse": 0.0}  0.0321
1         train   55100/102467 344:58/296:33      7.718         {"mlm": 7.711509486582327, "mse": 0.0}  0.0361
1         train   55200/102467 345:34/295:55      7.718         {"mlm": 7.71184041464269, "mse": 0.0}  0.0348
1         train   55300/102467 346:11/295:16      7.718         {"mlm": 7.7119049963488235, "mse": 0.0}  0.0357
1         train   55400/102467 346:47/294:37      7.718         {"mlm": 7.7108396378709525, "mse": 0.0}  0.0322
1         train   55500/102467 347:23/293:59      7.718         {"mlm": 7.710702734732023, "mse": 0.0}  0.0351
1         train   55600/102467 347:59/293:20      7.718         {"mlm": 7.712349188641105, "mse": 0.0}  0.0366
1         train   55700/102467 348:36/292:41      7.718         {"mlm": 7.712269175038601, "mse": 0.0}  0.0415
1         train   55800/102467 349:12/292:03      7.718         {"mlm": 7.713126501596809, "mse": 0.0}  0.031
1         train   55900/102467 349:48/291:24      7.718         {"mlm": 7.713718642927196, "mse": 0.0}  0.0365
1         train   56000/102467 350:24/290:45      7.718         {"mlm": 7.71362377477957, "mse": 0.0}  0.0395
1         train   56100/102467 351:00/290:06      7.718         {"mlm": 7.701125439909315, "mse": 0.0}  0.0404
1         train   56200/102467 351:37/289:28      7.718         {"mlm": 7.712907159388973, "mse": 0.0}  0.0343
1         train   56300/102467 352:13/288:49      7.718         {"mlm": 7.720621985618514, "mse": 0.0}  0.04
1         train   56400/102467 352:49/288:11      7.718         {"mlm": 7.724494295096218, "mse": 0.0}  0.0409
1         train   56500/102467 353:26/287:32      7.718         {"mlm": 7.728172590075364, "mse": 0.0}  0.0385
1         train   56600/102467 354:02/286:54      7.718         {"mlm": 7.727024923417237, "mse": 0.0}  0.0382
1         train   56700/102467 354:38/286:15      7.718         {"mlm": 7.724852600945975, "mse": 0.0}  0.0358
1         train   56800/102467 355:14/285:37      7.718         {"mlm": 7.722165415846419, "mse": 0.0}  0.0349
1         train   56900/102467 355:51/284:58      7.718         {"mlm": 7.720479528773722, "mse": 0.0}  0.0322
1         train   57000/102467 356:27/284:20      7.718         {"mlm": 7.7224330557743786, "mse": 0.0}  0.0425
1         train   57100/102467 357:03/283:41      7.718         {"mlm": 7.723616603514012, "mse": 0.0}  0.0411
1         train   57200/102467 357:40/283:03      7.718         {"mlm": 7.721652628086763, "mse": 0.0}  0.0353
1         train   57300/102467 358:16/282:24      7.718         {"mlm": 7.720143473690624, "mse": 0.0}  0.0304
1         train   57400/102467 358:52/281:46      7.718         {"mlm": 7.72095970981191, "mse": 0.0}  0.0345
1         train   57500/102467 359:28/281:07      7.718         {"mlm": 7.7195897404958025, "mse": 0.0}  0.0343
1         train   57600/102467 360:05/280:29      7.718         {"mlm": 7.718774179256776, "mse": 0.0}  0.0352
1         train   57700/102467 360:41/279:50      7.718         {"mlm": 7.717913112010967, "mse": 0.0}  0.0411
1         train   57800/102467 361:17/279:12      7.718         {"mlm": 7.7180289574709615, "mse": 0.0}  0.0403
1         train   57900/102467 361:53/278:33      7.718         {"mlm": 7.718001670817293, "mse": 0.0}  0.0375
1         train   58000/102467 362:29/277:55      7.718         {"mlm": 7.718558856350933, "mse": 0.0}  0.036
1         train   58100/102467 363:06/277:16      7.718         {"mlm": 7.737951422731082, "mse": 0.0}  0.0371
1         train   58200/102467 363:42/276:38      7.718         {"mlm": 7.74051270922836, "mse": 0.0}  0.0343
1         train   58300/102467 364:18/275:59      7.718         {"mlm": 7.733659515509734, "mse": 0.0}  0.0429
1         train   58400/102467 364:54/275:21      7.718         {"mlm": 7.731062060654765, "mse": 0.0}  0.035
1         train   58500/102467 365:31/274:42      7.718         {"mlm": 7.728825733546288, "mse": 0.0}  0.0343
1         train   58600/102467 366:07/274:04      7.718         {"mlm": 7.722922774769316, "mse": 0.0}  0.0399
1         train   58700/102467 366:43/273:25      7.718         {"mlm": 7.722812405262871, "mse": 0.0}  0.0364
1         train   58800/102467 367:19/272:47      7.718         {"mlm": 7.722073632868091, "mse": 0.0}  0.0323
1         train   58900/102467 367:55/272:09      7.718         {"mlm": 7.719238394605262, "mse": 0.0}  0.0351
1         train   59000/102467 368:32/271:30      7.718         {"mlm": 7.719803726337999, "mse": 0.0}  0.0359
1         train   59100/102467 369:08/270:52      7.718         {"mlm": 7.720874399599368, "mse": 0.0}  0.0529
1         train   59200/102467 369:44/270:13      7.718         {"mlm": 7.7197542023100585, "mse": 0.0}  0.0394
1         train   59300/102467 370:20/269:35      7.718         {"mlm": 7.721137507462207, "mse": 0.0}  0.0315
1         train   59400/102467 370:57/268:57      7.718         {"mlm": 7.719210394132444, "mse": 0.0}  0.039
1         train   59500/102467 371:33/268:18      7.718         {"mlm": 7.720158813790204, "mse": 0.0}  0.0356
1         train   59600/102467 372:09/267:40      7.718         {"mlm": 7.721793908821909, "mse": 0.0}  0.039
1         train   59700/102467 372:45/267:02      7.718         {"mlm": 7.721817804957336, "mse": 0.0}  0.0447
1         train   59800/102467 373:21/266:23      7.718         {"mlm": 7.721241412024721, "mse": 0.0}  0.0363
1         train   59900/102467 373:58/265:45      7.718         {"mlm": 7.721306389645685, "mse": 0.0}  0.041
1         train   60000/102467 374:34/265:07      7.718         {"mlm": 7.721901755055827, "mse": 0.0}  0.042

09/25/2022 00:09:02 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step60000.pkl
1         valid   1/781        0:21/283:04        7.596           None
1         valid   101/781      0:39/ 4:28         7.713           None
1         valid   201/781      0:55/ 2:39         7.717           None
1         valid   301/781      1:10/ 1:52         7.717           None
1         valid   401/781      1:25/ 1:21         7.717           None
1         valid   501/781      1:41/ 0:56         7.711           None
1         valid   601/781      1:56/ 0:34         7.709           None
1         valid   701/781      2:11/ 0:15         7.713           None
1         valid   781/781      2:24/ 0:00         7.720         {"mlm": 7.7202304737516005, "mse": 0.0, "train": 0.0}  None
1         train   60100/102467 377:34/266:10      7.718         {"mlm": 7.723246154785156, "mse": 0.0}  0.0357
1         train   60200/102467 378:10/265:31      7.718         {"mlm": 7.710457994937896, "mse": 0.0}  0.033
1         train   60300/102467 378:46/264:52      7.718         {"mlm": 7.718566846847534, "mse": 0.0}  0.036
1         train   60400/102467 379:22/264:13      7.718         {"mlm": 7.720278514623642, "mse": 0.0}  0.0365
1         train   60500/102467 379:58/263:34      7.718         {"mlm": 7.719983232498169, "mse": 0.0}  0.0332
1         train   60600/102467 380:34/262:55      7.718         {"mlm": 7.719685975710551, "mse": 0.0}  0.0351
1         train   60700/102467 381:10/262:17      7.718         {"mlm": 7.722441445078169, "mse": 0.0}  0.0335
1         train   60800/102467 381:46/261:38      7.718         {"mlm": 7.726941637396813, "mse": 0.0}  0.0329
1         train   60900/102467 382:22/260:59      7.718         {"mlm": 7.726928822729323, "mse": 0.0}  0.0455
1         train   61000/102467 382:58/260:20      7.718         {"mlm": 7.724561434268951, "mse": 0.0}  0.0342
1         train   61100/102467 383:35/259:42      7.718         {"mlm": 7.723749009912664, "mse": 0.0}  0.0341
1         train   61200/102467 384:11/259:03      7.718         {"mlm": 7.7226914354165395, "mse": 0.0}  0.0361
1         train   61300/102467 384:47/258:24      7.718         {"mlm": 7.7226313675366915, "mse": 0.0}  0.0353
1         train   61400/102467 385:23/257:46      7.718         {"mlm": 7.723029420716422, "mse": 0.0}  0.0326
1         train   61500/102467 385:59/257:07      7.718         {"mlm": 7.722973720232646, "mse": 0.0}  0.0344
1         train   61600/102467 386:35/256:28      7.718         {"mlm": 7.722302844524384, "mse": 0.0}  0.0386
1         train   61700/102467 387:12/255:50      7.718         {"mlm": 7.7218197522443885, "mse": 0.0}  0.0353
1         train   61800/102467 387:48/255:11      7.718         {"mlm": 7.723241563108232, "mse": 0.0}  0.0356
1         train   61900/102467 388:24/254:32      7.718         {"mlm": 7.723235613170423, "mse": 0.0}  0.0319
1         train   62000/102467 389:00/253:54      7.718         {"mlm": 7.72291510272026, "mse": 0.0}  0.0322
1         train   62100/102467 389:36/253:15      7.718         {"mlm": 7.7219130246326175, "mse": 0.0}  0.0359
1         train   62200/102467 390:13/252:37      7.718         {"mlm": 7.714143604489427, "mse": 0.0}  0.0383
1         train   62300/102467 390:49/251:58      7.718         {"mlm": 7.713033840409091, "mse": 0.0}  0.0349
1         train   62400/102467 391:25/251:20      7.718         {"mlm": 7.708761791238809, "mse": 0.0}  0.0363
1         train   62500/102467 392:01/250:41      7.718         {"mlm": 7.71005956825608, "mse": 0.0}  0.035
1         train   62600/102467 392:37/250:02      7.718         {"mlm": 7.708144932239004, "mse": 0.0}  0.0389
1         train   62700/102467 393:14/249:24      7.718         {"mlm": 7.707425782608883, "mse": 0.0}  0.0314
1         train   62800/102467 393:50/248:45      7.718         {"mlm": 7.710276936708911, "mse": 0.0}  0.0351
1         train   62900/102467 394:26/248:07      7.718         {"mlm": 7.710042433690972, "mse": 0.0}  0.0402
1         train   63000/102467 395:02/247:28      7.718         {"mlm": 7.711276567972697, "mse": 0.0}  0.0353
1         train   63100/102467 395:38/246:50      7.718         {"mlm": 7.711890699648662, "mse": 0.0}  0.0321
1         train   63200/102467 396:15/246:11      7.718         {"mlm": 7.711680038458511, "mse": 0.0}  0.0306
1         train   63300/102467 396:51/245:33      7.718         {"mlm": 7.71234198989457, "mse": 0.0}  0.0316
1         train   63400/102467 397:27/244:54      7.718         {"mlm": 7.711917377864573, "mse": 0.0}  0.0321
1         train   63500/102467 398:03/244:16      7.718         {"mlm": 7.712686355468668, "mse": 0.0}  0.0351
1         train   63600/102467 398:40/243:38      7.718         {"mlm": 7.712776647797967, "mse": 0.0}  0.0346
1         train   63700/102467 399:16/242:59      7.718         {"mlm": 7.71240477592823, "mse": 0.0}  0.0352
1         train   63800/102467 399:52/242:21      7.718         {"mlm": 7.711970720773541, "mse": 0.0}  0.0339
1         train   63900/102467 400:28/241:42      7.718         {"mlm": 7.71313803418176, "mse": 0.0}  0.0341
1         train   64000/102467 401:05/241:04      7.718         {"mlm": 7.714315721188384, "mse": 0.0}  0.0331
1         train   64100/102467 401:41/240:25      7.718         {"mlm": 7.708102756617021, "mse": 0.0}  0.0343
1         train   64200/102467 402:17/239:47      7.718         {"mlm": 7.720497651533647, "mse": 0.0}  0.0347
1         train   64300/102467 402:53/239:08      7.718         {"mlm": 7.720340011903904, "mse": 0.0}  0.034
1         train   64400/102467 403:29/238:30      7.718         {"mlm": 7.712323198366405, "mse": 0.0}  0.0382
1         train   64500/102467 404:06/237:52      7.718         {"mlm": 7.710245316287121, "mse": 0.0}  0.0336
1         train   64600/102467 404:42/237:13      7.718         {"mlm": 7.712800982803804, "mse": 0.0}  0.0336
1         train   64700/102467 405:18/236:35      7.718         {"mlm": 7.710697273128695, "mse": 0.0}  0.0321
1         train   64800/102467 405:54/235:56      7.718         {"mlm": 7.711864274247248, "mse": 0.0}  0.0373
1         train   64900/102467 406:31/235:18      7.718         {"mlm": 7.712721093991286, "mse": 0.0}  0.0319
1         train   65000/102467 407:07/234:40      7.718         {"mlm": 7.713979494117782, "mse": 0.0}  0.0357
1         train   65100/102467 407:43/234:01      7.718         {"mlm": 7.712135371831814, "mse": 0.0}  0.0414
1         train   65200/102467 408:19/233:23      7.718         {"mlm": 7.712529942268919, "mse": 0.0}  0.0412
1         train   65300/102467 408:56/232:45      7.718         {"mlm": 7.713683929208247, "mse": 0.0}  0.0419
1         train   65400/102467 409:32/232:06      7.718         {"mlm": 7.713893260737516, "mse": 0.0}  0.0368
1         train   65500/102467 410:08/231:28      7.718         {"mlm": 7.712726218041495, "mse": 0.0}  0.0325
1         train   65600/102467 410:44/230:50      7.718         {"mlm": 7.713854608308986, "mse": 0.0}  0.0343
1         train   65700/102467 411:21/230:11      7.718         {"mlm": 7.713603847580046, "mse": 0.0}  0.0331
1         train   65800/102467 411:57/229:33      7.718         {"mlm": 7.713640579790109, "mse": 0.0}  0.0319
1         train   65900/102467 412:33/228:55      7.718         {"mlm": 7.713407833533493, "mse": 0.0}  0.04
1         train   66000/102467 413:09/228:17      7.718         {"mlm": 7.714086528774257, "mse": 0.0}  0.0364
1         train   66100/102467 413:46/227:38      7.718         {"mlm": 7.7330553703701375, "mse": 0.0}  0.0298
1         train   66200/102467 414:22/227:00      7.718         {"mlm": 7.727817811336614, "mse": 0.0}  0.0376
1         train   66300/102467 414:58/226:22      7.718         {"mlm": 7.72816751139734, "mse": 0.0}  0.0344
1         train   66400/102467 415:35/225:44      7.718         {"mlm": 7.72433916567555, "mse": 0.0}  0.0354
1         train   66500/102467 416:11/225:06      7.718         {"mlm": 7.727529327154639, "mse": 0.0}  0.0441
1         train   66600/102467 416:47/224:27      7.718         {"mlm": 7.726487162125171, "mse": 0.0}  0.0393
1         train   66700/102467 417:24/223:49      7.718         {"mlm": 7.727502988434932, "mse": 0.0}  0.0322
1         train   66800/102467 418:00/223:11      7.718         {"mlm": 7.72702304512225, "mse": 0.0}  0.0408
1         train   66900/102467 418:37/222:33      7.718         {"mlm": 7.727558353407061, "mse": 0.0}  0.0348
1         train   67000/102467 419:13/221:55      7.718         {"mlm": 7.727309561301856, "mse": 0.0}  0.0326
1         train   67100/102467 419:49/221:16      7.718         {"mlm": 7.72688568779416, "mse": 0.0}  0.0386
1         train   67200/102467 420:25/220:38      7.718         {"mlm": 7.7264673815434834, "mse": 0.0}  0.0396
1         train   67300/102467 421:02/220:00      7.718         {"mlm": 7.722807690834393, "mse": 0.0}  0.0305
1         train   67400/102467 421:38/219:22      7.718         {"mlm": 7.722161296784408, "mse": 0.0}  0.0421
1         train   67500/102467 422:14/218:44      7.718         {"mlm": 7.722154806834025, "mse": 0.0}  0.038
1         train   67600/102467 422:50/218:05      7.718         {"mlm": 7.722755228495254, "mse": 0.0}  0.0338
1         train   67700/102467 423:27/217:27      7.718         {"mlm": 7.722222928939037, "mse": 0.0}  0.0347
1         train   67800/102467 424:03/216:49      7.718         {"mlm": 7.720831079222988, "mse": 0.0}  0.0392
1         train   67900/102467 424:39/216:11      7.718         {"mlm": 7.722165627296057, "mse": 0.0}  0.0343
1         train   68000/102467 425:15/215:33      7.718         {"mlm": 7.722460078667806, "mse": 0.0}  0.0357
1         train   68100/102467 425:52/214:55      7.718         {"mlm": 7.716984833280246, "mse": 0.0}  0.0371
1         train   68200/102467 426:28/214:16      7.718         {"mlm": 7.721492611632055, "mse": 0.0}  0.0305
1         train   68300/102467 427:04/213:38      7.718         {"mlm": 7.717068860659728, "mse": 0.0}  0.032
1         train   68400/102467 427:40/213:00      7.718         {"mlm": 7.713634872677351, "mse": 0.0}  0.0345
1         train   68500/102467 428:17/212:22      7.718         {"mlm": 7.716383783086654, "mse": 0.0}  0.0326
1         train   68600/102467 428:53/211:44      7.718         {"mlm": 7.713805349081155, "mse": 0.0}  0.0332
1         train   68700/102467 429:29/211:06      7.718         {"mlm": 7.713680392024161, "mse": 0.0}  0.038
1         train   68800/102467 430:06/210:28      7.718         {"mlm": 7.715413948399338, "mse": 0.0}  0.0314
1         train   68900/102467 430:42/209:50      7.718         {"mlm": 7.714890806802681, "mse": 0.0}  0.0372
1         train   69000/102467 431:18/209:11      7.718         {"mlm": 7.71658549347077, "mse": 0.0}  0.037
1         train   69100/102467 431:55/208:33      7.718         {"mlm": 7.716398929157396, "mse": 0.0}  0.0391
1         train   69200/102467 432:31/207:55      7.718         {"mlm": 7.715993137662626, "mse": 0.0}  0.0509
1         train   69300/102467 433:07/207:17      7.718         {"mlm": 7.715888705150581, "mse": 0.0}  0.0372
1         train   69400/102467 433:43/206:39      7.718         {"mlm": 7.7154545719097545, "mse": 0.0}  0.0383
1         train   69500/102467 434:19/206:01      7.718         {"mlm": 7.716510646483478, "mse": 0.0}  0.033
1         train   69600/102467 434:56/205:23      7.718         {"mlm": 7.716949581203604, "mse": 0.0}  0.0417
1         train   69700/102467 435:32/204:45      7.718         {"mlm": 7.716095007253143, "mse": 0.0}  0.0374
1         train   69800/102467 436:08/204:07      7.718         {"mlm": 7.715965927576435, "mse": 0.0}  0.0349
1         train   69900/102467 436:44/203:28      7.718         {"mlm": 7.716284839412834, "mse": 0.0}  0.0356
1         train   70000/102467 437:20/202:50      7.718         {"mlm": 7.716102384613129, "mse": 0.0}  0.0325

09/25/2022 01:11:49 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step70000.pkl
1         valid   1/781        0:19/258:42        7.708           None
1         valid   101/781      0:35/ 3:57         7.703           None
1         valid   201/781      0:50/ 2:25         7.721           None
1         valid   301/781      1:05/ 1:44         7.720           None
1         valid   401/781      1:21/ 1:16         7.729           None
1         valid   501/781      1:36/ 0:53         7.725           None
1         valid   601/781      1:51/ 0:33         7.725           None
1         valid   701/781      2:07/ 0:14         7.725           None
1         valid   781/781      2:20/ 0:00         7.721         {"mlm": 7.720550576184379, "mse": 0.0, "train": 0.0}  None
1         train   70100/102467 440:17/203:17      7.718         {"mlm": 7.730880150794983, "mse": 0.0}  0.0342
1         train   70200/102467 440:53/202:39      7.718         {"mlm": 7.726706252098084, "mse": 0.0}  0.0377
1         train   70300/102467 441:29/202:00      7.718         {"mlm": 7.731872469584147, "mse": 0.0}  0.0369
1         train   70400/102467 442:05/201:22      7.718         {"mlm": 7.73175656914711, "mse": 0.0}  0.0361
1         train   70500/102467 442:41/200:44      7.718         {"mlm": 7.732918452262878, "mse": 0.0}  0.0319
1         train   70600/102467 443:17/200:05      7.718         {"mlm": 7.730046396255493, "mse": 0.0}  0.0345
1         train   70700/102467 443:54/199:27      7.718         {"mlm": 7.7267980425698415, "mse": 0.0}  0.0353
1         train   70800/102467 444:30/198:48      7.718         {"mlm": 7.721329962611199, "mse": 0.0}  0.0379
1         train   70900/102467 445:06/198:10      7.718         {"mlm": 7.719203204049005, "mse": 0.0}  0.0347
1         train   71000/102467 445:42/197:32      7.718         {"mlm": 7.718863382339477, "mse": 0.0}  0.0349
1         train   71100/102467 446:18/196:53      7.718         {"mlm": 7.719136106751182, "mse": 0.0}  0.0366
1         train   71200/102467 446:54/196:15      7.718         {"mlm": 7.719678568442663, "mse": 0.0}  0.0321
1         train   71300/102467 447:31/195:37      7.718         {"mlm": 7.718145897938655, "mse": 0.0}  0.0364
1         train   71400/102467 448:07/194:59      7.718         {"mlm": 7.718980257511139, "mse": 0.0}  0.0453
1         train   71500/102467 448:43/194:20      7.718         {"mlm": 7.719442229906718, "mse": 0.0}  0.0347
1         train   71600/102467 449:19/193:42      7.718         {"mlm": 7.71887308806181, "mse": 0.0}  0.031
1         train   71700/102467 449:56/193:04      7.718         {"mlm": 7.719481465676251, "mse": 0.0}  0.0364
1         train   71800/102467 450:32/192:26      7.718         {"mlm": 7.720120106273227, "mse": 0.0}  0.0355
1         train   71900/102467 451:08/191:47      7.718         {"mlm": 7.720161666368183, "mse": 0.0}  0.0393
1         train   72000/102467 451:45/191:09      7.718         {"mlm": 7.719317268848419, "mse": 0.0}  0.0323
1         train   72100/102467 452:21/190:31      7.718         {"mlm": 7.716604112374662, "mse": 0.0}  0.0339
1         train   72200/102467 452:57/189:53      7.718         {"mlm": 7.72239050314055, "mse": 0.0}  0.0375
1         train   72300/102467 453:34/189:15      7.718         {"mlm": 7.728689042222141, "mse": 0.0}  0.0322
1         train   72400/102467 454:10/188:36      7.718         {"mlm": 7.725208388832876, "mse": 0.0}  0.0379
1         train   72500/102467 454:46/187:58      7.718         {"mlm": 7.724250595650835, "mse": 0.0}  0.0382
1         train   72600/102467 455:22/187:20      7.718         {"mlm": 7.723555301386048, "mse": 0.0}  0.0358
1         train   72700/102467 455:59/186:42      7.718         {"mlm": 7.724334149913215, "mse": 0.0}  0.0393
1         train   72800/102467 456:35/186:03      7.718         {"mlm": 7.7218490745009705, "mse": 0.0}  0.0359
1         train   72900/102467 457:11/185:25      7.718         {"mlm": 7.721764821762238, "mse": 0.0}  0.0339
1         train   73000/102467 457:47/184:47      7.718         {"mlm": 7.719920558853073, "mse": 0.0}  0.0349
1         train   73100/102467 458:24/184:09      7.718         {"mlm": 7.719269368082312, "mse": 0.0}  0.0348
1         train   73200/102467 459:00/183:31      7.718         {"mlm": 7.717525039542408, "mse": 0.0}  0.0361
1         train   73300/102467 459:36/182:53      7.718         {"mlm": 7.720227607495056, "mse": 0.0}  0.0322
1         train   73400/102467 460:12/182:14      7.718         {"mlm": 7.720027712262299, "mse": 0.0}  0.0343
1         train   73500/102467 460:48/181:36      7.718         {"mlm": 7.718967086557868, "mse": 0.0}  0.0375
1         train   73600/102467 461:25/180:58      7.718         {"mlm": 7.720244407355599, "mse": 0.0}  0.0337
1         train   73700/102467 462:01/180:20      7.718         {"mlm": 7.719850672070176, "mse": 0.0}  0.0351
1         train   73800/102467 462:37/179:42      7.718         {"mlm": 7.719310052266314, "mse": 0.0}  0.0339
1         train   73900/102467 463:13/179:04      7.718         {"mlm": 7.719166179403874, "mse": 0.0}  0.036
1         train   74000/102467 463:50/178:25      7.718         {"mlm": 7.719816120819428, "mse": 0.0}  0.0351
1         train   74100/102467 464:26/177:47      7.718         {"mlm": 7.729452931151098, "mse": 0.0}  0.0348
1         train   74200/102467 465:02/177:09      7.718         {"mlm": 7.725726965701941, "mse": 0.0}  0.033
1         train   74300/102467 465:38/176:31      7.718         {"mlm": 7.730976847194185, "mse": 0.0}  0.0348
1         train   74400/102467 466:14/175:53      7.718         {"mlm": 7.722506313467744, "mse": 0.0}  0.0356
1         train   74500/102467 466:51/175:15      7.718         {"mlm": 7.726050055170634, "mse": 0.0}  0.0327
1         train   74600/102467 467:27/174:37      7.718         {"mlm": 7.7269930919277225, "mse": 0.0}  0.0379
1         train   74700/102467 468:03/173:58      7.718         {"mlm": 7.724030248073589, "mse": 0.0}  0.047
1         train   74800/102467 468:39/173:20      7.718         {"mlm": 7.725603448418448, "mse": 0.0}  0.0391
1         train   74900/102467 469:15/172:42      7.718         {"mlm": 7.7264696202989676, "mse": 0.0}  0.0333
1         train   75000/102467 469:52/172:04      7.718         {"mlm": 7.724992332573167, "mse": 0.0}  0.0366
1         train   75100/102467 470:28/171:26      7.718         {"mlm": 7.724310628269105, "mse": 0.0}  0.0345
1         train   75200/102467 471:04/170:48      7.718         {"mlm": 7.72290111224918, "mse": 0.0}  0.0392
1         train   75300/102467 471:40/170:10      7.718         {"mlm": 7.7216844599125745, "mse": 0.0}  0.0394
1         train   75400/102467 472:17/169:32      7.718         {"mlm": 7.721711190814453, "mse": 0.0}  0.0355
1         train   75500/102467 472:53/168:54      7.718         {"mlm": 7.721290759951155, "mse": 0.0}  0.0319
1         train   75600/102467 473:29/168:16      7.718         {"mlm": 7.7234503100900085, "mse": 0.0}  0.0335
1         train   75700/102467 474:05/167:38      7.718         {"mlm": 7.7223382703828305, "mse": 0.0}  0.0326
1         train   75800/102467 474:42/167:00      7.718         {"mlm": 7.722298585267963, "mse": 0.0}  0.0334
1         train   75900/102467 475:18/166:22      7.718         {"mlm": 7.722644452928115, "mse": 0.0}  0.0295
1         train   76000/102467 475:54/165:44      7.718         {"mlm": 7.72271695008149, "mse": 0.0}  0.0338
1         train   76100/102467 476:30/165:06      7.718         {"mlm": 7.7186540230033325, "mse": 0.0}  0.0325
1         train   76200/102467 477:06/164:27      7.718         {"mlm": 7.7028244618837, "mse": 0.0}  0.0345
1         train   76300/102467 477:42/163:49      7.718         {"mlm": 7.7066129193161474, "mse": 0.0}  0.0447
1         train   76400/102467 478:19/163:11      7.718         {"mlm": 7.705445528630946, "mse": 0.0}  0.0327
1         train   76500/102467 478:55/162:33      7.718         {"mlm": 7.7086878014762155, "mse": 0.0}  0.0353
1         train   76600/102467 479:31/161:55      7.718         {"mlm": 7.7116700225139985, "mse": 0.0}  0.0325
1         train   76700/102467 480:07/161:17      7.718         {"mlm": 7.713258676241597, "mse": 0.0}  0.0349
1         train   76800/102467 480:43/160:39      7.718         {"mlm": 7.712303979483572, "mse": 0.0}  0.0335
1         train   76900/102467 481:20/160:01      7.718         {"mlm": 7.712193426878556, "mse": 0.0}  0.0281
1         train   77000/102467 481:56/159:23      7.718         {"mlm": 7.711869751558141, "mse": 0.0}  0.0331
1         train   77100/102467 482:32/158:45      7.718         {"mlm": 7.711897082837365, "mse": 0.0}  0.0372
1         train   77200/102467 483:08/158:07      7.718         {"mlm": 7.714145419790831, "mse": 0.0}  0.0326
1         train   77300/102467 483:44/157:29      7.718         {"mlm": 7.715165900007614, "mse": 0.0}  0.033
1         train   77400/102467 484:21/156:51      7.718         {"mlm": 7.716109292201682, "mse": 0.0}  0.0381
1         train   77500/102467 484:57/156:13      7.718         {"mlm": 7.71750517757877, "mse": 0.0}  0.037
1         train   77600/102467 485:33/155:35      7.718         {"mlm": 7.717541979787942, "mse": 0.0}  0.0312
1         train   77700/102467 486:09/154:57      7.718         {"mlm": 7.7179302171741995, "mse": 0.0}  0.0395
1         train   77800/102467 486:45/154:19      7.718         {"mlm": 7.718589574944396, "mse": 0.0}  0.0306
1         train   77900/102467 487:22/153:41      7.718         {"mlm": 7.718007586161966, "mse": 0.0}  0.0382
1         train   78000/102467 487:58/153:04      7.718         {"mlm": 7.718844758312166, "mse": 0.0}  0.0352
1         train   78100/102467 488:34/152:26      7.718         {"mlm": 7.728169277310371, "mse": 0.0}  0.0325
1         train   78200/102467 489:10/151:48      7.718         {"mlm": 7.725371338883225, "mse": 0.0}  0.0462
1         train   78300/102467 489:47/151:10      7.718         {"mlm": 7.722289198153728, "mse": 0.0}  0.0336
1         train   78400/102467 490:23/150:32      7.719         {"mlm": 7.723552183671431, "mse": 0.0}  0.0304
1         train   78500/102467 490:59/149:54      7.719         {"mlm": 7.7263054193988925, "mse": 0.0}  0.0371
1         train   78600/102467 491:35/149:16      7.719         {"mlm": 7.725344274668085, "mse": 0.0}  0.0301
1         train   78700/102467 492:11/148:38      7.719         {"mlm": 7.727340180298378, "mse": 0.0}  0.0312
1         train   78800/102467 492:48/148:00      7.719         {"mlm": 7.726093389880118, "mse": 0.0}  0.0346
1         train   78900/102467 493:24/147:22      7.719         {"mlm": 7.723915589175054, "mse": 0.0}  0.0298
1         train   79000/102467 494:00/146:44      7.719         {"mlm": 7.7248838943649965, "mse": 0.0}  0.0313
1         train   79100/102467 494:36/146:06      7.719         {"mlm": 7.725537618146325, "mse": 0.0}  0.0305
1         train   79200/102467 495:12/145:28      7.719         {"mlm": 7.725571635973494, "mse": 0.0}  0.0307
1         train   79300/102467 495:49/144:51      7.719         {"mlm": 7.723875698116091, "mse": 0.0}  0.0332
1         train   79400/102467 496:25/144:13      7.719         {"mlm": 7.725915832300924, "mse": 0.0}  0.0332
1         train   79500/102467 497:01/143:35      7.719         {"mlm": 7.725766052218044, "mse": 0.0}  0.0369
1         train   79600/102467 497:37/142:57      7.719         {"mlm": 7.726044823651326, "mse": 0.0}  0.035
1         train   79700/102467 498:14/142:19      7.719         {"mlm": 7.725763455314456, "mse": 0.0}  0.0296
1         train   79800/102467 498:50/141:41      7.719         {"mlm": 7.725503616980827, "mse": 0.0}  0.0317
1         train   79900/102467 499:26/141:03      7.719         {"mlm": 7.724954653892839, "mse": 0.0}  0.0379
1         train   80000/102467 500:03/140:26      7.719         {"mlm": 7.724509359600549, "mse": 0.0}  0.0329

09/25/2022 02:14:31 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step80000.pkl
1         valid   1/781        0:19/259:02        7.709           None
1         valid   101/781      0:35/ 3:56         7.706           None
1         valid   201/781      0:50/ 2:25         7.712           None
1         valid   301/781      1:05/ 1:44         7.715           None
1         valid   401/781      1:21/ 1:16         7.719           None
1         valid   501/781      1:36/ 0:53         7.716           None
1         valid   601/781      1:51/ 0:33         7.716           None
1         valid   701/781      2:06/ 0:14         7.715           None
1         valid   781/781      2:21/ 0:00         7.720         {"mlm": 7.719590268886043, "mse": 0.0, "train": 0.0}  None
1         train   80100/102467 503:00/140:27      7.719         {"mlm": 7.712564706802368, "mse": 0.0}  0.0333
1         train   80200/102467 503:36/139:49      7.719         {"mlm": 7.719825112819672, "mse": 0.0}  0.0359
1         train   80300/102467 504:12/139:11      7.719         {"mlm": 7.721092375119527, "mse": 0.0}  0.0363
1         train   80400/102467 504:48/138:33      7.719         {"mlm": 7.721362422704697, "mse": 0.0}  0.0308
1         train   80500/102467 505:24/137:54      7.719         {"mlm": 7.722308452606201, "mse": 0.0}  0.0355
1         train   80600/102467 506:00/137:16      7.719         {"mlm": 7.719895399411519, "mse": 0.0}  0.0368
1         train   80700/102467 506:36/136:38      7.719         {"mlm": 7.719602943147931, "mse": 0.0}  0.0333
1         train   80800/102467 507:12/136:00      7.719         {"mlm": 7.721073411107064, "mse": 0.0}  0.0307
1         train   80900/102467 507:48/135:22      7.719         {"mlm": 7.719081708590189, "mse": 0.0}  0.0334
1         train   81000/102467 508:24/134:44      7.719         {"mlm": 7.71925486946106, "mse": 0.0}  0.0372
1         train   81100/102467 509:00/134:06      7.719         {"mlm": 7.7191171862862324, "mse": 0.0}  0.0451
1         train   81200/102467 509:37/133:28      7.719         {"mlm": 7.718339644670486, "mse": 0.0}  0.0372
1         train   81300/102467 510:13/132:50      7.719         {"mlm": 7.718844536634592, "mse": 0.0}  0.0358
1         train   81400/102467 510:49/132:12      7.719         {"mlm": 7.719640817301614, "mse": 0.0}  0.0359
1         train   81500/102467 511:25/131:34      7.719         {"mlm": 7.719167041778564, "mse": 0.0}  0.0326
1         train   81600/102467 512:01/130:56      7.719         {"mlm": 7.718501975536347, "mse": 0.0}  0.037
1         train   81700/102467 512:37/130:18      7.719         {"mlm": 7.718032571848701, "mse": 0.0}  0.0393
1         train   81800/102467 513:14/129:40      7.719         {"mlm": 7.718412420219845, "mse": 0.0}  0.0313
1         train   81900/102467 513:50/129:02      7.719         {"mlm": 7.718718566643564, "mse": 0.0}  0.0335
1         train   82000/102467 514:26/128:24      7.719         {"mlm": 7.718120323181152, "mse": 0.0}  0.0322
1         train   82100/102467 515:02/127:46      7.719         {"mlm": 7.720273894493026, "mse": 0.0}  0.0311
1         train   82200/102467 515:39/127:08      7.719         {"mlm": 7.730370559884076, "mse": 0.0}  0.0371
1         train   82300/102467 516:15/126:30      7.719         {"mlm": 7.721596193154121, "mse": 0.0}  0.0342
1         train   82400/102467 516:51/125:52      7.719         {"mlm": 7.721803142910912, "mse": 0.0}  0.0331
1         train   82500/102467 517:27/125:14      7.719         {"mlm": 7.717226588415478, "mse": 0.0}  0.0302
1         train   82600/102467 518:03/124:36      7.719         {"mlm": 7.717981555028033, "mse": 0.0}  0.0366
1         train   82700/102467 518:40/123:58      7.719         {"mlm": 7.720530182506905, "mse": 0.0}  0.0302
1         train   82800/102467 519:16/123:20      7.719         {"mlm": 7.720443553709715, "mse": 0.0}  0.0399
1         train   82900/102467 519:52/122:42      7.719         {"mlm": 7.721414804193414, "mse": 0.0}  0.0338
1         train   83000/102467 520:28/122:04      7.719         {"mlm": 7.720798605555171, "mse": 0.0}  0.037
1         train   83100/102467 521:05/121:26      7.719         {"mlm": 7.720388858073185, "mse": 0.0}  0.0303
1         train   83200/102467 521:41/120:48      7.719         {"mlm": 7.7196530937850225, "mse": 0.0}  0.0318
1         train   83300/102467 522:17/120:10      7.719         {"mlm": 7.720239891099232, "mse": 0.0}  0.0309
1         train   83400/102467 522:53/119:32      7.719         {"mlm": 7.719639615555845, "mse": 0.0}  0.0389
1         train   83500/102467 523:30/118:54      7.719         {"mlm": 7.7187491630696705, "mse": 0.0}  0.0493
1         train   83600/102467 524:06/118:16      7.719         {"mlm": 7.719034216417381, "mse": 0.0}  0.0385
1         train   83700/102467 524:42/117:38      7.719         {"mlm": 7.718591750124189, "mse": 0.0}  0.0322
1         train   83800/102467 525:18/117:01      7.719         {"mlm": 7.717974929692946, "mse": 0.0}  0.032
1         train   83900/102467 525:55/116:23      7.719         {"mlm": 7.717750299975017, "mse": 0.0}  0.0345
1         train   84000/102467 526:31/115:45      7.719         {"mlm": 7.717399329289965, "mse": 0.0}  0.0328
1         train   84100/102467 527:07/115:07      7.719         {"mlm": 7.682790795151068, "mse": 0.0}  0.0299
1         train   84200/102467 527:43/114:29      7.719         {"mlm": 7.707160393397014, "mse": 0.0}  0.0305
1         train   84300/102467 528:20/113:51      7.719         {"mlm": 7.718079090118408, "mse": 0.0}  0.031
1         train   84400/102467 528:56/113:13      7.719         {"mlm": 7.7142551747997805, "mse": 0.0}  0.0342
1         train   84500/102467 529:32/112:35      7.719         {"mlm": 7.719675247927746, "mse": 0.0}  0.0332
1         train   84600/102467 530:08/111:57      7.719         {"mlm": 7.72070133725935, "mse": 0.0}  0.0339
1         train   84700/102467 530:45/111:19      7.719         {"mlm": 7.720261304631274, "mse": 0.0}  0.032
1         train   84800/102467 531:21/110:42      7.719         {"mlm": 7.719794060652118, "mse": 0.0}  0.0312
1         train   84900/102467 531:57/110:04      7.719         {"mlm": 7.71956764456955, "mse": 0.0}  0.0352
1         train   85000/102467 532:34/109:26      7.719         {"mlm": 7.719508824224223, "mse": 0.0}  0.0418
1         train   85100/102467 533:10/108:48      7.719         {"mlm": 7.720744803086439, "mse": 0.0}  0.0313
1         train   85200/102467 533:46/108:10      7.719         {"mlm": 7.719780434750158, "mse": 0.0}  0.036
1         train   85300/102467 534:23/107:32      7.719         {"mlm": 7.721019130274768, "mse": 0.0}  0.0298
1         train   85400/102467 534:59/106:55      7.719         {"mlm": 7.720120359388032, "mse": 0.0}  0.033
1         train   85500/102467 535:35/106:17      7.719         {"mlm": 7.720601494385499, "mse": 0.0}  0.0324
1         train   85600/102467 536:12/105:39      7.719         {"mlm": 7.721676213571216, "mse": 0.0}  0.0352
1         train   85700/102467 536:48/105:01      7.719         {"mlm": 7.722344900328925, "mse": 0.0}  0.0392
1         train   85800/102467 537:24/104:23      7.719         {"mlm": 7.721888667351677, "mse": 0.0}  0.0334
1         train   85900/102467 538:01/103:45      7.719         {"mlm": 7.721925940227207, "mse": 0.0}  0.031
1         train   86000/102467 538:37/103:08      7.719         {"mlm": 7.721685546773809, "mse": 0.0}  0.0319
1         train   86100/102467 539:13/102:30      7.719         {"mlm": 7.725149798639042, "mse": 0.0}  0.0338
1         train   86200/102467 539:49/101:52      7.719         {"mlm": 7.738780808327767, "mse": 0.0}  0.0308
1         train   86300/102467 540:25/101:14      7.719         {"mlm": 7.732732625120016, "mse": 0.0}  0.0351
1         train   86400/102467 541:02/100:36      7.719         {"mlm": 7.730266930174167, "mse": 0.0}  0.0388
1         train   86500/102467 541:38/99:58       7.719         {"mlm": 7.72464663162078, "mse": 0.0}  0.0352
1         train   86600/102467 542:14/99:21       7.719         {"mlm": 7.721363078809064, "mse": 0.0}  0.0357
1         train   86700/102467 542:50/98:43       7.719         {"mlm": 7.717981211937312, "mse": 0.0}  0.034
1         train   86800/102467 543:27/98:05       7.719         {"mlm": 7.7148900977341714, "mse": 0.0}  0.0343
1         train   86900/102467 544:03/97:27       7.719         {"mlm": 7.715710002575962, "mse": 0.0}  0.0357
1         train   87000/102467 544:39/96:49       7.719         {"mlm": 7.71465694748889, "mse": 0.0}  0.0366
1         train   87100/102467 545:15/96:12       7.719         {"mlm": 7.715537639781357, "mse": 0.0}  0.0341
1         train   87200/102467 545:52/95:34       7.719         {"mlm": 7.71500355418563, "mse": 0.0}  0.0386
1         train   87300/102467 546:28/94:56       7.719         {"mlm": 7.714715658377572, "mse": 0.0}  0.0323
1         train   87400/102467 547:04/94:18       7.719         {"mlm": 7.714530525330398, "mse": 0.0}  0.0404
1         train   87500/102467 547:40/93:40       7.719         {"mlm": 7.715002627872831, "mse": 0.0}  0.0301
1         train   87600/102467 548:17/93:03       7.719         {"mlm": 7.716012980091475, "mse": 0.0}  0.031
1         train   87700/102467 548:53/92:25       7.719         {"mlm": 7.717348547212222, "mse": 0.0}  0.0325
1         train   87800/102467 549:29/91:47       7.719         {"mlm": 7.718381437514979, "mse": 0.0}  0.0349
1         train   87900/102467 550:05/91:09       7.719         {"mlm": 7.717957402632999, "mse": 0.0}  0.0319
1         train   88000/102467 550:42/90:32       7.719         {"mlm": 7.717128133797681, "mse": 0.0}  0.0299
1         train   88100/102467 551:18/89:54       7.719         {"mlm": 7.741460154453914, "mse": 0.0}  0.0373
1         train   88200/102467 551:54/89:16       7.719         {"mlm": 7.734408699736303, "mse": 0.0}  0.0327
1         train   88300/102467 552:30/88:38       7.719         {"mlm": 7.733379926230456, "mse": 0.0}  0.03
1         train   88400/102467 553:07/88:01       7.719         {"mlm": 7.733584823030414, "mse": 0.0}  0.0393
1         train   88500/102467 553:43/87:23       7.719         {"mlm": 7.732326260497493, "mse": 0.0}  0.038
1         train   88600/102467 554:19/86:45       7.719         {"mlm": 7.731177446826193, "mse": 0.0}  0.0347
1         train   88700/102467 554:55/86:07       7.719         {"mlm": 7.728335401107525, "mse": 0.0}  0.0421
1         train   88800/102467 555:32/85:30       7.719         {"mlm": 7.728527554315538, "mse": 0.0}  0.0288
1         train   88900/102467 556:08/84:52       7.719         {"mlm": 7.72759106914912, "mse": 0.0}  0.0303
1         train   89000/102467 556:44/84:14       7.719         {"mlm": 7.725264608620639, "mse": 0.0}  0.0312
1         train   89100/102467 557:20/83:36       7.719         {"mlm": 7.724567723100203, "mse": 0.0}  0.0351
1         train   89200/102467 557:56/82:59       7.719         {"mlm": 7.723835866586819, "mse": 0.0}  0.0297
1         train   89300/102467 558:33/82:21       7.719         {"mlm": 7.725221852093567, "mse": 0.0}  0.0324
1         train   89400/102467 559:09/81:43       7.719         {"mlm": 7.724677695244295, "mse": 0.0}  0.0332
1         train   89500/102467 559:45/81:05       7.719         {"mlm": 7.724924286419058, "mse": 0.0}  0.0309
1         train   89600/102467 560:21/80:28       7.719         {"mlm": 7.724014186620114, "mse": 0.0}  0.0308
1         train   89700/102467 560:58/79:50       7.719         {"mlm": 7.725200023291246, "mse": 0.0}  0.0323
1         train   89800/102467 561:34/79:12       7.719         {"mlm": 7.725242715104916, "mse": 0.0}  0.0365
1         train   89900/102467 562:10/78:35       7.719         {"mlm": 7.725821297882981, "mse": 0.0}  0.0326
1         train   90000/102467 562:46/77:57       7.719         {"mlm": 7.725544092650404, "mse": 0.0}  0.031

09/25/2022 03:17:15 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step90000.pkl
1         valid   1/781        0:20/260:50        7.698           None
1         valid   101/781      0:35/ 3:57         7.696           None
1         valid   201/781      0:50/ 2:26         7.705           None
1         valid   301/781      1:06/ 1:45         7.711           None
1         valid   401/781      1:21/ 1:17         7.717           None
1         valid   501/781      1:36/ 0:54         7.715           None
1         valid   601/781      1:51/ 0:33         7.716           None
1         valid   701/781      2:07/ 0:14         7.717           None
1         valid   781/781      2:21/ 0:00         7.716         {"mlm": 7.7163892445582585, "mse": 0.0, "train": 0.0}  None
1         train   90100/102467 565:44/77:39       7.719         {"mlm": 7.719744009971619, "mse": 0.0}  0.0333
1         train   90200/102467 566:20/77:01       7.719         {"mlm": 7.723119640350342, "mse": 0.0}  0.0351
1         train   90300/102467 566:56/76:23       7.719         {"mlm": 7.721983825365703, "mse": 0.0}  0.0341
1         train   90400/102467 567:32/75:45       7.719         {"mlm": 7.720916966199875, "mse": 0.0}  0.0393
1         train   90500/102467 568:08/75:07       7.719         {"mlm": 7.722503361701965, "mse": 0.0}  0.0305
1         train   90600/102467 568:44/74:29       7.719         {"mlm": 7.7194712432225545, "mse": 0.0}  0.0356
1         train   90700/102467 569:20/73:51       7.719         {"mlm": 7.718640434401376, "mse": 0.0}  0.0352
1         train   90800/102467 569:56/73:13       7.719         {"mlm": 7.7194403010606765, "mse": 0.0}  0.0308
1         train   90900/102467 570:32/72:36       7.719         {"mlm": 7.719196723302205, "mse": 0.0}  0.0333
1         train   91000/102467 571:09/71:58       7.719         {"mlm": 7.717333930492401, "mse": 0.0}  0.0303
1         train   91100/102467 571:45/71:20       7.719         {"mlm": 7.717501697540283, "mse": 0.0}  0.0337
1         train   91200/102467 572:21/70:42       7.719         {"mlm": 7.718058751026789, "mse": 0.0}  0.0351
1         train   91300/102467 572:57/70:04       7.719         {"mlm": 7.719966706129221, "mse": 0.0}  0.0326
1         train   91400/102467 573:34/69:26       7.719         {"mlm": 7.720923728942871, "mse": 0.0}  0.0329
1         train   91500/102467 574:10/68:49       7.719         {"mlm": 7.721346164703369, "mse": 0.0}  0.0353
1         train   91600/102467 574:46/68:11       7.719         {"mlm": 7.722169825434685, "mse": 0.0}  0.0345
1         train   91700/102467 575:22/67:33       7.719         {"mlm": 7.7215105093226715, "mse": 0.0}  0.0426
1         train   91800/102467 575:59/66:55       7.719         {"mlm": 7.721983642578125, "mse": 0.0}  0.0312
1         train   91900/102467 576:35/66:17       7.719         {"mlm": 7.721369909236306, "mse": 0.0}  0.0341
1         train   92000/102467 577:11/65:40       7.719         {"mlm": 7.721208648920059, "mse": 0.0}  0.035
1         train   92100/102467 577:47/65:02       7.719         {"mlm": 7.70668905431574, "mse": 0.0}  0.0313
1         train   92200/102467 578:23/64:24       7.719         {"mlm": 7.722168946385983, "mse": 0.0}  0.0334
1         train   92300/102467 579:00/63:46       7.719         {"mlm": 7.714106888276677, "mse": 0.0}  0.0316
1         train   92400/102467 579:36/63:08       7.719         {"mlm": 7.7140067669383265, "mse": 0.0}  0.0346
1         train   92500/102467 580:12/62:31       7.719         {"mlm": 7.717054119568789, "mse": 0.0}  0.0445
1         train   92600/102467 580:48/61:53       7.719         {"mlm": 7.7124124807189025, "mse": 0.0}  0.0365
1         train   92700/102467 581:25/61:15       7.719         {"mlm": 7.710621273057144, "mse": 0.0}  0.0308
1         train   92800/102467 582:01/60:37       7.719         {"mlm": 7.7131281197444075, "mse": 0.0}  0.0289
1         train   92900/102467 582:37/60:00       7.719         {"mlm": 7.715969250119966, "mse": 0.0}  0.0307
1         train   93000/102467 583:14/59:22       7.719         {"mlm": 7.713096873538272, "mse": 0.0}  0.036
1         train   93100/102467 583:50/58:44       7.719         {"mlm": 7.71634827623376, "mse": 0.0}  0.0313
1         train   93200/102467 584:26/58:06       7.719         {"mlm": 7.717796336819074, "mse": 0.0}  0.0337
1         train   93300/102467 585:02/57:28       7.719         {"mlm": 7.7171339074678835, "mse": 0.0}  0.0336
1         train   93400/102467 585:38/56:51       7.719         {"mlm": 7.716207461667282, "mse": 0.0}  0.0305
1         train   93500/102467 586:14/56:13       7.719         {"mlm": 7.7160557383613, "mse": 0.0}  0.0325
1         train   93600/102467 586:51/55:35       7.719         {"mlm": 7.716273621218588, "mse": 0.0}  0.033
1         train   93700/102467 587:27/54:57       7.719         {"mlm": 7.715635201733136, "mse": 0.0}  0.0341
1         train   93800/102467 588:03/54:20       7.719         {"mlm": 7.717162816374748, "mse": 0.0}  0.0334
1         train   93900/102467 588:39/53:42       7.719         {"mlm": 7.717399649647677, "mse": 0.0}  0.0322
1         train   94000/102467 589:16/53:04       7.719         {"mlm": 7.717923830365348, "mse": 0.0}  0.0301
1         train   94100/102467 589:52/52:26       7.719         {"mlm": 7.72499156971367, "mse": 0.0}  0.0382
1         train   94200/102467 590:28/51:49       7.719         {"mlm": 7.722695266357576, "mse": 0.0}  0.0281
1         train   94300/102467 591:04/51:11       7.719         {"mlm": 7.718274124516737, "mse": 0.0}  0.0339
1         train   94400/102467 591:40/50:33       7.719         {"mlm": 7.7156907110358, "mse": 0.0}  0.0311
1         train   94500/102467 592:17/49:56       7.719         {"mlm": 7.715084735648221, "mse": 0.0}  0.0258
1         train   94600/102467 592:53/49:18       7.719         {"mlm": 7.717294374038543, "mse": 0.0}  0.031
1         train   94700/102467 593:29/48:40       7.719         {"mlm": 7.716202634111858, "mse": 0.0}  0.0332
1         train   94800/102467 594:05/48:02       7.719         {"mlm": 7.716478692559073, "mse": 0.0}  0.0339
1         train   94900/102467 594:41/47:25       7.719         {"mlm": 7.717613232958821, "mse": 0.0}  0.0305
1         train   95000/102467 595:18/46:47       7.719         {"mlm": 7.716617405056237, "mse": 0.0}  0.0378
1         train   95100/102467 595:54/46:09       7.719         {"mlm": 7.718230232732109, "mse": 0.0}  0.0357
1         train   95200/102467 596:30/45:32       7.719         {"mlm": 7.718173322375112, "mse": 0.0}  0.033
1         train   95300/102467 597:06/44:54       7.719         {"mlm": 7.718132481552971, "mse": 0.0}  0.03
1         train   95400/102467 597:43/44:16       7.719         {"mlm": 7.717793110955256, "mse": 0.0}  0.031
1         train   95500/102467 598:19/43:38       7.719         {"mlm": 7.720129927264673, "mse": 0.0}  0.0305
1         train   95600/102467 598:55/43:01       7.719         {"mlm": 7.719343371027253, "mse": 0.0}  0.0307
1         train   95700/102467 599:31/42:23       7.719         {"mlm": 7.719508273862697, "mse": 0.0}  0.0358
1         train   95800/102467 600:08/41:45       7.719         {"mlm": 7.718334600578028, "mse": 0.0}  0.0367
1         train   95900/102467 600:44/41:08       7.719         {"mlm": 7.718411514455074, "mse": 0.0}  0.0395
1         train   96000/102467 601:20/40:30       7.719         {"mlm": 7.718860504028198, "mse": 0.0}  0.0303
1         train   96100/102467 601:56/39:52       7.719         {"mlm": 7.7193880032018285, "mse": 0.0}  0.034
1         train   96200/102467 602:32/39:15       7.719         {"mlm": 7.728995199736, "mse": 0.0}  0.0333
1         train   96300/102467 603:09/38:37       7.719         {"mlm": 7.725500546721898, "mse": 0.0}  0.0449
1         train   96400/102467 603:45/37:59       7.719         {"mlm": 7.72265493839754, "mse": 0.0}  0.0326
1         train   96500/102467 604:21/37:22       7.719         {"mlm": 7.719092850713903, "mse": 0.0}  0.0371
1         train   96600/102467 604:57/36:44       7.719         {"mlm": 7.715392586374203, "mse": 0.0}  0.0336
1         train   96700/102467 605:33/36:06       7.719         {"mlm": 7.71539099076215, "mse": 0.0}  0.034
1         train   96800/102467 606:10/35:29       7.719         {"mlm": 7.715930322481967, "mse": 0.0}  0.0344
1         train   96900/102467 606:46/34:51       7.719         {"mlm": 7.716409271244487, "mse": 0.0}  0.037
1         train   97000/102467 607:22/34:13       7.719         {"mlm": 7.715331030226758, "mse": 0.0}  0.035
1         train   97100/102467 607:58/33:36       7.719         {"mlm": 7.716105420261268, "mse": 0.0}  0.0373
1         train   97200/102467 608:34/32:58       7.719         {"mlm": 7.716173667158798, "mse": 0.0}  0.0343
1         train   97300/102467 609:11/32:20       7.719         {"mlm": 7.71874706397722, "mse": 0.0}  0.0354
1         train   97400/102467 609:47/31:43       7.719         {"mlm": 7.717335100249042, "mse": 0.0}  0.0352
1         train   97500/102467 610:23/31:05       7.719         {"mlm": 7.716648021855988, "mse": 0.0}  0.03
1         train   97600/102467 610:59/30:28       7.719         {"mlm": 7.71632797895705, "mse": 0.0}  0.0316
1         train   97700/102467 611:36/29:50       7.719         {"mlm": 7.716682193275052, "mse": 0.0}  0.036
1         train   97800/102467 612:12/29:12       7.719         {"mlm": 7.717306555014023, "mse": 0.0}  0.0306
1         train   97900/102467 612:48/28:35       7.719         {"mlm": 7.717368071118216, "mse": 0.0}  0.0319
1         train   98000/102467 613:24/27:57       7.719         {"mlm": 7.716250593446169, "mse": 0.0}  0.0337
1         train   98100/102467 614:01/27:20       7.719         {"mlm": 7.754738658666611, "mse": 0.0}  0.0301
1         train   98200/102467 614:37/26:42       7.719         {"mlm": 7.735211839481276, "mse": 0.0}  0.0325
1         train   98300/102467 615:13/26:04       7.719         {"mlm": 7.730664902442211, "mse": 0.0}  0.0321
1         train   98400/102467 615:49/25:27       7.719         {"mlm": 7.736484440890226, "mse": 0.0}  0.0298
1         train   98500/102467 616:26/24:49       7.719         {"mlm": 7.7376483380794525, "mse": 0.0}  0.0331
1         train   98600/102467 617:02/24:11       7.719         {"mlm": 7.7367250863337675, "mse": 0.0}  0.0293
1         train   98700/102467 617:38/23:34       7.719         {"mlm": 7.733812719240956, "mse": 0.0}  0.0334
1         train   98800/102467 618:14/22:56       7.719         {"mlm": 7.733983807228318, "mse": 0.0}  0.0306
1         train   98900/102467 618:51/22:19       7.719         {"mlm": 7.730491645634174, "mse": 0.0}  0.0334
1         train   99000/102467 619:27/21:41       7.719         {"mlm": 7.727940722162944, "mse": 0.0}  0.0378
1         train   99100/102467 620:03/21:04       7.719         {"mlm": 7.725869879235316, "mse": 0.0}  0.0345
1         train   99200/102467 620:39/20:26       7.719         {"mlm": 7.725622215398578, "mse": 0.0}  0.029
1         train   99300/102467 621:16/19:48       7.719         {"mlm": 7.726491206222111, "mse": 0.0}  0.03
1         train   99400/102467 621:52/19:11       7.719         {"mlm": 7.7263107897559005, "mse": 0.0}  0.0317
1         train   99500/102467 622:28/18:33       7.719         {"mlm": 7.723582602439717, "mse": 0.0}  0.0351
1         train   99600/102467 623:05/17:56       7.719         {"mlm": 7.723303884491885, "mse": 0.0}  0.0322
1         train   99700/102467 623:41/17:18       7.719         {"mlm": 7.723470847561674, "mse": 0.0}  0.042
1         train   99800/102467 624:17/16:41       7.719         {"mlm": 7.724014104341877, "mse": 0.0}  0.0299
1         train   99900/102467 624:54/16:03       7.719         {"mlm": 7.722972366628768, "mse": 0.0}  0.0352
1         train   100000/102467 625:30/15:25      7.719         {"mlm": 7.7233261593835865, "mse": 0.0}  0.0317

09/25/2022 04:19:59 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step100000.pkl
1         valid   1/781        0:19/258:25        7.711           None
1         valid   101/781      0:35/ 3:56         7.714           None
1         valid   201/781      0:50/ 2:25         7.714           None
1         valid   301/781      1:05/ 1:45         7.712           None
1         valid   401/781      1:21/ 1:16         7.718           None
1         valid   501/781      1:36/ 0:53         7.715           None
1         valid   601/781      1:51/ 0:33         7.714           None
1         valid   701/781      2:07/ 0:14         7.717           None
1         valid   781/781      2:21/ 0:00         7.718         {"mlm": 7.71814980793854, "mse": 0.0, "train": 0.0}  None
1         train   100100/102467 628:28/14:51      7.719         {"mlm": 7.71153832912445, "mse": 0.0}  0.0367
1         train   100200/102467 629:04/14:13      7.719         {"mlm": 7.7157949709892275, "mse": 0.0}  0.0313
1         train   100300/102467 629:40/13:36      7.719         {"mlm": 7.721912902196248, "mse": 0.0}  0.0356
1         train   100400/102467 630:16/12:58      7.719         {"mlm": 7.721558395624161, "mse": 0.0}  0.0303
1         train   100500/102467 630:52/12:20      7.719         {"mlm": 7.72737672996521, "mse": 0.0}  0.0349
1         train   100600/102467 631:28/11:43      7.719         {"mlm": 7.730001021226247, "mse": 0.0}  0.0316
1         train   100700/102467 632:04/11:05      7.719         {"mlm": 7.729429321970258, "mse": 0.0}  0.0332
1         train   100800/102467 632:40/10:27      7.719         {"mlm": 7.728785523176193, "mse": 0.0}  0.035
1         train   100900/102467 633:16/ 9:50      7.719         {"mlm": 7.7264952347013685, "mse": 0.0}  0.0316
1         train   101000/102467 633:53/ 9:12      7.719         {"mlm": 7.723567015647888, "mse": 0.0}  0.0369
1         train   101100/102467 634:29/ 8:34      7.719         {"mlm": 7.722754619338295, "mse": 0.0}  0.0351
1         train   101200/102467 635:05/ 7:57      7.719         {"mlm": 7.722937226692835, "mse": 0.0}  0.034
1         train   101300/102467 635:41/ 7:19      7.719         {"mlm": 7.720664254335257, "mse": 0.0}  0.0337
1         train   101400/102467 636:18/ 6:41      7.719         {"mlm": 7.720660966123853, "mse": 0.0}  0.0323
1         train   101500/102467 636:54/ 6:04      7.719         {"mlm": 7.72126917775472, "mse": 0.0}  0.0332
1         train   101600/102467 637:30/ 5:26      7.719         {"mlm": 7.721741169989109, "mse": 0.0}  0.0303
1         train   101700/102467 638:06/ 4:48      7.719         {"mlm": 7.721476935498854, "mse": 0.0}  0.0293
1         train   101800/102467 638:43/ 4:11      7.719         {"mlm": 7.721044272051917, "mse": 0.0}  0.0372
1         train   101900/102467 639:19/ 3:33      7.719         {"mlm": 7.721099616100914, "mse": 0.0}  0.0374
1         train   102000/102467 639:55/ 2:55      7.719         {"mlm": 7.721027208089828, "mse": 0.0}  0.0336
1         train   102100/102467 640:31/ 2:18      7.719         {"mlm": 7.733851928903599, "mse": 0.0}  0.0331
1         train   102200/102467 641:08/ 1:40      7.719         {"mlm": 7.733433697091874, "mse": 0.0}  0.0363
1         train   102300/102467 641:44/ 1:02      7.719         {"mlm": 7.728502498422578, "mse": 0.0}  0.031
1         train   102400/102467 642:20/ 0:25      7.719         {"mlm": 7.727949969451828, "mse": 0.0}  0.0299
True False
skip validation
2         train   100/102467   0:49/843:06        7.706         {"mlm": 7.705632185935974, "mse": 0.0}  0.0312
2         train   200/102467   1:25/727:24        7.719         {"mlm": 7.718572087287903, "mse": 0.0}  0.0353
2         train   300/102467   2:01/688:13        7.722         {"mlm": 7.722026956876119, "mse": 0.0}  0.041
2         train   400/102467   2:37/668:28        7.719         {"mlm": 7.719065099954605, "mse": 0.0}  0.0315
2         train   500/102467   3:13/656:36        7.721         {"mlm": 7.720632789611816, "mse": 0.0}  0.0388
2         train   600/102467   3:49/648:13        7.721         {"mlm": 7.720789525508881, "mse": 0.0}  0.0312
2         train   700/102467   4:25/642:22        7.722         {"mlm": 7.722265439714704, "mse": 0.0}  0.0331
2         train   800/102467   5:01/637:46        7.723         {"mlm": 7.723161087036133, "mse": 0.0}  0.0314
2         train   900/102467   5:37/634:08        7.722         {"mlm": 7.7215240780512495, "mse": 0.0}  0.0304
2         train   1000/102467  6:13/631:12        7.720         {"mlm": 7.71994868183136, "mse": 0.0}  0.0368
2         train   1100/102467  6:49/628:40        7.720         {"mlm": 7.719518698779019, "mse": 0.0}  0.0324
2         train   1200/102467  7:25/626:30        7.717         {"mlm": 7.717245411872864, "mse": 0.0}  0.0281
2         train   1300/102467  8:01/624:38        7.719         {"mlm": 7.7185412274874174, "mse": 0.0}  0.0309
2         train   1400/102467  8:37/623:00        7.719         {"mlm": 7.718596641336169, "mse": 0.0}  0.0375
2         train   1500/102467  9:13/621:26        7.719         {"mlm": 7.719130603472392, "mse": 0.0}  0.0362
2         train   1600/102467  9:50/619:59        7.719         {"mlm": 7.718654001951218, "mse": 0.0}  0.0307
2         train   1700/102467 10:26/618:39        7.718         {"mlm": 7.7176254067701455, "mse": 0.0}  0.0349
2         train   1800/102467 11:02/617:24        7.717         {"mlm": 7.71718966378106, "mse": 0.0}  0.0353
2         train   1900/102467 11:38/616:13        7.717         {"mlm": 7.716999308686507, "mse": 0.0}  0.0327
2         train   2000/102467 12:14/615:03        7.717         {"mlm": 7.717346683263779, "mse": 0.0}  0.0359
2         train   2100/102467 12:50/613:57        7.718         {"mlm": 7.723193476898501, "mse": 0.0}  0.0332
2         train   2200/102467 13:26/612:58        7.718         {"mlm": 7.719831897984797, "mse": 0.0}  0.0353
2         train   2300/102467 14:03/612:01        7.717         {"mlm": 7.716805623925251, "mse": 0.0}  0.0284
2         train   2400/102467 14:39/611:00        7.718         {"mlm": 7.72204928171067, "mse": 0.0}  0.0289
2         train   2500/102467 15:15/610:05        7.717         {"mlm": 7.717126270095427, "mse": 0.0}  0.0353
2         train   2600/102467 15:51/609:14        7.718         {"mlm": 7.718536587111739, "mse": 0.0}  0.0351
2         train   2700/102467 16:27/608:22        7.717         {"mlm": 7.716688493802312, "mse": 0.0}  0.0284
2         train   2800/102467 17:04/607:35        7.717         {"mlm": 7.717708822782705, "mse": 0.0}  0.0361
2         train   2900/102467 17:40/606:47        7.717         {"mlm": 7.716701150603501, "mse": 0.0}  0.0372
2         train   3000/102467 18:16/606:01        7.717         {"mlm": 7.716193026369876, "mse": 0.0}  0.0345
2         train   3100/102467 18:52/605:13        7.717         {"mlm": 7.717544643308381, "mse": 0.0}  0.0385
2         train   3200/102467 19:29/604:27        7.717         {"mlm": 7.717554995176492, "mse": 0.0}  0.0326
2         train   3300/102467 20:05/603:41        7.718         {"mlm": 7.719021046868281, "mse": 0.0}  0.0324
2         train   3400/102467 20:41/602:55        7.718         {"mlm": 7.719884303231338, "mse": 0.0}  0.0364
2         train   3500/102467 21:17/602:11        7.718         {"mlm": 7.719182725108569, "mse": 0.0}  0.03
2         train   3600/102467 21:54/601:28        7.719         {"mlm": 7.719946523097398, "mse": 0.0}  0.0364
2         train   3700/102467 22:30/600:43        7.719         {"mlm": 7.720596543896402, "mse": 0.0}  0.0384
2         train   3800/102467 23:06/600:01        7.719         {"mlm": 7.7198255484868845, "mse": 0.0}  0.0336
2         train   3900/102467 23:42/599:18        7.719         {"mlm": 7.719716800772058, "mse": 0.0}  0.0315
2         train   4000/102467 24:18/598:34        7.719         {"mlm": 7.720996637473171, "mse": 0.0}  0.0346
2         train   4100/102467 24:55/597:51        7.719         {"mlm": 7.70449498234963, "mse": 0.0}  0.0342
2         train   4200/102467 25:31/597:07        7.718         {"mlm": 7.703369547622373, "mse": 0.0}  0.0379
2         train   4300/102467 26:07/596:25        7.718         {"mlm": 7.703966521576747, "mse": 0.0}  0.0334
2         train   4400/102467 26:43/595:45        7.718         {"mlm": 7.707248652999724, "mse": 0.0}  0.0314
2         train   4500/102467 27:20/595:04        7.718         {"mlm": 7.710346603010553, "mse": 0.0}  0.0298
2         train   4600/102467 27:56/594:24        7.718         {"mlm": 7.7128460144119515, "mse": 0.0}  0.0336
2         train   4700/102467 28:32/593:46        7.718         {"mlm": 7.712362566103566, "mse": 0.0}  0.0321
2         train   4800/102467 29:08/593:04        7.718         {"mlm": 7.711874443785589, "mse": 0.0}  0.0286
2         train   4900/102467 29:45/592:24        7.718         {"mlm": 7.711599028190154, "mse": 0.0}  0.0386
2         train   5000/102467 30:21/591:43        7.718         {"mlm": 7.713477877194514, "mse": 0.0}  0.0305
2         train   5100/102467 30:57/591:05        7.718         {"mlm": 7.714416479152408, "mse": 0.0}  0.04
2         train   5200/102467 31:33/590:25        7.718         {"mlm": 7.713900133046961, "mse": 0.0}  0.0304
2         train   5300/102467 32:10/589:45        7.718         {"mlm": 7.714638214449302, "mse": 0.0}  0.0314
2         train   5400/102467 32:46/589:06        7.718         {"mlm": 7.714769102814201, "mse": 0.0}  0.0287
2         train   5500/102467 33:22/588:26        7.718         {"mlm": 7.715369124915476, "mse": 0.0}  0.0334
2         train   5600/102467 33:58/587:48        7.718         {"mlm": 7.7146339580621826, "mse": 0.0}  0.0316
2         train   5700/102467 34:35/587:09        7.718         {"mlm": 7.715883187045759, "mse": 0.0}  0.0379
2         train   5800/102467 35:11/586:30        7.718         {"mlm": 7.7161061358001, "mse": 0.0}  0.0295
2         train   5900/102467 35:47/585:52        7.719         {"mlm": 7.717162434996996, "mse": 0.0}  0.0307
2         train   6000/102467 36:23/585:13        7.718         {"mlm": 7.7160826913110006, "mse": 0.0}  0.0393
2         train   6100/102467 37:00/584:34        7.718         {"mlm": 7.706794016140023, "mse": 0.0}  0.0312
2         train   6200/102467 37:36/583:54        7.718         {"mlm": 7.709684381630215, "mse": 0.0}  0.0292
2         train   6300/102467 38:12/583:15        7.718         {"mlm": 7.711610652782299, "mse": 0.0}  0.032
2         train   6400/102467 38:48/582:36        7.718         {"mlm": 7.715075393167491, "mse": 0.0}  0.0297
2         train   6500/102467 39:25/581:57        7.718         {"mlm": 7.714355254796909, "mse": 0.0}  0.0349
2         train   6600/102467 40:01/581:17        7.718         {"mlm": 7.714811408939074, "mse": 0.0}  0.0371
2         train   6700/102467 40:37/580:38        7.718         {"mlm": 7.713840973086473, "mse": 0.0}  0.0325
2         train   6800/102467 41:13/579:59        7.718         {"mlm": 7.7138280204428336, "mse": 0.0}  0.0333
2         train   6900/102467 41:49/579:21        7.717         {"mlm": 7.712018956574575, "mse": 0.0}  0.0302
2         train   7000/102467 42:25/578:42        7.717         {"mlm": 7.713397083932921, "mse": 0.0}  0.0297
2         train   7100/102467 43:02/578:03        7.718         {"mlm": 7.715525195939821, "mse": 0.0}  0.0319
2         train   7200/102467 43:38/577:25        7.718         {"mlm": 7.715851535972398, "mse": 0.0}  0.0288
2         train   7300/102467 44:14/576:47        7.718         {"mlm": 7.715933330994711, "mse": 0.0}  0.0274
2         train   7400/102467 44:50/576:10        7.718         {"mlm": 7.7167353845103435, "mse": 0.0}  0.0273
2         train   7500/102467 45:27/575:34        7.718         {"mlm": 7.7166792048401405, "mse": 0.0}  0.0388
2         train   7600/102467 46:03/574:56        7.718         {"mlm": 7.716551251910071, "mse": 0.0}  0.0332
2         train   7700/102467 46:39/574:19        7.718         {"mlm": 7.716518226481917, "mse": 0.0}  0.0325
2         train   7800/102467 47:16/573:41        7.718         {"mlm": 7.716288661850647, "mse": 0.0}  0.0386
2         train   7900/102467 47:52/573:03        7.718         {"mlm": 7.716126181039926, "mse": 0.0}  0.0324
2         train   8000/102467 48:28/572:25        7.717         {"mlm": 7.715494091414068, "mse": 0.0}  0.0301
2         train   8100/102467 49:04/571:47        7.717         {"mlm": 7.707603057225545, "mse": 0.0}  0.0287
2         train   8200/102467 49:41/571:10        7.717         {"mlm": 7.701925559919708, "mse": 0.0}  0.0299
