/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/SQuAD/LoRa.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/SQuAD/LoRa.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/SQuAD/LoRa.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/SQuAD/LoRa.config
None
10/04/2022 07:34:48 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
10/04/2022 07:34:48 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
10/04/2022 07:34:48 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
10/04/2022 07:34:48 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
10/04/2022 07:34:48 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
10/04/2022 07:34:48 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
10/04/2022 07:34:48 - INFO - __main__ -   CUDA available: True
10/04/2022 07:34:48 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
10/04/2022 07:34:48 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
10/04/2022 07:34:48 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
10/04/2022 07:34:48 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
10/04/2022 07:34:48 - INFO - __main__ -   CUDA available: True
formatter roberta-base
10/04/2022 07:34:48 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
10/04/2022 07:34:56 - INFO - tools.init_tool -   Begin to initialize models...
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── qa_outputs (Linear) weight:[2, 768] bias:[2]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   ├── query,value(Linear) weight:[768, 768] bias:[768]
│               │   │   │   └── lora (LowRankLinear) lora_A:[32, 768] lora_B:[768, 32]
│               │   │   └── key (Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── qa_outputs (Linear) weight:[2, 768] bias:[2]
[INFO|(OpenDelta)basemodel:675]2022-10-04 07:35:00,582 >> Trainable Ratio: 0.943166%
[INFO|(OpenDelta)basemodel:677]2022-10-04 07:35:00,582 >> Delta Parameter Ratio: 0.941938%
[INFO|(OpenDelta)basemodel:679]2022-10-04 07:35:00,582 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── qa_outputs (Linear) weight:[2, 768] bias:[2]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   ├── query,value(Linear) weight:[768, 768] bias:[768]
│               │   │   │   └── lora (LowRankLinear) lora_A:[32, 768] lora_B:[768, 32]
│               │   │   └── key (Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── qa_outputs (Linear) weight:[2, 768] bias:[2]
[INFO|(OpenDelta)basemodel:675]2022-10-04 07:35:01,041 >> Trainable Ratio: 0.943166%
[INFO|(OpenDelta)basemodel:677]2022-10-04 07:35:01,041 >> Delta Parameter Ratio: 0.941938%
[INFO|(OpenDelta)basemodel:679]2022-10-04 07:35:01,041 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── qa_outputs (Linear) weight:[2, 768] bias:[2]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   ├── query,value(Linear) weight:[768, 768] bias:[768]
│               │   │   │   └── lora (LowRankLinear) lora_A:[32, 768] lora_B:[768, 32]
│               │   │   └── key (Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── qa_outputs (Linear) weight:[2, 768] bias:[2]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── qa_outputs (Linear) weight:[2, 768] bias:[2]
[INFO|(OpenDelta)basemodel:675]2022-10-04 07:35:01,258 >> Trainable Ratio: 0.943166%
[INFO|(OpenDelta)basemodel:677]2022-10-04 07:35:01,258 >> Delta Parameter Ratio: 0.941938%
[INFO|(OpenDelta)basemodel:679]2022-10-04 07:35:01,259 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   ├── query,value(Linear) weight:[768, 768] bias:[768]
│               │   │   │   └── lora (LowRankLinear) lora_A:[32, 768] lora_B:[768, 32]
│               │   │   └── key (Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── qa_outputs (Linear) weight:[2, 768] bias:[2]
[INFO|(OpenDelta)basemodel:675]2022-10-04 07:35:01,350 >> Trainable Ratio: 0.943166%
[INFO|(OpenDelta)basemodel:677]2022-10-04 07:35:01,350 >> Delta Parameter Ratio: 0.941938%
[INFO|(OpenDelta)basemodel:679]2022-10-04 07:35:01,350 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
formatter roberta-base
odict_keys(['roberta.encoder.layer.0.attention.self.query.lora.lora_A', 'roberta.encoder.layer.0.attention.self.query.lora.lora_B', 'roberta.encoder.layer.0.attention.self.value.lora.lora_A', 'roberta.encoder.layer.0.attention.self.value.lora.lora_B', 'roberta.encoder.layer.1.attention.self.query.lora.lora_A', 'roberta.encoder.layer.1.attention.self.query.lora.lora_B', 'roberta.encoder.layer.1.attention.self.value.lora.lora_A', 'roberta.encoder.layer.1.attention.self.value.lora.lora_B', 'roberta.encoder.layer.2.attention.self.query.lora.lora_A', 'roberta.encoder.layer.2.attention.self.query.lora.lora_B', 'roberta.encoder.layer.2.attention.self.value.lora.lora_A', 'roberta.encoder.layer.2.attention.self.value.lora.lora_B', 'roberta.encoder.layer.3.attention.self.query.lora.lora_A', 'roberta.encoder.layer.3.attention.self.query.lora.lora_B', 'roberta.encoder.layer.3.attention.self.value.lora.lora_A', 'roberta.encoder.layer.3.attention.self.value.lora.lora_B', 'roberta.encoder.layer.4.attention.self.query.lora.lora_A', 'roberta.encoder.layer.4.attention.self.query.lora.lora_B', 'roberta.encoder.layer.4.attention.self.value.lora.lora_A', 'roberta.encoder.layer.4.attention.self.value.lora.lora_B', 'roberta.encoder.layer.5.attention.self.query.lora.lora_A', 'roberta.encoder.layer.5.attention.self.query.lora.lora_B', 'roberta.encoder.layer.5.attention.self.value.lora.lora_A', 'roberta.encoder.layer.5.attention.self.value.lora.lora_B', 'roberta.encoder.layer.6.attention.self.query.lora.lora_A', 'roberta.encoder.layer.6.attention.self.query.lora.lora_B', 'roberta.encoder.layer.6.attention.self.value.lora.lora_A', 'roberta.encoder.layer.6.attention.self.value.lora.lora_B', 'roberta.encoder.layer.7.attention.self.query.lora.lora_A', 'roberta.encoder.layer.7.attention.self.query.lora.lora_B', 'roberta.encoder.layer.7.attention.self.value.lora.lora_A', 'roberta.encoder.layer.7.attention.self.value.lora.lora_B', 'roberta.encoder.layer.8.attention.self.query.lora.lora_A', 'roberta.encoder.layer.8.attention.self.query.lora.lora_B', 'roberta.encoder.layer.8.attention.self.value.lora.lora_A', 'roberta.encoder.layer.8.attention.self.value.lora.lora_B', 'roberta.encoder.layer.9.attention.self.query.lora.lora_A', 'roberta.encoder.layer.9.attention.self.query.lora.lora_B', 'roberta.encoder.layer.9.attention.self.value.lora.lora_A', 'roberta.encoder.layer.9.attention.self.value.lora.lora_B', 'roberta.encoder.layer.10.attention.self.query.lora.lora_A', 'roberta.encoder.layer.10.attention.self.query.lora.lora_B', 'roberta.encoder.layer.10.attention.self.value.lora.lora_A', 'roberta.encoder.layer.10.attention.self.value.lora.lora_B', 'roberta.encoder.layer.11.attention.self.query.lora.lora_A', 'roberta.encoder.layer.11.attention.self.query.lora.lora_B', 'roberta.encoder.layer.11.attention.self.value.lora.lora_A', 'roberta.encoder.layer.11.attention.self.value.lora.lora_B', 'qa_outputs.weight', 'qa_outputs.bias'])
formatter roberta-base
10/04/2022 07:35:12 - INFO - tools.init_tool -   Begin to load checkpoint... from None
formatter roberta-base
10/04/2022 07:35:12 - WARNING - tools.init_tool -   Cannot load checkpoint file with error 'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.
10/04/2022 07:35:12 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
max_len: 512
warmup_steps: 2000
training_steps: 20000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: batch
lora_r: 32
lora_alpha: 64
question_first: True
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 4
========
[data]
train_dataset_type: SQuAD
train_formatter_type: SQuAD
train_data_path: /data/disk1/private/xcj/DomainPlugin/data/SQuAD/train-v1.1.json
valid_dataset_type: SQuAD
valid_formatter_type: SQuAD
valid_data_path: /data/disk1/private/xcj/DomainPlugin/data/SQuAD/dev-v1.1.json
test_dataset_type: SQuAD
test_formatter_type: SQuAD
test_data_path: /data/disk1/private/xcj/DomainPlugin/data/BioSAQ/BioASQ-6b/train/Appended-Snippet/BioASQ-train-factoid-6b-snippet-2sent.json
========
[model]
model_name: SQuAD
pretrained_model: roberta-base
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints
model_name: SQuAD-Lora
output_function: squad1
========
10/04/2022 07:35:12 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
valid_mode batch no_valid False
step_epoch None
10/04/2022 07:35:20 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
10/04/2022 07:35:36 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/04/2022 07:35:36 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/04/2022 07:35:36 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/04/2022 07:35:36 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   100/2737     0:41/18:10         5.904         {"position_acc": 0.0119}  4.0234
0         train   200/2737     1:06/14:05         4.871         {"position_acc": 0.0703}  10.2117
0         train   300/2737     1:32/12:29         4.009         {"position_acc": 0.1841}  11.7334
0         train   400/2737     1:57/11:28         3.454         {"position_acc": 0.273}  7.0257
0         train   500/2737     2:23/10:42         3.074         {"position_acc": 0.3366}  5.7729
0         train   600/2737     2:49/10:04         2.798         {"position_acc": 0.3838}  5.0646
0         train   700/2737     3:16/ 9:31         2.585         {"position_acc": 0.4176}  5.1081
0         train   800/2737     3:42/ 8:59         2.421         {"position_acc": 0.4491}  4.6598
0         train   900/2737     4:09/ 8:28         2.296         {"position_acc": 0.4718}  4.4268
0         train   1000/2737    4:35/ 7:59         2.184         {"position_acc": 0.4914}  10.1843
0         train   1100/2737    5:02/ 7:29         2.090         {"position_acc": 0.5086}  3.4203
0         train   1200/2737    5:28/ 7:00         2.018         {"position_acc": 0.5218}  5.6507
0         train   1300/2737    5:54/ 6:32         1.959         {"position_acc": 0.5307}  6.6893
0         train   1400/2737    6:21/ 6:04         1.904         {"position_acc": 0.5403}  4.8611
0         train   1500/2737    6:47/ 5:36         1.858         {"position_acc": 0.5482}  6.4294
0         train   1600/2737    7:14/ 5:08         1.812         {"position_acc": 0.5563}  5.6284
0         train   1700/2737    7:40/ 4:41         1.778         {"position_acc": 0.5617}  7.1832
0         train   1800/2737    8:07/ 4:13         1.750         {"position_acc": 0.5663}  4.9123
0         train   1900/2737    8:33/ 3:46         1.722         {"position_acc": 0.5709}  4.4719
0         train   2000/2737    9:00/ 3:19         1.695         {"position_acc": 0.5762}  5.872
0         train   2100/2737    9:26/ 2:51         1.669         {"position_acc": 0.5809}  8.3428
0         train   2200/2737    9:52/ 2:24         1.648         {"position_acc": 0.585}  6.9621
0         train   2300/2737   10:19/ 1:57         1.628         {"position_acc": 0.5879}  8.454
0         train   2400/2737   10:45/ 1:30         1.609         {"position_acc": 0.5909}  7.8096
0         train   2500/2737   11:12/ 1:03         1.589         {"position_acc": 0.5948}  15.5995
0         train   2600/2737   11:38/ 0:36         1.572         {"position_acc": 0.5985}  8.2807
0         train   2700/2737   12:05/ 0:09         1.556         {"position_acc": 0.6012}  11.0675
False False
==================== begin saving model and validation ====================
0         train   2737/2737   12:16/ 0:00         1.551         {"position_acc": 0.6023}  None
10/04/2022 07:47:36 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/0.pkl
0         valid   1/330        0:13/76:00         0.000           None
0         valid   101/330      0:25/ 0:57         0.000           None
0         valid   201/330      0:37/ 0:23         0.000           None
0         valid   301/330      0:49/ 0:04         0.000           None
0         valid   330/330      0:53/ 0:00         0.000         {"EM": 0.8001, "F1": 0.8808}  None
0         test    1/127        0:12/26:57         0.000           None
0         test    101/127      0:24/ 0:06         0.000           None
0         test    127/127      0:28/ 0:00         0.000         {"EM": 0.445, "F1": 0.5886}  None
1         train   100/2737     0:41/18:22         1.101         {"position_acc": 0.7044}  5.6538
1         train   200/2737     1:07/14:19         1.062         {"position_acc": 0.7037}  5.797
1         train   300/2737     1:34/12:44         1.088         {"position_acc": 0.698}  7.2184
1         train   400/2737     2:00/11:43         1.111         {"position_acc": 0.6899}  8.1534
1         train   500/2737     2:26/10:56         1.117         {"position_acc": 0.6876}  6.3758
1         train   600/2737     2:53/10:17         1.115         {"position_acc": 0.6891}  6.4516
1         train   700/2737     3:19/ 9:41         1.109         {"position_acc": 0.6905}  8.1191
1         train   800/2737     3:46/ 9:07         1.107         {"position_acc": 0.6902}  8.0659
1         train   900/2737     4:12/ 8:35         1.101         {"position_acc": 0.6917}  5.5046
1         train   1000/2737    4:39/ 8:04         1.093         {"position_acc": 0.6917}  8.4159
1         train   1100/2737    5:05/ 7:34         1.087         {"position_acc": 0.6919}  7.878
1         train   1200/2737    5:31/ 7:05         1.092         {"position_acc": 0.6902}  11.5405
1         train   1300/2737    5:58/ 6:36         1.096         {"position_acc": 0.6877}  9.496
1         train   1400/2737    6:24/ 6:07         1.097         {"position_acc": 0.6879}  8.7547
1         train   1500/2737    6:51/ 5:39         1.098         {"position_acc": 0.6872}  7.88
1         train   1600/2737    7:17/ 5:10         1.095         {"position_acc": 0.6877}  7.8213
1         train   1700/2737    7:43/ 4:42         1.099         {"position_acc": 0.6871}  10.3009
1         train   1800/2737    8:10/ 4:15         1.102         {"position_acc": 0.6865}  7.9336
1         train   1900/2737    8:36/ 3:47         1.101         {"position_acc": 0.6856}  5.4628
1         train   2000/2737    9:02/ 3:20         1.099         {"position_acc": 0.6868}  6.7965
1         train   2100/2737    9:29/ 2:52         1.095         {"position_acc": 0.6876}  10.3358
1         train   2200/2737    9:55/ 2:25         1.095         {"position_acc": 0.6877}  8.3628
1         train   2300/2737   10:22/ 1:58         1.092         {"position_acc": 0.688}  8.3282
1         train   2400/2737   10:48/ 1:31         1.088         {"position_acc": 0.6879}  11.3052
1         train   2500/2737   11:14/ 1:03         1.088         {"position_acc": 0.6883}  6.097
1         train   2600/2737   11:41/ 0:36         1.085         {"position_acc": 0.689}  9.7852
1         train   2700/2737   12:07/ 0:09         1.084         {"position_acc": 0.6889}  14.8054
False False
==================== begin saving model and validation ====================
1         train   2737/2737   12:18/ 0:00         1.084         {"position_acc": 0.6887}  None
10/04/2022 08:01:16 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/1.pkl
1         valid   1/330        0:13/75:16         0.000           None
1         valid   101/330      0:25/ 0:57         0.000           None
1         valid   201/330      0:37/ 0:23         0.000           None
1         valid   301/330      0:48/ 0:04         0.000           None
1         valid   330/330      0:53/ 0:00         0.000         {"EM": 0.8133, "F1": 0.8897}  None
1         test    1/127        0:13/27:26         0.000           None
1         test    101/127      0:24/ 0:06         0.000           None
1         test    127/127      0:28/ 0:00         0.000         {"EM": 0.4647, "F1": 0.599}  None
2         train   100/2737     0:41/18:25         0.981         {"position_acc": 0.7225}  8.3353
2         train   200/2737     1:07/14:21         0.969         {"position_acc": 0.7206}  6.4275
2         train   300/2737     1:34/12:43         0.995         {"position_acc": 0.7176}  9.533
2         train   400/2737     2:00/11:42         1.017         {"position_acc": 0.7114}  4.8961
2         train   500/2737     2:26/10:54         1.010         {"position_acc": 0.7131}  6.3521
2         train   600/2737     2:52/10:15         1.004         {"position_acc": 0.7126}  9.8949
2         train   700/2737     3:18/ 9:38         0.998         {"position_acc": 0.7128}  8.3985
2         train   800/2737     3:44/ 9:03         0.994         {"position_acc": 0.7124}  8.8781
2         train   900/2737     4:10/ 8:31         0.996         {"position_acc": 0.7125}  6.6849
2         train   1000/2737    4:36/ 8:00         0.989         {"position_acc": 0.7131}  8.8201
2         train   1100/2737    5:02/ 7:30         0.983         {"position_acc": 0.7136}  7.1427
2         train   1200/2737    5:28/ 7:00         0.987         {"position_acc": 0.712}  9.6484
2         train   1300/2737    5:54/ 6:31         0.991         {"position_acc": 0.7103}  11.3468
2         train   1400/2737    6:20/ 6:03         0.990         {"position_acc": 0.7103}  9.8398
2         train   1500/2737    6:46/ 5:35         0.992         {"position_acc": 0.7109}  9.851
2         train   1600/2737    7:12/ 5:07         0.991         {"position_acc": 0.7105}  7.2613
2         train   1700/2737    7:38/ 4:39         0.994         {"position_acc": 0.7103}  10.8123
2         train   1800/2737    8:04/ 4:12         0.997         {"position_acc": 0.7093}  7.6468
2         train   1900/2737    8:30/ 3:44         0.998         {"position_acc": 0.7086}  6.3304
2         train   2000/2737    8:56/ 3:17         0.995         {"position_acc": 0.7097}  7.4302
2         train   2100/2737    9:22/ 2:50         0.994         {"position_acc": 0.7103}  9.592
2         train   2200/2737    9:48/ 2:23         0.993         {"position_acc": 0.71}  48.5687
2         train   2300/2737   10:14/ 1:56         0.990         {"position_acc": 0.7105}  10.7127
2         train   2400/2737   10:40/ 1:29         0.986         {"position_acc": 0.7109}  12.2028
2         train   2500/2737   11:06/ 1:03         0.985         {"position_acc": 0.7114}  12.3569
2         train   2600/2737   11:32/ 0:36         0.982         {"position_acc": 0.7117}  13.1922
2         train   2700/2737   11:58/ 0:09         0.980         {"position_acc": 0.7118}  7.1035
False False
==================== begin saving model and validation ====================
2         train   2737/2737   12:08/ 0:00         0.981         {"position_acc": 0.7117}  None
10/04/2022 08:14:47 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/2.pkl
2         valid   1/330        0:10/56:53         0.000           None
2         valid   101/330      0:21/ 0:49         0.000           None
2         valid   201/330      0:32/ 0:21         0.000           None
2         valid   301/330      0:44/ 0:04         0.000           None
2         valid   330/330      0:49/ 0:00         0.000         {"EM": 0.8172, "F1": 0.8905}  None
2         test    1/127        0:09/20:37         0.000           None
2         test    101/127      0:20/ 0:05         0.000           None
2         test    127/127      0:24/ 0:00         0.000         {"EM": 0.4842, "F1": 0.612}  None
3         train   100/2737     0:37/16:30         0.910         {"position_acc": 0.74}  8.7893
3         train   200/2737     1:02/13:18         0.900         {"position_acc": 0.7369}  8.3846
3         train   300/2737     1:28/12:00         0.925         {"position_acc": 0.7255}  9.0766
3         train   400/2737     1:54/11:07         0.942         {"position_acc": 0.7206}  8.0008
3         train   500/2737     2:19/10:25         0.936         {"position_acc": 0.7242}  9.0212
3         train   600/2737     2:45/ 9:49         0.932         {"position_acc": 0.7252}  11.8581
3         train   700/2737     3:11/ 9:15         0.930         {"position_acc": 0.7246}  8.0671
3         train   800/2737     3:36/ 8:44         0.927         {"position_acc": 0.7257}  9.7285
3         train   900/2737     4:02/ 8:14         0.927         {"position_acc": 0.7251}  7.4183
3         train   1000/2737    4:27/ 7:44         0.919         {"position_acc": 0.7256}  12.7854
3         train   1100/2737    4:53/ 7:16         0.914         {"position_acc": 0.7262}  10.9196
3         train   1200/2737    5:18/ 6:48         0.913         {"position_acc": 0.7269}  14.9698
3         train   1300/2737    5:44/ 6:20         0.916         {"position_acc": 0.7261}  13.5692
3         train   1400/2737    6:09/ 5:52         0.915         {"position_acc": 0.7256}  10.2026
3         train   1500/2737    6:35/ 5:25         0.915         {"position_acc": 0.7262}  12.0481
3         train   1600/2737    7:00/ 4:58         0.913         {"position_acc": 0.7267}  6.9127
3         train   1700/2737    7:26/ 4:32         0.916         {"position_acc": 0.7254}  8.9981
3         train   1800/2737    7:51/ 4:05         0.916         {"position_acc": 0.7255}  7.066
3         train   1900/2737    8:17/ 3:39         0.915         {"position_acc": 0.7254}  6.0053
3         train   2000/2737    8:42/ 3:12         0.912         {"position_acc": 0.7265}  5.5815
3         train   2100/2737    9:08/ 2:46         0.909         {"position_acc": 0.728}  8.8364
3         train   2200/2737    9:34/ 2:20         0.909         {"position_acc": 0.7285}  9.2049
3         train   2300/2737   10:00/ 1:54         0.907         {"position_acc": 0.7287}  14.2565
3         train   2400/2737   10:26/ 1:27         0.904         {"position_acc": 0.7292}  10.148
3         train   2500/2737   10:51/ 1:01         0.903         {"position_acc": 0.7293}  6.7946
3         train   2600/2737   11:17/ 0:35         0.900         {"position_acc": 0.7303}  9.1832
3         train   2700/2737   11:43/ 0:09         0.898         {"position_acc": 0.7306}  12.5232
False False
==================== begin saving model and validation ====================
3         train   2737/2737   11:53/ 0:00         0.897         {"position_acc": 0.7307}  None
10/04/2022 08:27:55 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/3.pkl
3         valid   1/330        0:10/57:11         0.000           None
3         valid   101/330      0:21/ 0:49         0.000           None
3         valid   201/330      0:32/ 0:21         0.000           None
3         valid   301/330      0:44/ 0:04         0.000           None
3         valid   330/330      0:49/ 0:00         0.000         {"EM": 0.8266, "F1": 0.8976}  None
3         test    1/127        0:09/20:49         0.000           None
3         test    101/127      0:21/ 0:05         0.000           None
3         test    127/127      0:24/ 0:00         0.000         {"EM": 0.4886, "F1": 0.6259}  None
4         train   100/2737     0:37/16:19         0.829         {"position_acc": 0.7638}  6.0922
4         train   200/2737     1:02/13:14         0.810         {"position_acc": 0.7622}  11.1596
4         train   300/2737     1:28/11:58         0.822         {"position_acc": 0.7593}  9.2253
4         train   400/2737     1:54/11:06         0.827         {"position_acc": 0.7561}  9.3535
4         train   500/2737     2:19/10:25         0.821         {"position_acc": 0.7564}  7.9339
4         train   600/2737     2:45/ 9:49         0.820         {"position_acc": 0.7559}  13.0294
4         train   700/2737     3:11/ 9:16         0.813         {"position_acc": 0.7554}  7.622
4         train   800/2737     3:36/ 8:45         0.811         {"position_acc": 0.7554}  11.5601
4         train   900/2737     4:02/ 8:15         0.811         {"position_acc": 0.7558}  6.6209
4         train   1000/2737    4:28/ 7:45         0.808         {"position_acc": 0.756}  13.4852
4         train   1100/2737    4:53/ 7:16         0.807         {"position_acc": 0.7549}  6.4032
4         train   1200/2737    5:19/ 6:48         0.809         {"position_acc": 0.7546}  6.6954
4         train   1300/2737    5:44/ 6:21         0.809         {"position_acc": 0.7547}  13.9526
4         train   1400/2737    6:10/ 5:53         0.811         {"position_acc": 0.754}  10.8267
4         train   1500/2737    6:35/ 5:26         0.811         {"position_acc": 0.7533}  8.1149
4         train   1600/2737    7:01/ 4:59         0.809         {"position_acc": 0.7538}  6.9943
4         train   1700/2737    7:26/ 4:32         0.812         {"position_acc": 0.7535}  7.9574
4         train   1800/2737    7:51/ 4:05         0.814         {"position_acc": 0.7529}  6.5005
4         train   1900/2737    8:17/ 3:39         0.813         {"position_acc": 0.7526}  4.561
4         train   2000/2737    8:42/ 3:12         0.812         {"position_acc": 0.7531}  6.4025
4         train   2100/2737    9:08/ 2:46         0.811         {"position_acc": 0.7534}  8.0263
4         train   2200/2737    9:33/ 2:20         0.810         {"position_acc": 0.7536}  110.132
4         train   2300/2737    9:59/ 1:53         0.807         {"position_acc": 0.7545}  8.9512
4         train   2400/2737   10:24/ 1:27         0.806         {"position_acc": 0.7539}  8.348
4         train   2500/2737   10:50/ 1:01         0.803         {"position_acc": 0.7543}  8.6554
4         train   2600/2737   11:15/ 0:35         0.800         {"position_acc": 0.7545}  8.389
4         train   2700/2737   11:41/ 0:09         0.797         {"position_acc": 0.7553}  9.686
False False
==================== begin saving model and validation ====================
4         train   2737/2737   11:51/ 0:00         0.796         {"position_acc": 0.7554}  None
10/04/2022 08:41:00 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/4.pkl
4         valid   1/330        0:10/55:49         0.000           None
4         valid   101/330      0:21/ 0:48         0.000           None
4         valid   201/330      0:32/ 0:20         0.000           None
4         valid   301/330      0:44/ 0:04         0.000           None
4         valid   330/330      0:49/ 0:00         0.000         {"EM": 0.8353, "F1": 0.906}  None
4         test    1/127        0:09/20:59         0.000           None
4         test    101/127      0:21/ 0:05         0.000           None
4         test    127/127      0:24/ 0:00         0.000         {"EM": 0.4714, "F1": 0.6098}  None
5         train   100/2737     0:37/16:25         0.729         {"position_acc": 0.7762}  8.7611
5         train   200/2737     1:02/13:16         0.712         {"position_acc": 0.7784}  4.8752
5         train   300/2737     1:28/11:58         0.728         {"position_acc": 0.7741}  9.0397
5         train   400/2737     1:53/11:05         0.732         {"position_acc": 0.7708}  5.4201
5         train   500/2737     2:19/10:24         0.724         {"position_acc": 0.7738}  7.8098
5         train   600/2737     2:45/ 9:48         0.718         {"position_acc": 0.7749}  11.4882
5         train   700/2737     3:10/ 9:15         0.710         {"position_acc": 0.7758}  12.7898
5         train   800/2737     3:36/ 8:43         0.708         {"position_acc": 0.7769}  9.1004
5         train   900/2737     4:01/ 8:13         0.708         {"position_acc": 0.7774}  8.1878
5         train   1000/2737    4:27/ 7:44         0.703         {"position_acc": 0.7784}  16.6521
5         train   1100/2737    4:52/ 7:15         0.703         {"position_acc": 0.7779}  8.8394
5         train   1200/2737    5:18/ 6:48         0.706         {"position_acc": 0.7761}  11.8074
5         train   1300/2737    5:44/ 6:20         0.706         {"position_acc": 0.7753}  10.3314
5         train   1400/2737    6:09/ 5:53         0.707         {"position_acc": 0.7755}  8.3952
5         train   1500/2737    6:35/ 5:25         0.709         {"position_acc": 0.775}  9.0341
5         train   1600/2737    7:00/ 4:59         0.706         {"position_acc": 0.7761}  8.8593
5         train   1700/2737    7:26/ 4:32         0.709         {"position_acc": 0.7757}  12.2962
5         train   1800/2737    7:51/ 4:05         0.711         {"position_acc": 0.7747}  6.5789
5         train   1900/2737    8:17/ 3:39         0.712         {"position_acc": 0.7744}  8.9501
5         train   2000/2737    8:42/ 3:12         0.711         {"position_acc": 0.775}  5.0917
5         train   2100/2737    9:08/ 2:46         0.708         {"position_acc": 0.7756}  11.9869
5         train   2200/2737    9:33/ 2:20         0.708         {"position_acc": 0.7757}  4.8637
5         train   2300/2737    9:59/ 1:53         0.707         {"position_acc": 0.7757}  8.1629
5         train   2400/2737   10:24/ 1:27         0.705         {"position_acc": 0.7764}  10.3732
5         train   2500/2737   10:50/ 1:01         0.703         {"position_acc": 0.7771}  9.0054
5         train   2600/2737   11:16/ 0:35         0.700         {"position_acc": 0.7777}  24.0504
5         train   2700/2737   11:42/ 0:09         0.698         {"position_acc": 0.7781}  9.1037
False False
==================== begin saving model and validation ====================
5         train   2737/2737   11:53/ 0:00         0.697         {"position_acc": 0.7782}  None
10/04/2022 08:54:08 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/5.pkl
5         valid   1/330        0:12/65:50         0.000           None
5         valid   101/330      0:23/ 0:53         0.000           None
5         valid   201/330      0:35/ 0:22         0.000           None
5         valid   301/330      0:46/ 0:04         0.000           None
5         valid   330/330      0:51/ 0:00         0.000         {"EM": 0.8429, "F1": 0.9105}  None
5         test    1/127        0:11/23:35         0.000           None
5         test    101/127      0:22/ 0:05         0.000           None
5         test    127/127      0:26/ 0:00         0.000         {"EM": 0.4815, "F1": 0.6202}  None
6         train   100/2737     0:39/17:22         0.678         {"position_acc": 0.7887}  5.9289
6         train   200/2737     1:05/13:48         0.648         {"position_acc": 0.7903}  5.1567
6         train   300/2737     1:31/12:22         0.645         {"position_acc": 0.7926}  8.2676
6         train   400/2737     1:57/11:24         0.646         {"position_acc": 0.7897}  6.2824
6         train   500/2737     2:23/10:39         0.642         {"position_acc": 0.7912}  5.1492
6         train   600/2737     2:48/10:01         0.638         {"position_acc": 0.7929}  12.0881
6         train   700/2737     3:14/ 9:26         0.630         {"position_acc": 0.795}  11.2834
6         train   800/2737     3:40/ 8:53         0.623         {"position_acc": 0.7978}  10.085
6         train   900/2737     4:05/ 8:22         0.624         {"position_acc": 0.7979}  10.6658
6         train   1000/2737    4:31/ 7:51         0.618         {"position_acc": 0.7998}  9.231
6         train   1100/2737    4:57/ 7:22         0.618         {"position_acc": 0.7992}  9.8129
6         train   1200/2737    5:23/ 6:53         0.622         {"position_acc": 0.7976}  15.4521
6         train   1300/2737    5:48/ 6:25         0.622         {"position_acc": 0.7973}  9.0327
6         train   1400/2737    6:14/ 5:57         0.623         {"position_acc": 0.7966}  9.357
6         train   1500/2737    6:40/ 5:30         0.623         {"position_acc": 0.7969}  7.4665
6         train   1600/2737    7:06/ 5:02         0.620         {"position_acc": 0.7979}  9.6027
6         train   1700/2737    7:31/ 4:35         0.621         {"position_acc": 0.7976}  9.4922
6         train   1800/2737    7:57/ 4:08         0.622         {"position_acc": 0.7972}  4.6731
6         train   1900/2737    8:23/ 3:41         0.623         {"position_acc": 0.7967}  8.3413
6         train   2000/2737    8:49/ 3:15         0.622         {"position_acc": 0.7974}  6.3176
6         train   2100/2737    9:14/ 2:48         0.620         {"position_acc": 0.7976}  8.5642
6         train   2200/2737    9:41/ 2:21         0.621         {"position_acc": 0.7976}  5.7465
6         train   2300/2737   10:07/ 1:55         0.619         {"position_acc": 0.798}  9.0801
6         train   2400/2737   10:33/ 1:28         0.618         {"position_acc": 0.7983}  10.5116
6         train   2500/2737   11:00/ 1:02         0.616         {"position_acc": 0.7984}  7.7844
6         train   2600/2737   11:26/ 0:36         0.613         {"position_acc": 0.799}  8.1891
6         train   2700/2737   11:52/ 0:09         0.611         {"position_acc": 0.7994}  8.8804
False False
==================== begin saving model and validation ====================
6         train   2737/2737   12:04/ 0:00         0.611         {"position_acc": 0.7995}  None
10/04/2022 09:07:30 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/6.pkl
6         valid   1/330        0:11/64:57         0.000           None
6         valid   101/330      0:23/ 0:53         0.000           None
6         valid   201/330      0:35/ 0:22         0.000           None
6         valid   301/330      0:46/ 0:04         0.000           None
6         valid   330/330      0:51/ 0:00         0.000         {"EM": 0.8462, "F1": 0.9139}  None
6         test    1/127        0:11/24:02         0.000           None
6         test    101/127      0:22/ 0:05         0.000           None
6         test    127/127      0:25/ 0:00         0.000         {"EM": 0.4889, "F1": 0.6266}  None
7         train   100/2737     0:37/16:21         0.592         {"position_acc": 0.8125}  6.085
7         train   200/2737     1:02/13:10         0.550         {"position_acc": 0.8166}  4.7565
7         train   300/2737     1:27/11:52         0.565         {"position_acc": 0.8197}  8.3482
7         train   400/2737     1:53/11:00         0.560         {"position_acc": 0.817}  4.3309
7         train   500/2737     2:18/10:19         0.554         {"position_acc": 0.8183}  6.3827
7         train   600/2737     2:43/ 9:43         0.554         {"position_acc": 0.8195}  37.318
7         train   700/2737     3:09/ 9:10         0.546         {"position_acc": 0.8215}  2.8033
7         train   800/2737     3:34/ 8:39         0.543         {"position_acc": 0.822}  7.2279
7         train   900/2737     4:00/ 8:09         0.547         {"position_acc": 0.8197}  5.1115
7         train   1000/2737    4:25/ 7:40         0.541         {"position_acc": 0.8205}  9.2456
7         train   1100/2737    4:50/ 7:12         0.543         {"position_acc": 0.8193}  8.0379
7         train   1200/2737    5:15/ 6:44         0.544         {"position_acc": 0.8189}  8.3533
7         train   1300/2737    5:41/ 6:17         0.543         {"position_acc": 0.8186}  5.3031
7         train   1400/2737    6:06/ 5:50         0.546         {"position_acc": 0.8181}  12.2291
7         train   1500/2737    6:31/ 5:23         0.548         {"position_acc": 0.8185}  7.3729
7         train   1600/2737    6:57/ 4:56         0.547         {"position_acc": 0.8193}  9.3133
7         train   1700/2737    7:22/ 4:29         0.548         {"position_acc": 0.8189}  13.3679
7         train   1800/2737    7:47/ 4:03         0.550         {"position_acc": 0.8179}  5.0264
7         train   1900/2737    8:12/ 3:37         0.551         {"position_acc": 0.8172}  6.4447
7         train   2000/2737    8:38/ 3:11         0.550         {"position_acc": 0.8172}  6.8804
7         train   2100/2737    9:04/ 2:45         0.549         {"position_acc": 0.8172}  8.5756
7         train   2200/2737    9:30/ 2:19         0.551         {"position_acc": 0.8173}  5.8488
7         train   2300/2737    9:56/ 1:53         0.551         {"position_acc": 0.8174}  10.208
7         train   2400/2737   10:22/ 1:27         0.551         {"position_acc": 0.817}  7.8283
7         train   2500/2737   10:48/ 1:01         0.550         {"position_acc": 0.817}  8.2904
7         train   2600/2737   11:13/ 0:35         0.550         {"position_acc": 0.8173}  7.3473
7         train   2700/2737   11:39/ 0:09         0.548         {"position_acc": 0.8178}  7.4307
False False
==================== begin saving model and validation ====================
7         train   2737/2737   11:50/ 0:00         0.549         {"position_acc": 0.8177}  None
10/04/2022 09:20:38 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/7.pkl
7         valid   1/330        0:12/66:18         0.000           None
7         valid   101/330      0:23/ 0:53         0.000           None
7         valid   201/330      0:35/ 0:22         0.000           None
7         valid   301/330      0:46/ 0:04         0.000           None
7         valid   330/330      0:51/ 0:00         0.000         {"EM": 0.8468, "F1": 0.9143}  None
7         test    1/127        0:11/24:04         0.000           None
7         test    101/127      0:22/ 0:05         0.000           None
7         test    127/127      0:26/ 0:00         0.000         {"EM": 0.4894, "F1": 0.6249}  None
8         train   100/2737     0:39/17:21         0.525         {"position_acc": 0.8225}  23.467
8         train   200/2737     1:05/13:45         0.515         {"position_acc": 0.8253}  5.7957
8         train   300/2737     1:30/12:18         0.523         {"position_acc": 0.8257}  7.4819
8         train   400/2737     1:56/11:21         0.525         {"position_acc": 0.8254}  8.7239
8         train   500/2737     2:22/10:36         0.520         {"position_acc": 0.828}  7.0875
8         train   600/2737     2:48/ 9:58         0.522         {"position_acc": 0.8267}  11.7726
8         train   700/2737     3:13/ 9:23         0.521         {"position_acc": 0.827}  4.5103
8         train   800/2737     3:38/ 8:50         0.523         {"position_acc": 0.828}  7.2356
8         train   900/2737     4:04/ 8:18         0.530         {"position_acc": 0.8257}  7.6252
8         train   1000/2737    4:29/ 7:48         0.525         {"position_acc": 0.8267}  9.0751
8         train   1100/2737    4:55/ 7:19         0.528         {"position_acc": 0.8255}  5.9853
8         train   1200/2737    5:20/ 6:51         0.529         {"position_acc": 0.8248}  11.5833
8         train   1300/2737    5:47/ 6:23         0.529         {"position_acc": 0.8245}  5.6378
8         train   1400/2737    6:13/ 5:56         0.532         {"position_acc": 0.8236}  9.9553
8         train   1500/2737    6:40/ 5:29         0.533         {"position_acc": 0.8243}  7.6096
8         train   1600/2737    7:06/ 5:02         0.532         {"position_acc": 0.8239}  11.027
8         train   1700/2737    7:32/ 4:35         0.535         {"position_acc": 0.8231}  11.7925
8         train   1800/2737    7:58/ 4:09         0.538         {"position_acc": 0.8217}  4.0244
8         train   1900/2737    8:24/ 3:42         0.540         {"position_acc": 0.8204}  5.7868
8         train   2000/2737    8:50/ 3:15         0.539         {"position_acc": 0.8205}  5.2845
8         train   2100/2737    9:15/ 2:48         0.540         {"position_acc": 0.8204}  11.7538
8         train   2200/2737    9:42/ 2:22         0.542         {"position_acc": 0.8198}  7.1554
8         train   2300/2737   10:08/ 1:55         0.541         {"position_acc": 0.8202}  12.3941
8         train   2400/2737   10:34/ 1:29         0.542         {"position_acc": 0.8197}  11.3155
8         train   2500/2737   11:01/ 1:02         0.541         {"position_acc": 0.8193}  7.8115
8         train   2600/2737   11:27/ 0:36         0.541         {"position_acc": 0.8191}  9.1707
8         train   2700/2737   11:53/ 0:09         0.540         {"position_acc": 0.8192}  12.4435
False False
==================== begin saving model and validation ====================
8         train   2737/2737   12:04/ 0:00         0.540         {"position_acc": 0.8194}  None
10/04/2022 09:34:00 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/8.pkl
8         valid   1/330        0:10/56:14         0.000           None
8         valid   101/330      0:21/ 0:48         0.000           None
8         valid   201/330      0:32/ 0:20         0.000           None
8         valid   301/330      0:43/ 0:04         0.000           None
8         valid   330/330      0:49/ 0:00         0.000         {"EM": 0.8468, "F1": 0.9143}  None
8         test    1/127        0:11/24:12         0.000           None
8         test    101/127      0:22/ 0:05         0.000           None
8         test    127/127      0:26/ 0:00         0.000         {"EM": 0.4894, "F1": 0.6249}  None
9         train   100/2737     0:37/16:28         0.568         {"position_acc": 0.8206}  5.8763
9         train   200/2737     1:02/13:15         0.542         {"position_acc": 0.8184}  3.8506
9         train   300/2737     1:28/11:57         0.541         {"position_acc": 0.8224}  5.9143
9         train   400/2737     1:53/11:04         0.539         {"position_acc": 0.8234}  3.7255
9         train   500/2737     2:19/10:23         0.534         {"position_acc": 0.8239}  8.0689
9         train   600/2737     2:44/ 9:47         0.530         {"position_acc": 0.8254}  13.1997
9         train   700/2737     3:10/ 9:14         0.525         {"position_acc": 0.8259}  2.6167
9         train   800/2737     3:36/ 8:43         0.527         {"position_acc": 0.825}  9.1018
9         train   900/2737     4:01/ 8:13         0.532         {"position_acc": 0.8239}  14.5988
9         train   1000/2737    4:28/ 7:46         0.528         {"position_acc": 0.8248}  7.9693
9         train   1100/2737    4:54/ 7:18         0.530         {"position_acc": 0.8234}  7.4058
9         train   1200/2737    5:21/ 6:51         0.533         {"position_acc": 0.8229}  12.8642
9         train   1300/2737    5:47/ 6:23         0.535         {"position_acc": 0.8221}  6.833
9         train   1400/2737    6:13/ 5:56         0.538         {"position_acc": 0.8207}  7.003
9         train   1500/2737    6:39/ 5:29         0.538         {"position_acc": 0.8216}  7.7772
9         train   1600/2737    7:06/ 5:02         0.538         {"position_acc": 0.8214}  8.3179
9         train   1700/2737    7:32/ 4:35         0.539         {"position_acc": 0.8208}  45.8908
9         train   1800/2737    7:58/ 4:09         0.541         {"position_acc": 0.8202}  4.2446
9         train   1900/2737    8:24/ 3:42         0.542         {"position_acc": 0.8195}  4.7704
9         train   2000/2737    8:51/ 3:15         0.541         {"position_acc": 0.8202}  3.6368
9         train   2100/2737    9:17/ 2:49         0.541         {"position_acc": 0.8202}  7.7186
9         train   2200/2737    9:43/ 2:22         0.542         {"position_acc": 0.8196}  7.4123
9         train   2300/2737   10:10/ 1:55         0.541         {"position_acc": 0.8199}  8.5233
9         train   2400/2737   10:36/ 1:29         0.542         {"position_acc": 0.82}  8.8224
9         train   2500/2737   11:02/ 1:02         0.541         {"position_acc": 0.8197}  6.6869
9         train   2600/2737   11:28/ 0:36         0.541         {"position_acc": 0.8199}  6.1017
9         train   2700/2737   11:55/ 0:09         0.541         {"position_acc": 0.82}  9.8332
False False
==================== begin saving model and validation ====================
9         train   2737/2737   12:06/ 0:00         0.541         {"position_acc": 0.82}  None
10/04/2022 09:47:22 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/9.pkl
9         valid   1/330        0:13/74:16         0.000           None
9         valid   101/330      0:25/ 0:57         0.000           None
9         valid   201/330      0:36/ 0:23         0.000           None
9         valid   301/330      0:48/ 0:04         0.000           None
9         valid   330/330      0:53/ 0:00         0.000         {"EM": 0.8468, "F1": 0.9143}  None
9         test    1/127        0:12/26:41         0.000           None
9         test    101/127      0:24/ 0:06         0.000           None
9         test    127/127      0:27/ 0:00         0.000         {"EM": 0.4894, "F1": 0.6249}  None
10        train   100/2737     0:41/18:15         0.556         {"position_acc": 0.83}  4.7238
10        train   200/2737     1:07/14:17         0.526         {"position_acc": 0.8269}  3.9695
10        train   300/2737     1:33/12:41         0.531         {"position_acc": 0.8288}  6.4577
10        train   400/2737     1:59/11:40         0.531         {"position_acc": 0.8262}  4.1806
10        train   500/2737     2:26/10:54         0.526         {"position_acc": 0.8258}  8.4177
10        train   600/2737     2:52/10:14         0.525         {"position_acc": 0.8266}  10.3331
10        train   700/2737     3:18/ 9:38         0.522         {"position_acc": 0.8266}  5.8098
10        train   800/2737     3:45/ 9:05         0.524         {"position_acc": 0.8262}  7.0125
10        train   900/2737     4:11/ 8:33         0.529         {"position_acc": 0.8253}  7.4524
10        train   1000/2737    4:37/ 8:02         0.523         {"position_acc": 0.8272}  7.4815
10        train   1100/2737    5:03/ 7:32         0.527         {"position_acc": 0.8257}  6.0549
10        train   1200/2737    5:30/ 7:02         0.529         {"position_acc": 0.8249}  12.5277
10        train   1300/2737    5:56/ 6:34         0.530         {"position_acc": 0.8238}  8.2921
10        train   1400/2737    6:22/ 6:05         0.533         {"position_acc": 0.8221}  8.2615
10        train   1500/2737    6:49/ 5:37         0.534         {"position_acc": 0.8227}  8.8266
10        train   1600/2737    7:15/ 5:09         0.534         {"position_acc": 0.823}  15.2661
10        train   1700/2737    7:41/ 4:41         0.537         {"position_acc": 0.8216}  13.5034
10        train   1800/2737    8:08/ 4:14         0.540         {"position_acc": 0.8203}  4.5505
10        train   1900/2737    8:34/ 3:46         0.543         {"position_acc": 0.8193}  5.7123
10        train   2000/2737    9:00/ 3:19         0.542         {"position_acc": 0.8196}  3.8655
10        train   2100/2737    9:27/ 2:52         0.542         {"position_acc": 0.8196}  10.3693
10        train   2200/2737    9:53/ 2:24         0.543         {"position_acc": 0.8197}  6.9939
10        train   2300/2737   10:20/ 1:57         0.542         {"position_acc": 0.8196}  11.7765
10        train   2400/2737   10:46/ 1:30         0.542         {"position_acc": 0.8193}  6.5449
10        train   2500/2737   11:12/ 1:03         0.543         {"position_acc": 0.8187}  5.5721
10        train   2600/2737   11:38/ 0:36         0.544         {"position_acc": 0.8184}  6.3996
10        train   2700/2737   12:05/ 0:09         0.543         {"position_acc": 0.8186}  6.4252
False False
==================== begin saving model and validation ====================
10        train   2737/2737   12:16/ 0:00         0.542         {"position_acc": 0.8189}  None
10/04/2022 10:00:59 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/10.pkl
10        valid   1/330        0:13/74:19         0.000           None
10        valid   101/330      0:25/ 0:57         0.000           None
10        valid   201/330      0:36/ 0:23         0.000           None
10        valid   301/330      0:48/ 0:04         0.000           None
10        valid   330/330      0:53/ 0:00         0.000         {"EM": 0.8468, "F1": 0.9143}  None
10        test    1/127        0:12/27:03         0.000           None
10        test    101/127      0:24/ 0:06         0.000           None
10        test    127/127      0:27/ 0:00         0.000         {"EM": 0.4894, "F1": 0.6249}  None
11        train   100/2737     0:41/18:15         0.537         {"position_acc": 0.8369}  7.8515
11        train   200/2737     1:07/14:16         0.522         {"position_acc": 0.8278}  4.0516
11        train   300/2737     1:33/12:40         0.537         {"position_acc": 0.8265}  7.8737
11        train   400/2737     1:59/11:39         0.535         {"position_acc": 0.824}  3.8874
11        train   500/2737     2:26/10:54         0.533         {"position_acc": 0.8241}  7.7796
11        train   600/2737     2:52/10:14         0.538         {"position_acc": 0.8239}  11.7538
11        train   700/2737     3:18/ 9:38         0.533         {"position_acc": 0.8238}  3.0186
11        train   800/2737     3:44/ 9:04         0.530         {"position_acc": 0.8253}  9.9498
11        train   900/2737     4:11/ 8:32         0.534         {"position_acc": 0.8242}  5.5392
11        train   1000/2737    4:37/ 8:02         0.528         {"position_acc": 0.8256}  8.2464
11        train   1100/2737    5:03/ 7:31         0.528         {"position_acc": 0.8248}  8.408
11        train   1200/2737    5:29/ 7:02         0.532         {"position_acc": 0.8233}  6.9573
11        train   1300/2737    5:55/ 6:33         0.532         {"position_acc": 0.8232}  5.9251
11        train   1400/2737    6:22/ 6:05         0.534         {"position_acc": 0.8222}  11.711
11        train   1500/2737    6:48/ 5:37         0.535         {"position_acc": 0.8221}  7.5685
11        train   1600/2737    7:14/ 5:09         0.536         {"position_acc": 0.8211}  9.9821
11        train   1700/2737    7:41/ 4:41         0.539         {"position_acc": 0.8197}  14.1206
11        train   1800/2737    8:07/ 4:13         0.541         {"position_acc": 0.8193}  5.4303
11        train   1900/2737    8:33/ 3:46         0.543         {"position_acc": 0.8182}  5.271
11        train   2000/2737    8:59/ 3:18         0.543         {"position_acc": 0.8187}  3.6121
11        train   2100/2737    9:25/ 2:51         0.542         {"position_acc": 0.8189}  6.9851
11        train   2200/2737    9:52/ 2:24         0.545         {"position_acc": 0.8185}  6.4505
11        train   2300/2737   10:18/ 1:57         0.544         {"position_acc": 0.819}  13.886
11        train   2400/2737   10:44/ 1:30         0.543         {"position_acc": 0.8196}  8.4013
11        train   2500/2737   11:10/ 1:03         0.543         {"position_acc": 0.8193}  7.2156
11        train   2600/2737   11:36/ 0:36         0.543         {"position_acc": 0.8196}  15.3676
11        train   2700/2737   12:03/ 0:09         0.542         {"position_acc": 0.8197}  7.7739
False False
==================== begin saving model and validation ====================
11        train   2737/2737   12:13/ 0:00         0.542         {"position_acc": 0.8197}  None
10/04/2022 10:14:35 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/11.pkl
11        valid   1/330        0:13/73:52         0.000           None
11        valid   101/330      0:25/ 0:56         0.000           None
11        valid   201/330      0:36/ 0:23         0.000           None
11        valid   301/330      0:48/ 0:04         0.000           None
11        valid   330/330      0:53/ 0:00         0.000         {"EM": 0.8468, "F1": 0.9143}  None
11        test    1/127        0:12/26:43         0.000           None
11        test    101/127      0:24/ 0:06         0.000           None
11        test    127/127      0:27/ 0:00         0.000         {"EM": 0.4894, "F1": 0.6249}  None
12        train   100/2737     0:41/18:16         0.545         {"position_acc": 0.8225}  4.8902
12        train   200/2737     1:07/14:16         0.533         {"position_acc": 0.8206}  4.0462
12        train   300/2737     1:33/12:40         0.538         {"position_acc": 0.8253}  8.1209
12        train   400/2737     1:59/11:40         0.540         {"position_acc": 0.8243}  4.2282
12        train   500/2737     2:26/10:53         0.537         {"position_acc": 0.8239}  8.0508
12        train   600/2737     2:52/10:14         0.535         {"position_acc": 0.8258}  16.1652
12        train   700/2737     3:18/ 9:38         0.531         {"position_acc": 0.8266}  3.3242
12        train   800/2737     3:45/ 9:05         0.527         {"position_acc": 0.8278}  8.0103
12        train   900/2737     4:11/ 8:33         0.531         {"position_acc": 0.8254}  9.1824
12        train   1000/2737    4:37/ 8:02         0.525         {"position_acc": 0.8264}  8.764
12        train   1100/2737    5:03/ 7:32         0.529         {"position_acc": 0.8244}  6.6056
12        train   1200/2737    5:30/ 7:03         0.533         {"position_acc": 0.8235}  10.3885
12        train   1300/2737    5:56/ 6:34         0.532         {"position_acc": 0.8227}  7.0686
12        train   1400/2737    6:22/ 6:05         0.533         {"position_acc": 0.8224}  7.6933
12        train   1500/2737    6:49/ 5:37         0.535         {"position_acc": 0.8224}  9.034
12        train   1600/2737    7:15/ 5:09         0.535         {"position_acc": 0.8225}  13.6212
12        train   1700/2737    7:41/ 4:41         0.538         {"position_acc": 0.8213}  14.8939
12        train   1800/2737    8:07/ 4:13         0.540         {"position_acc": 0.82}  3.6806
12        train   1900/2737    8:33/ 3:46         0.542         {"position_acc": 0.8187}  6.5689
12        train   2000/2737    9:00/ 3:19         0.541         {"position_acc": 0.8191}  8.0034
12        train   2100/2737    9:26/ 2:51         0.541         {"position_acc": 0.8192}  7.5421
12        train   2200/2737    9:52/ 2:24         0.543         {"position_acc": 0.819}  4.5166
12        train   2300/2737   10:19/ 1:57         0.541         {"position_acc": 0.8195}  8.5749
12        train   2400/2737   10:45/ 1:30         0.542         {"position_acc": 0.8189}  7.9088
12        train   2500/2737   11:11/ 1:03         0.542         {"position_acc": 0.8188}  7.5198
12        train   2600/2737   11:37/ 0:36         0.541         {"position_acc": 0.819}  5.9189
12        train   2700/2737   12:04/ 0:09         0.540         {"position_acc": 0.8189}  11.5631
False False
==================== begin saving model and validation ====================
12        train   2737/2737   12:14/ 0:00         0.540         {"position_acc": 0.8187}  None
10/04/2022 10:28:11 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/12.pkl
12        valid   1/330        0:13/73:55         0.000           None
12        valid   101/330      0:25/ 0:56         0.000           None
12        valid   201/330      0:36/ 0:23         0.000           None
12        valid   301/330      0:48/ 0:04         0.000           None
12        valid   330/330      0:53/ 0:00         0.000         {"EM": 0.8468, "F1": 0.9143}  None
12        test    1/127        0:12/27:01         0.000           None
12        test    101/127      0:24/ 0:06         0.000           None
12        test    127/127      0:27/ 0:00         0.000         {"EM": 0.4894, "F1": 0.6249}  None
13        train   100/2737     0:41/18:23         0.542         {"position_acc": 0.8306}  7.9906
13        train   200/2737     1:07/14:19         0.521         {"position_acc": 0.8269}  4.6806
13        train   300/2737     1:33/12:41         0.527         {"position_acc": 0.824}  6.0416
13        train   400/2737     1:59/11:39         0.532         {"position_acc": 0.8207}  2.7794
13        train   500/2737     2:25/10:52         0.527         {"position_acc": 0.8224}  6.8985
13        train   600/2737     2:52/10:12         0.527         {"position_acc": 0.824}  13.114
13        train   700/2737     3:18/ 9:37         0.526         {"position_acc": 0.8239}  2.9666
13        train   800/2737     3:44/ 9:04         0.524         {"position_acc": 0.8266}  6.548
13        train   900/2737     4:11/ 8:32         0.527         {"position_acc": 0.8249}  5.7342
13        train   1000/2737    4:37/ 8:01         0.522         {"position_acc": 0.8268}  7.8986
13        train   1100/2737    5:03/ 7:32         0.524         {"position_acc": 0.8251}  9.4005
13        train   1200/2737    5:30/ 7:03         0.528         {"position_acc": 0.8234}  7.6977
13        train   1300/2737    5:56/ 6:34         0.528         {"position_acc": 0.8225}  5.9117
13        train   1400/2737    6:23/ 6:05         0.529         {"position_acc": 0.8221}  9.0052
13        train   1500/2737    6:49/ 5:37         0.531         {"position_acc": 0.8227}  6.6923
13        train   1600/2737    7:15/ 5:09         0.532         {"position_acc": 0.8223}  9.9246
13        train   1700/2737    7:42/ 4:41         0.536         {"position_acc": 0.8212}  12.1118
13        train   1800/2737    8:08/ 4:14         0.539         {"position_acc": 0.8201}  3.723
13        train   1900/2737    8:34/ 3:46         0.541         {"position_acc": 0.8195}  5.2233
13        train   2000/2737    9:01/ 3:19         0.540         {"position_acc": 0.8197}  4.2955
13        train   2100/2737    9:27/ 2:52         0.540         {"position_acc": 0.8198}  8.471
13        train   2200/2737    9:53/ 2:24         0.542         {"position_acc": 0.8196}  6.8901
13        train   2300/2737   10:20/ 1:57         0.541         {"position_acc": 0.8197}  10.6146
13        train   2400/2737   10:46/ 1:30         0.543         {"position_acc": 0.8189}  7.8845
13        train   2500/2737   11:12/ 1:03         0.542         {"position_acc": 0.8187}  5.7019
13        train   2600/2737   11:38/ 0:36         0.541         {"position_acc": 0.8192}  7.0866
13        train   2700/2737   12:05/ 0:09         0.540         {"position_acc": 0.8192}  7.5134
False False
==================== begin saving model and validation ====================
13        train   2737/2737   12:16/ 0:00         0.540         {"position_acc": 0.8191}  None
10/04/2022 10:41:48 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/13.pkl
13        valid   1/330        0:13/74:21         0.000           None
13        valid   101/330      0:25/ 0:57         0.000           None
13        valid   201/330      0:36/ 0:23         0.000           None
13        valid   301/330      0:48/ 0:04         0.000           None
13        valid   330/330      0:53/ 0:00         0.000         {"EM": 0.8468, "F1": 0.9143}  None
13        test    1/127        0:12/26:39         0.000           None
13        test    101/127      0:24/ 0:06         0.000           None
13        test    127/127      0:28/ 0:00         0.000         {"EM": 0.4894, "F1": 0.6249}  None
14        train   100/2737     0:41/18:16         0.547         {"position_acc": 0.8244}  6.7831
14        train   200/2737     1:07/14:15         0.521         {"position_acc": 0.8253}  4.4962
14        train   300/2737     1:33/12:40         0.536         {"position_acc": 0.824}  5.9456
14        train   400/2737     1:59/11:40         0.536         {"position_acc": 0.8245}  4.7382
14        train   500/2737     2:26/10:53         0.531         {"position_acc": 0.8241}  5.8093
14        train   600/2737     2:52/10:14         0.531         {"position_acc": 0.8249}  13.2119
14        train   700/2737     3:18/ 9:38         0.527         {"position_acc": 0.8263}  2.7657
14        train   800/2737     3:45/ 9:05         0.525         {"position_acc": 0.8267}  8.082
14        train   900/2737     4:11/ 8:33         0.530         {"position_acc": 0.825}  5.4183
14        train   1000/2737    4:38/ 8:03         0.524         {"position_acc": 0.8267}  8.2251
14        train   1100/2737    5:04/ 7:33         0.526         {"position_acc": 0.8252}  8.2396
14        train   1200/2737    5:30/ 7:03         0.531         {"position_acc": 0.8236}  15.0147
14        train   1300/2737    5:57/ 6:34         0.531         {"position_acc": 0.8235}  11.5744
14        train   1400/2737    6:23/ 6:06         0.534         {"position_acc": 0.8231}  10.64
14        train   1500/2737    6:49/ 5:37         0.536         {"position_acc": 0.8236}  6.9347
14        train   1600/2737    7:15/ 5:09         0.536         {"position_acc": 0.824}  12.8348
14        train   1700/2737    7:42/ 4:42         0.539         {"position_acc": 0.8229}  15.6977
14        train   1800/2737    8:08/ 4:14         0.541         {"position_acc": 0.8223}  4.1948
14        train   1900/2737    8:34/ 3:46         0.543         {"position_acc": 0.8212}  5.2023
14        train   2000/2737    9:01/ 3:19         0.542         {"position_acc": 0.8215}  4.6024
14        train   2100/2737    9:27/ 2:52         0.543         {"position_acc": 0.8212}  14.955
14        train   2200/2737    9:53/ 2:24         0.544         {"position_acc": 0.8207}  6.774
14        train   2300/2737   10:20/ 1:57         0.543         {"position_acc": 0.8208}  8.0253
14        train   2400/2737   10:46/ 1:30         0.542         {"position_acc": 0.8208}  10.518
14        train   2500/2737   11:12/ 1:03         0.542         {"position_acc": 0.8204}  6.6753
14        train   2600/2737   11:38/ 0:36         0.541         {"position_acc": 0.8208}  6.8459
14        train   2700/2737   12:05/ 0:09         0.541         {"position_acc": 0.8208}  6.0037
False False
==================== begin saving model and validation ====================
14        train   2737/2737   12:16/ 0:00         0.541         {"position_acc": 0.8205}  None
10/04/2022 10:55:26 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/14.pkl
14        valid   1/330        0:13/73:53         0.000           None
14        valid   101/330      0:25/ 0:57         0.000           None
14        valid   201/330      0:36/ 0:23         0.000           None
14        valid   301/330      0:48/ 0:04         0.000           None
14        valid   330/330      0:53/ 0:00         0.000         {"EM": 0.8468, "F1": 0.9143}  None
14        test    1/127        0:12/25:57         0.000           None
14        test    101/127      0:23/ 0:06         0.000           None
14        test    127/127      0:28/ 0:00         0.000         {"EM": 0.4894, "F1": 0.6249}  None
15        train   100/2737     0:42/18:31         0.542         {"position_acc": 0.8219}  5.6621
15        train   200/2737     1:08/14:22         0.524         {"position_acc": 0.8269}  5.2762
15        train   300/2737     1:34/12:44         0.529         {"position_acc": 0.828}  9.7871
15        train   400/2737     2:00/11:42         0.536         {"position_acc": 0.8256}  4.795
15        train   500/2737     2:26/10:56         0.530         {"position_acc": 0.8258}  8.9509
15        train   600/2737     2:53/10:16         0.530         {"position_acc": 0.8271}  13.2312
15        train   700/2737     3:19/ 9:40         0.524         {"position_acc": 0.8276}  3.2523
15        train   800/2737     3:45/ 9:06         0.526         {"position_acc": 0.8276}  7.4943
15        train   900/2737     4:12/ 8:34         0.531         {"position_acc": 0.8256}  11.3035
15        train   1000/2737    4:38/ 8:03         0.526         {"position_acc": 0.8269}  11.6615
15        train   1100/2737    5:04/ 7:32         0.528         {"position_acc": 0.825}  7.3517
15        train   1200/2737    5:30/ 7:03         0.529         {"position_acc": 0.8243}  13.5699
15        train   1300/2737    5:56/ 6:34         0.531         {"position_acc": 0.8228}  5.013
15        train   1400/2737    6:22/ 6:05         0.532         {"position_acc": 0.8222}  7.4985
15        train   1500/2737    6:49/ 5:37         0.535         {"position_acc": 0.8222}  6.6292
15        train   1600/2737    7:15/ 5:09         0.536         {"position_acc": 0.8219}  8.8781
15        train   1700/2737    7:41/ 4:41         0.539         {"position_acc": 0.8214}  22.9146
15        train   1800/2737    8:07/ 4:13         0.542         {"position_acc": 0.82}  21.3576
15        train   1900/2737    8:33/ 3:46         0.543         {"position_acc": 0.819}  4.7846
15        train   2000/2737    9:00/ 3:19         0.542         {"position_acc": 0.8191}  5.2661
15        train   2100/2737    9:26/ 2:51         0.542         {"position_acc": 0.8194}  12.532
15        train   2200/2737    9:52/ 2:24         0.544         {"position_acc": 0.8196}  6.4964
15        train   2300/2737   10:19/ 1:57         0.543         {"position_acc": 0.8198}  12.4444
15        train   2400/2737   10:45/ 1:30         0.543         {"position_acc": 0.8198}  7.952
15        train   2500/2737   11:11/ 1:03         0.542         {"position_acc": 0.8198}  5.4393
15        train   2600/2737   11:38/ 0:36         0.541         {"position_acc": 0.82}  6.0901
15        train   2700/2737   12:04/ 0:09         0.540         {"position_acc": 0.8203}  6.813
False False
==================== begin saving model and validation ====================
15        train   2737/2737   12:15/ 0:00         0.540         {"position_acc": 0.8202}  None
10/04/2022 11:09:03 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/15.pkl
15        valid   1/330        0:13/73:53         0.000           None
15        valid   101/330      0:25/ 0:56         0.000           None
15        valid   201/330      0:37/ 0:23         0.000           None
15        valid   301/330      0:48/ 0:04         0.000           None
15        valid   330/330      0:53/ 0:00         0.000         {"EM": 0.8468, "F1": 0.9143}  None
15        test    1/127        0:12/27:14         0.000           None
15        test    101/127      0:24/ 0:06         0.000           None
15        test    127/127      0:28/ 0:00         0.000         {"EM": 0.4894, "F1": 0.6249}  None
16        train   100/2737     0:41/18:25         0.554         {"position_acc": 0.8263}  5.3503
16        train   200/2737     1:07/14:18         0.528         {"position_acc": 0.8231}  5.0951
16        train   300/2737     1:33/12:41         0.533         {"position_acc": 0.8207}  7.53
16        train   400/2737     1:59/11:40         0.532         {"position_acc": 0.8212}  3.3135
16        train   500/2737     2:26/10:53         0.525         {"position_acc": 0.8241}  8.5688
16        train   600/2737     2:52/10:13         0.527         {"position_acc": 0.8248}  6.6301
16        train   700/2737     3:18/ 9:38         0.524         {"position_acc": 0.8258}  4.0516
16        train   800/2737     3:45/ 9:05         0.522         {"position_acc": 0.8282}  7.1621
16        train   900/2737     4:11/ 8:33         0.528         {"position_acc": 0.8264}  9.3541
16        train   1000/2737    4:37/ 8:02         0.524         {"position_acc": 0.8266}  10.4496
16        train   1100/2737    5:03/ 7:32         0.527         {"position_acc": 0.8252}  5.4698
16        train   1200/2737    5:30/ 7:02         0.530         {"position_acc": 0.8242}  5.9178
16        train   1300/2737    5:56/ 6:33         0.528         {"position_acc": 0.8237}  11.6948
16        train   1400/2737    6:22/ 6:05         0.532         {"position_acc": 0.8219}  10.4881
16        train   1500/2737    6:48/ 5:37         0.534         {"position_acc": 0.8227}  7.03
16        train   1600/2737    7:14/ 5:09         0.533         {"position_acc": 0.823}  9.458
16        train   1700/2737    7:41/ 4:41         0.536         {"position_acc": 0.8224}  16.2254
16        train   1800/2737    8:07/ 4:13         0.538         {"position_acc": 0.8217}  4.2836
16        train   1900/2737    8:33/ 3:46         0.540         {"position_acc": 0.8209}  6.8001
16        train   2000/2737    8:59/ 3:18         0.540         {"position_acc": 0.8209}  3.0331
16        train   2100/2737    9:26/ 2:51         0.539         {"position_acc": 0.8213}  7.2896
16        train   2200/2737    9:52/ 2:24         0.542         {"position_acc": 0.8205}  4.8589
16        train   2300/2737   10:18/ 1:57         0.542         {"position_acc": 0.8206}  14.1382
16        train   2400/2737   10:44/ 1:30         0.541         {"position_acc": 0.8206}  9.4546
16        train   2500/2737   11:10/ 1:03         0.542         {"position_acc": 0.8202}  5.7498
16        train   2600/2737   11:37/ 0:36         0.540         {"position_acc": 0.8207}  8.4057
16        train   2700/2737   12:03/ 0:09         0.540         {"position_acc": 0.8207}  5.4506
False False
==================== begin saving model and validation ====================
16        train   2737/2737   12:14/ 0:00         0.540         {"position_acc": 0.8208}  None
10/04/2022 11:22:39 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/SQuAD-Lora/16.pkl
16        valid   1/330        0:13/72:54         0.000           None
16        valid   101/330      0:24/ 0:56         0.000           None
16        valid   201/330      0:36/ 0:23         0.000           None
16        valid   301/330      0:48/ 0:04         0.000           None
16        valid   330/330      0:53/ 0:00         0.000         {"EM": 0.8468, "F1": 0.9143}  None
16        test    1/127        0:12/26:56         0.000           None
16        test    101/127      0:24/ 0:06         0.000           None
16        test    127/127      0:28/ 0:00         0.000         {"EM": 0.4894, "F1": 0.6249}  None
17        train   100/2737     0:41/18:12         0.557         {"position_acc": 0.8256}  5.0512
17        train   200/2737     1:07/14:15         0.532         {"position_acc": 0.8231}  5.915
17        train   300/2737     1:33/12:40         0.541         {"position_acc": 0.819}  7.696
17        train   400/2737     1:59/11:38         0.539         {"position_acc": 0.8195}  6.9957
17        train   500/2737     2:25/10:53         0.533         {"position_acc": 0.8231}  7.8482
17        train   600/2737     2:52/10:13         0.527         {"position_acc": 0.8244}  10.0108
17        train   700/2737     3:18/ 9:37         0.528         {"position_acc": 0.8238}  3.4458
17        train   800/2737     3:44/ 9:04         0.527         {"position_acc": 0.8238}  11.1932
17        train   900/2737     4:11/ 8:32         0.532         {"position_acc": 0.8212}  4.7474
17        train   1000/2737    4:37/ 8:01         0.526         {"position_acc": 0.8222}  7.8545
17        train   1100/2737    5:03/ 7:32         0.528         {"position_acc": 0.8208}  8.2161
17        train   1200/2737    5:30/ 7:02         0.532         {"position_acc": 0.8201}  12.1267
17        train   1300/2737    5:56/ 6:34         0.531         {"position_acc": 0.8196}  8.106
17        train   1400/2737    6:22/ 6:05         0.533         {"position_acc": 0.819}  10.1458
17        train   1500/2737    6:49/ 5:37         0.535         {"position_acc": 0.8192}  7.2645
