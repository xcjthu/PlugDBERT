/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter64.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter64.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter64.config
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter64.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter64.config
None
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter64.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter64.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter64.config
None
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/05/2022 04:48:31 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/05/2022 04:48:31 - INFO - __main__ -   CUDA available: True
10/05/2022 04:48:31 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
10/05/2022 04:48:31 - INFO - __main__ -   CUDA available: True
10/05/2022 04:48:31 - INFO - __main__ -   CUDA available: True
10/05/2022 04:48:31 - INFO - __main__ -   CUDA available: True
10/05/2022 04:48:31 - INFO - __main__ -   CUDA available: True
10/05/2022 04:48:31 - INFO - __main__ -   CUDA available: True
10/05/2022 04:48:31 - INFO - __main__ -   CUDA available: True
10/05/2022 04:48:31 - INFO - __main__ -   CUDA available: True
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
10/05/2022 04:48:41 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[64, 768] bias:[64]
│               │                   └── up_proj (Linear) weight:[768, 64] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[64, 768] bias:[64]
│                               └── up_proj (Linear) weight:[768, 64] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-05 04:48:45,196 >> Trainable Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:677]2022-10-05 04:48:45,196 >> Delta Parameter Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:679]2022-10-05 04:48:45,196 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[64, 768] bias:[64]
│               │                   └── up_proj (Linear) weight:[768, 64] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[64, 768] bias:[64]
│                               └── up_proj (Linear) weight:[768, 64] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-05 04:48:45,210 >> Trainable Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:677]2022-10-05 04:48:45,210 >> Delta Parameter Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:679]2022-10-05 04:48:45,210 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[64, 768] bias:[64]
│               │                   └── up_proj (Linear) weight:[768, 64] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[64, 768] bias:[64]
│                               └── up_proj (Linear) weight:[768, 64] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-05 04:48:45,475 >> Trainable Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:677]2022-10-05 04:48:45,475 >> Delta Parameter Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:679]2022-10-05 04:48:45,475 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[64, 768] bias:[64]
│               │                   └── up_proj (Linear) weight:[768, 64] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[64, 768] bias:[64]
│                               └── up_proj (Linear) weight:[768, 64] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-05 04:48:45,512 >> Trainable Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:677]2022-10-05 04:48:45,512 >> Delta Parameter Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:679]2022-10-05 04:48:45,512 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[64, 768] bias:[64]
│               │                   └── up_proj (Linear) weight:[768, 64] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[64, 768] bias:[64]
│                               └── up_proj (Linear) weight:[768, 64] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-05 04:48:45,770 >> Trainable Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:677]2022-10-05 04:48:45,770 >> Delta Parameter Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:679]2022-10-05 04:48:45,770 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[64, 768] bias:[64]
│               │                   └── up_proj (Linear) weight:[768, 64] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[64, 768] bias:[64]
│                               └── up_proj (Linear) weight:[768, 64] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-05 04:48:45,971 >> Trainable Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:677]2022-10-05 04:48:45,971 >> Delta Parameter Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:679]2022-10-05 04:48:45,971 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[64, 768] bias:[64]
│               │                   └── up_proj (Linear) weight:[768, 64] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[64, 768] bias:[64]
│                               └── up_proj (Linear) weight:[768, 64] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-05 04:48:46,597 >> Trainable Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:677]2022-10-05 04:48:46,597 >> Delta Parameter Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:679]2022-10-05 04:48:46,597 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[64, 768] bias:[64]
│               │                   └── up_proj (Linear) weight:[768, 64] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[64, 768] bias:[64]
│                               └── up_proj (Linear) weight:[768, 64] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-05 04:48:46,738 >> Trainable Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:677]2022-10-05 04:48:46,738 >> Delta Parameter Ratio: 1.872306%
[INFO|(OpenDelta)basemodel:679]2022-10-05 04:48:46,738 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
odict_keys(['roberta.encoder.layer.0.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.0.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.0.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.0.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.0.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.1.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.1.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.1.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.1.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.2.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.2.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.2.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.2.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.3.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.3.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.3.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.3.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.4.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.4.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.4.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.4.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.5.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.5.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.5.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.5.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.6.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.6.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.6.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.6.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.7.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.7.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.7.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.7.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.8.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.8.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.8.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.8.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.9.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.9.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.9.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.9.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.10.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.10.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.10.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.10.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.11.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.11.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.11.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.11.output.LayerNorm.adapter.modulelist.up_proj.bias'])
10/05/2022 04:48:52 - INFO - tools.init_tool -   Begin to load checkpoint... from None
10/05/2022 04:48:52 - WARNING - tools.init_tool -   Cannot load checkpoint file with error 'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.
10/05/2022 04:48:52 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 64
grad_accumulate: 2
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data3/private/lirun/biomed/kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data3/private/lirun/biomed/kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
domain_plugin_path: None
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter-64
output_function: avgloss
load_from_path: False
========
valid_mode step no_valid False
step_epoch 5000
10/05/2022 04:48:52 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
10/05/2022 04:49:04 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/05/2022 04:49:04 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/05/2022 04:49:04 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/05/2022 04:49:04 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/05/2022 04:49:04 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/05/2022 04:49:04 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/05/2022 04:49:04 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/05/2022 04:49:04 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   100/102467   0:42/724:23        1.464         {"mlm": 1.463788549900055, "mse": 0.0}  0.3425
0         train   200/102467   1:14/631:45        1.468         {"mlm": 1.4681073069572448, "mse": 0.0}  0.3023
0         train   300/102467   1:46/602:33        1.476         {"mlm": 1.4759619613488515, "mse": 0.0}  0.3132
0         train   400/102467   2:18/589:28        1.482         {"mlm": 1.4821119168400765, "mse": 0.0}  0.3281
0         train   500/102467   2:51/581:55        1.478         {"mlm": 1.4784230406284333, "mse": 0.0}  0.3101
0         train   600/102467   3:23/576:35        1.475         {"mlm": 1.4747236597537994, "mse": 0.0}  0.3055
0         train   700/102467   3:56/572:35        1.471         {"mlm": 1.4705877026489802, "mse": 0.0}  0.3118
0         train   800/102467   4:28/569:27        1.473         {"mlm": 1.4732600831985474, "mse": 0.0}  0.2909
0         train   900/102467   5:01/566:55        1.470         {"mlm": 1.469724460442861, "mse": 0.0}  0.2938
0         train   1000/102467  5:34/564:56        1.470         {"mlm": 1.469755978345871, "mse": 0.0}  0.3329
0         train   1100/102467  6:06/563:12        1.467         {"mlm": 1.4671647179126739, "mse": 0.0}  0.3187
0         train   1200/102467  6:39/561:32        1.464         {"mlm": 1.4640931884447734, "mse": 0.0}  0.5771
0         train   1300/102467  7:11/560:05        1.465         {"mlm": 1.4652420474932744, "mse": 0.0}  0.2857
0         train   1400/102467  7:44/558:45        1.463         {"mlm": 1.4629982744795935, "mse": 0.0}  0.305
0         train   1500/102467  8:17/557:33        1.462         {"mlm": 1.461981867869695, "mse": 0.0}  0.4925
0         train   1600/102467  8:49/556:23        1.461         {"mlm": 1.4605005753040314, "mse": 0.0}  0.3175
0         train   1700/102467  9:22/555:16        1.460         {"mlm": 1.4598922744919272, "mse": 0.0}  0.2961
0         train   1800/102467  9:54/554:13        1.458         {"mlm": 1.4584095406532287, "mse": 0.0}  0.3148
0         train   1900/102467 10:27/553:17        1.458         {"mlm": 1.4578346589364504, "mse": 0.0}  0.3011
0         train   2000/102467 10:59/552:20        1.457         {"mlm": 1.4572336535453796, "mse": 0.0}  0.3205
0         train   2100/102467 11:32/551:24        1.457         {"mlm": 1.453933252228631, "mse": 0.0}  0.3175
0         train   2200/102467 12:04/550:32        1.456         {"mlm": 1.448074171291524, "mse": 0.0}  0.3439
0         train   2300/102467 12:37/549:39        1.456         {"mlm": 1.445323984758511, "mse": 0.0}  0.3306
0         train   2400/102467 13:09/548:52        1.455         {"mlm": 1.4449006193562557, "mse": 0.0}  0.3126
0         train   2500/102467 13:42/548:04        1.454         {"mlm": 1.4433036373707957, "mse": 0.0}  0.3203
0         train   2600/102467 14:14/547:20        1.453         {"mlm": 1.439475811383561, "mse": 0.0}  0.2864
0         train   2700/102467 14:47/546:36        1.451         {"mlm": 1.4341922558428393, "mse": 0.0}  0.3009
0         train   2800/102467 15:20/545:53        1.450         {"mlm": 1.4304229085227576, "mse": 0.0}  0.2881
0         train   2900/102467 15:52/545:07        1.448         {"mlm": 1.4274950047356136, "mse": 0.0}  0.3071
0         train   3000/102467 16:25/544:24        1.447         {"mlm": 1.4280204170339696, "mse": 0.0}  0.2987
0         train   3100/102467 16:57/543:42        1.447         {"mlm": 1.4281567071545005, "mse": 0.0}  0.3084
0         train   3200/102467 17:30/542:59        1.446         {"mlm": 1.4277110509419064, "mse": 0.0}  0.2849
0         train   3300/102467 18:02/542:11        1.445         {"mlm": 1.4272424818645357, "mse": 0.0}  0.3171
0         train   3400/102467 18:34/541:27        1.444         {"mlm": 1.4249765573014865, "mse": 0.0}  0.3471
0         train   3500/102467 19:07/540:45        1.443         {"mlm": 1.423330808576542, "mse": 0.0}  0.3434
0         train   3600/102467 19:39/540:05        1.442         {"mlm": 1.4231162713124201, "mse": 0.0}  0.2975
0         train   3700/102467 20:12/539:26        1.441         {"mlm": 1.4224931026571566, "mse": 0.0}  0.2956
0         train   3800/102467 20:44/538:46        1.441         {"mlm": 1.4222221711465157, "mse": 0.0}  0.3122
0         train   3900/102467 21:17/538:06        1.440         {"mlm": 1.4224414229393005, "mse": 0.0}  0.3426
0         train   4000/102467 21:50/537:28        1.439         {"mlm": 1.4215460959346726, "mse": 0.0}  0.3001
0         train   4100/102467 22:22/536:49        1.439         {"mlm": 1.423141258103507, "mse": 0.0}  0.3144
0         train   4200/102467 22:54/536:09        1.438         {"mlm": 1.4098303275878983, "mse": 0.0}  0.5609
0         train   4300/102467 23:27/535:31        1.437         {"mlm": 1.4090979227283658, "mse": 0.0}  0.3073
0         train   4400/102467 23:59/534:53        1.436         {"mlm": 1.4048731120387514, "mse": 0.0}  0.3455
0         train   4500/102467 24:32/534:15        1.436         {"mlm": 1.4077087310423333, "mse": 0.0}  0.2943
0         train   4600/102467 25:04/533:37        1.435         {"mlm": 1.4082039492983482, "mse": 0.0}  0.3266
0         train   4700/102467 25:37/532:59        1.434         {"mlm": 1.4050760498873485, "mse": 0.0}  0.2893
0         train   4800/102467 26:09/532:24        1.434         {"mlm": 1.4060968439232437, "mse": 0.0}  0.3066
0         train   4900/102467 26:42/531:48        1.433         {"mlm": 1.4064282220694961, "mse": 0.0}  0.3195
0         train   5000/102467 27:15/531:13        1.433         {"mlm": 1.4053147846090053, "mse": 0.0}  0.2997
0         train   5100/102467 27:47/530:39        1.432         {"mlm": 1.4071410947592966, "mse": 0.0}  0.3124
0         train   5200/102467 28:20/530:04        1.432         {"mlm": 1.4070755230863026, "mse": 0.0}  0.3264
0         train   5300/102467 28:52/529:30        1.432         {"mlm": 1.408357572803512, "mse": 0.0}  0.3162
0         train   5400/102467 29:25/528:56        1.431         {"mlm": 1.4087392572681279, "mse": 0.0}  0.3282
0         train   5500/102467 29:58/528:21        1.431         {"mlm": 1.4079185484725738, "mse": 0.0}  0.3272
0         train   5600/102467 30:30/527:47        1.431         {"mlm": 1.408289060537448, "mse": 0.0}  0.3062
0         train   5700/102467 31:03/527:12        1.431         {"mlm": 1.409653938911829, "mse": 0.0}  1.3824
0         train   5800/102467 31:35/526:35        1.430         {"mlm": 1.410635171596943, "mse": 0.0}  0.3307
0         train   5900/102467 32:08/526:00        1.430         {"mlm": 1.4104886375312684, "mse": 0.0}  0.3064
0         train   6000/102467 32:40/525:24        1.430         {"mlm": 1.4101730199368507, "mse": 0.0}  0.3672
0         train   6100/102467 33:13/524:49        1.429         {"mlm": 1.36862428348089, "mse": 0.0}  0.3064
0         train   6200/102467 33:45/524:13        1.428         {"mlm": 1.3882504940638083, "mse": 0.0}  0.3518
0         train   6300/102467 34:18/523:38        1.428         {"mlm": 1.3909827288152392, "mse": 0.0}  0.3239
0         train   6400/102467 34:50/523:03        1.427         {"mlm": 1.3869537368829665, "mse": 0.0}  0.3145
0         train   6500/102467 35:23/522:28        1.427         {"mlm": 1.39297075086916, "mse": 0.0}  0.3933
0         train   6600/102467 35:55/521:52        1.427         {"mlm": 1.3965684042703765, "mse": 0.0}  0.3538
0         train   6700/102467 36:28/521:16        1.426         {"mlm": 1.397313439606594, "mse": 0.0}  0.3205
0         train   6800/102467 37:00/520:41        1.426         {"mlm": 1.396916491772928, "mse": 0.0}  0.3847
0         train   6900/102467 37:33/520:06        1.425         {"mlm": 1.3959894189600695, "mse": 0.0}  0.3125
0         train   7000/102467 38:05/519:31        1.425         {"mlm": 1.3968614040908507, "mse": 0.0}  0.3288
0         train   7100/102467 38:38/518:57        1.425         {"mlm": 1.3971696854072368, "mse": 0.0}  0.3436
0         train   7200/102467 39:10/518:23        1.424         {"mlm": 1.3982111605984426, "mse": 0.0}  0.3602
0         train   7300/102467 39:43/517:48        1.424         {"mlm": 1.3967803752505052, "mse": 0.0}  0.3344
0         train   7400/102467 40:15/517:12        1.423         {"mlm": 1.3970232225778534, "mse": 0.0}  0.316
0         train   7500/102467 40:48/516:38        1.423         {"mlm": 1.3952934572357454, "mse": 0.0}  0.3188
0         train   7600/102467 41:20/516:04        1.422         {"mlm": 1.3940612779383221, "mse": 0.0}  0.3379
0         train   7700/102467 41:53/515:31        1.422         {"mlm": 1.3930803368424554, "mse": 0.0}  0.3452
0         train   7800/102467 42:25/514:57        1.421         {"mlm": 1.3918388455061363, "mse": 0.0}  0.3531
0         train   7900/102467 42:58/514:23        1.420         {"mlm": 1.3907356896272256, "mse": 0.0}  0.3265
0         train   8000/102467 43:30/513:49        1.419         {"mlm": 1.3890854597509534, "mse": 0.0}  0.3375
0         train   8100/102467 44:03/513:15        1.419         {"mlm": 1.3795377301673095, "mse": 0.0}  0.3392
0         train   8200/102467 44:35/512:41        1.419         {"mlm": 1.3877393731049128, "mse": 0.0}  0.332
0         train   8300/102467 45:08/512:07        1.418         {"mlm": 1.3879557119027988, "mse": 0.0}  0.3375
0         train   8400/102467 45:40/511:33        1.418         {"mlm": 1.393174045615726, "mse": 0.0}  0.3557
0         train   8500/102467 46:13/511:00        1.417         {"mlm": 1.3863701080122302, "mse": 0.0}  0.3494
0         train   8600/102467 46:46/510:27        1.417         {"mlm": 1.3887550699830855, "mse": 0.0}  0.379
0         train   8700/102467 47:18/509:54        1.417         {"mlm": 1.389365218065936, "mse": 0.0}  0.3552
0         train   8800/102467 47:51/509:21        1.417         {"mlm": 1.388568186804877, "mse": 0.0}  0.3256
0         train   8900/102467 48:23/508:48        1.416         {"mlm": 1.3896715942371105, "mse": 0.0}  0.3369
0         train   9000/102467 48:56/508:14        1.416         {"mlm": 1.388872498429444, "mse": 0.0}  0.3664
0         train   9100/102467 49:28/507:41        1.416         {"mlm": 1.3892880186003491, "mse": 0.0}  0.3688
0         train   9200/102467 50:01/507:07        1.416         {"mlm": 1.3892473121450897, "mse": 0.0}  0.3489
0         train   9300/102467 50:33/506:33        1.415         {"mlm": 1.3897015318090533, "mse": 0.0}  0.3389
0         train   9400/102467 51:06/505:59        1.415         {"mlm": 1.3901904367102593, "mse": 0.0}  0.366
0         train   9500/102467 51:38/505:25        1.415         {"mlm": 1.3895772956909342, "mse": 0.0}  0.361
0         train   9600/102467 52:11/504:51        1.414         {"mlm": 1.3880063236357276, "mse": 0.0}  0.3429
0         train   9700/102467 52:43/504:18        1.414         {"mlm": 1.3858610156853244, "mse": 0.0}  0.3741
0         train   9800/102467 53:16/503:44        1.413         {"mlm": 1.3866241599841744, "mse": 0.0}  0.3977
0         train   9900/102467 53:48/503:11        1.413         {"mlm": 1.3867827221311095, "mse": 0.0}  0.3584
0         train   10000/102467 54:21/502:37       1.413         {"mlm": 1.3874766349434136, "mse": 0.0}  0.3983

10/05/2022 05:43:13 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter-64/0-step10000.pkl
0         valid   1/781        0:10/141:25        1.292           None
0         valid   101/781      0:25/ 2:53         1.276           None
0         valid   201/781      0:40/ 1:57         1.289           None
0         valid   301/781      0:55/ 1:28         1.281           None
0         valid   401/781      1:10/ 1:06         1.283           None
0         valid   501/781      1:25/ 0:47         1.286           None
0         valid   601/781      1:39/ 0:29         1.283           None
0         valid   701/781      1:54/ 0:13         1.287           None
0         valid   781/781      2:10/ 0:00         1.283         {"mlm": 1.2826504481434058, "mse": 0.0, "train": 0.0}  None
0         train   10100/102467 57:04/521:58       1.412         {"mlm": 1.3571729493141174, "mse": 0.0}  0.3689
0         train   10200/102467 57:37/521:12       1.412         {"mlm": 1.3625383225083352, "mse": 0.0}  0.3909
0         train   10300/102467 58:09/520:26       1.412         {"mlm": 1.3741350998481114, "mse": 0.0}  0.3832
0         train   10400/102467 58:42/519:40       1.412         {"mlm": 1.3741985496878624, "mse": 0.0}  0.3627
0         train   10500/102467 59:14/518:55       1.411         {"mlm": 1.3750615537166595, "mse": 0.0}  0.3794
0         train   10600/102467 59:47/518:09       1.411         {"mlm": 1.3730924077828726, "mse": 0.0}  0.3765
0         train   10700/102467 60:19/517:24       1.410         {"mlm": 1.37206221308027, "mse": 0.0}  0.3577
0         train   10800/102467 60:52/516:38       1.410         {"mlm": 1.372910972982645, "mse": 0.0}  0.3633
0         train   10900/102467 61:24/515:53       1.410         {"mlm": 1.374780662589603, "mse": 0.0}  0.3833
0         train   11000/102467 61:57/515:08       1.410         {"mlm": 1.3745766003727913, "mse": 0.0}  0.3631
0         train   11100/102467 62:29/514:24       1.409         {"mlm": 1.3713299887288701, "mse": 0.0}  0.4015
0         train   11200/102467 63:02/513:40       1.409         {"mlm": 1.3715307085216046, "mse": 0.0}  0.3854
0         train   11300/102467 63:34/512:56       1.408         {"mlm": 1.3723048974000491, "mse": 0.0}  0.3777
0         train   11400/102467 64:07/512:11       1.408         {"mlm": 1.3725619032553265, "mse": 0.0}  0.3683
0         train   11500/102467 64:39/511:28       1.408         {"mlm": 1.372125926176707, "mse": 0.0}  0.3556
0         train   11600/102467 65:12/510:44       1.408         {"mlm": 1.3733401183784009, "mse": 0.0}  0.4112
0         train   11700/102467 65:44/510:01       1.407         {"mlm": 1.372307296921225, "mse": 0.0}  0.3829
0         train   11800/102467 66:17/509:18       1.407         {"mlm": 1.3720433237155278, "mse": 0.0}  0.3865
0         train   11900/102467 66:49/508:35       1.406         {"mlm": 1.371380519239526, "mse": 0.0}  0.3749
0         train   12000/102467 67:22/507:52       1.406         {"mlm": 1.371439799785614, "mse": 0.0}  0.376
0         train   12100/102467 67:54/507:10       1.406         {"mlm": 1.375545124814968, "mse": 0.0}  0.3862
0         train   12200/102467 68:27/506:27       1.406         {"mlm": 1.3730224266124131, "mse": 0.0}  0.4142
0         train   12300/102467 68:59/505:45       1.405         {"mlm": 1.3740771963046148, "mse": 0.0}  0.3835
0         train   12400/102467 69:32/505:03       1.405         {"mlm": 1.3729617894442756, "mse": 0.0}  0.4506
0         train   12500/102467 70:04/504:21       1.405         {"mlm": 1.3754916886289517, "mse": 0.0}  0.3956
0         train   12600/102467 70:37/503:40       1.404         {"mlm": 1.370156410142456, "mse": 0.0}  0.3923
0         train   12700/102467 71:09/502:59       1.404         {"mlm": 1.3669509101493846, "mse": 0.0}  0.3957
0         train   12800/102467 71:42/502:18       1.404         {"mlm": 1.3660497860108807, "mse": 0.0}  0.3798
0         train   12900/102467 72:14/501:37       1.403         {"mlm": 1.364699810312375, "mse": 0.0}  0.421
0         train   13000/102467 72:47/500:56       1.403         {"mlm": 1.3652838311634503, "mse": 0.0}  0.4649
0         train   13100/102467 73:19/500:16       1.403         {"mlm": 1.366996349260089, "mse": 0.0}  0.3635
0         train   13200/102467 73:52/499:35       1.403         {"mlm": 1.366544373339668, "mse": 0.0}  0.3712
0         train   13300/102467 74:24/498:54       1.402         {"mlm": 1.3672182810958116, "mse": 0.0}  0.3898
0         train   13400/102467 74:57/498:13       1.402         {"mlm": 1.367641471299723, "mse": 0.0}  0.375
0         train   13500/102467 75:30/497:34       1.402         {"mlm": 1.3692582956626465, "mse": 0.0}  0.4035
0         train   13600/102467 76:02/496:53       1.402         {"mlm": 1.3703158707302612, "mse": 0.0}  0.3771
0         train   13700/102467 76:35/496:12       1.402         {"mlm": 1.371053921502783, "mse": 0.0}  0.3875
0         train   13800/102467 77:07/495:32       1.402         {"mlm": 1.3725115669774242, "mse": 0.0}  0.5112
0         train   13900/102467 77:40/494:52       1.402         {"mlm": 1.3732122741541528, "mse": 0.0}  0.4123
0         train   14000/102467 78:12/494:12       1.401         {"mlm": 1.3734997191865663, "mse": 0.0}  0.3838
0         train   14100/102467 78:44/493:31       1.401         {"mlm": 1.3745966681412287, "mse": 0.0}  0.4011
0         train   14200/102467 79:17/492:51       1.401         {"mlm": 1.3683623776893423, "mse": 0.0}  0.3934
0         train   14300/102467 79:49/492:11       1.401         {"mlm": 1.3809463761796887, "mse": 0.0}  0.4126
0         train   14400/102467 80:22/491:32       1.401         {"mlm": 1.3767259049056164, "mse": 0.0}  0.4094
0         train   14500/102467 80:54/490:52       1.401         {"mlm": 1.3791626538617543, "mse": 0.0}  0.4041
0         train   14600/102467 81:27/490:13       1.401         {"mlm": 1.3784365915135794, "mse": 0.0}  0.4475
0         train   14700/102467 81:59/489:33       1.400         {"mlm": 1.3774826372079658, "mse": 0.0}  4.4573
0         train   14800/102467 82:32/488:54       1.400         {"mlm": 1.3760956766312582, "mse": 0.0}  0.4323
0         train   14900/102467 83:04/488:15       1.400         {"mlm": 1.370255259402876, "mse": 0.0}  0.3852
0         train   15000/102467 83:37/487:36       1.399         {"mlm": 1.367799654334246, "mse": 0.0}  0.3854
0         train   15100/102467 84:09/486:57       1.399         {"mlm": 1.3676298821450148, "mse": 0.0}  0.4087
0         train   15200/102467 84:42/486:19       1.399         {"mlm": 1.3679376583366043, "mse": 0.0}  0.4004
0         train   15300/102467 85:14/485:40       1.399         {"mlm": 1.3697986480175806, "mse": 0.0}  0.4475
0         train   15400/102467 85:47/485:02       1.398         {"mlm": 1.367577101966001, "mse": 0.0}  0.4203
0         train   15500/102467 86:20/484:24       1.398         {"mlm": 1.3671328890148564, "mse": 0.0}  0.4113
0         train   15600/102467 86:52/483:46       1.398         {"mlm": 1.367448261182508, "mse": 0.0}  0.4047
0         train   15700/102467 87:25/483:07       1.398         {"mlm": 1.3662323003593126, "mse": 0.0}  0.4046
0         train   15800/102467 87:57/482:29       1.397         {"mlm": 1.3662447832914295, "mse": 0.0}  0.4049
0         train   15900/102467 88:30/481:51       1.397         {"mlm": 1.3667511546197255, "mse": 0.0}  0.3843
0         train   16000/102467 89:02/481:13       1.397         {"mlm": 1.3645091509377514, "mse": 0.0}  0.4085
0         train   16100/102467 89:35/480:35       1.397         {"mlm": 1.3563423703626258, "mse": 0.0}  0.3999
0         train   16200/102467 90:07/479:56       1.396         {"mlm": 1.3437594290312171, "mse": 0.0}  0.4138
0         train   16300/102467 90:40/479:18       1.396         {"mlm": 1.354760559721025, "mse": 0.0}  0.408
0         train   16400/102467 91:12/478:40       1.396         {"mlm": 1.349305842325129, "mse": 0.0}  0.4646
0         train   16500/102467 91:45/478:02       1.395         {"mlm": 1.348046834200201, "mse": 0.0}  0.3987
0         train   16600/102467 92:17/477:24       1.395         {"mlm": 1.3508721908532595, "mse": 0.0}  0.4093
0         train   16700/102467 92:50/476:46       1.395         {"mlm": 1.3497573735040094, "mse": 0.0}  0.4175
0         train   16800/102467 93:22/476:09       1.395         {"mlm": 1.3515856715487118, "mse": 0.0}  0.4362
0         train   16900/102467 93:55/475:31       1.394         {"mlm": 1.3511557739846816, "mse": 0.0}  0.6299
0         train   17000/102467 94:27/474:53       1.394         {"mlm": 1.3522271555189862, "mse": 0.0}  0.4408
0         train   17100/102467 95:00/474:16       1.394         {"mlm": 1.3516191252818843, "mse": 0.0}  0.4245
0         train   17200/102467 95:32/473:38       1.394         {"mlm": 1.3508515788698157, "mse": 0.0}  0.4539
0         train   17300/102467 96:04/473:00       1.393         {"mlm": 1.3501931272659655, "mse": 0.0}  0.4108
0         train   17400/102467 96:37/472:22       1.393         {"mlm": 1.3486649791257417, "mse": 0.0}  0.3889
0         train   17500/102467 97:09/471:45       1.393         {"mlm": 1.3502625471445109, "mse": 0.0}  0.4648
0         train   17600/102467 97:42/471:08       1.393         {"mlm": 1.3492313826091602, "mse": 0.0}  0.4087
0         train   17700/102467 98:14/470:31       1.392         {"mlm": 1.3507477929118668, "mse": 0.0}  0.4104
0         train   17800/102467 98:47/469:53       1.392         {"mlm": 1.3525755826671984, "mse": 0.0}  0.5793
0         train   17900/102467 99:19/469:16       1.392         {"mlm": 1.3521757541233197, "mse": 0.0}  1.4424
0         train   18000/102467 99:52/468:39       1.392         {"mlm": 1.3522834284161112, "mse": 0.0}  0.3974
0         train   18100/102467 100:24/468:03      1.392         {"mlm": 1.3516697579373915, "mse": 0.0}  0.4275
0         train   18200/102467 100:57/467:26      1.392         {"mlm": 1.3574007849912255, "mse": 0.0}  0.4822
0         train   18300/102467 101:29/466:49      1.391         {"mlm": 1.351907893008477, "mse": 0.0}  0.4389
0         train   18400/102467 102:02/466:12      1.391         {"mlm": 1.3506659761522755, "mse": 0.0}  0.4222
0         train   18500/102467 102:35/465:36      1.391         {"mlm": 1.3558515414835945, "mse": 0.0}  0.4481
0         train   18600/102467 103:07/464:59      1.391         {"mlm": 1.3592644402844793, "mse": 0.0}  0.4372
0         train   18700/102467 103:40/464:22      1.391         {"mlm": 1.3575024206576676, "mse": 0.0}  0.4362
0         train   18800/102467 104:12/463:46      1.391         {"mlm": 1.358691815006074, "mse": 0.0}  0.42
0         train   18900/102467 104:44/463:09      1.391         {"mlm": 1.3618298080483717, "mse": 0.0}  0.452
0         train   19000/102467 105:17/462:32      1.390         {"mlm": 1.3597951557143146, "mse": 0.0}  0.6894
0         train   19100/102467 105:49/461:56      1.390         {"mlm": 1.3603783097036564, "mse": 0.0}  0.4611
0         train   19200/102467 106:22/461:19      1.390         {"mlm": 1.3582980031253502, "mse": 0.0}  0.443
0         train   19300/102467 106:54/460:43      1.390         {"mlm": 1.357781863644903, "mse": 0.0}  0.4733
0         train   19400/102467 107:27/460:06      1.389         {"mlm": 1.357058540370881, "mse": 0.0}  0.8958
0         train   19500/102467 107:59/459:30      1.389         {"mlm": 1.3582342553903712, "mse": 0.0}  0.4392
0         train   19600/102467 108:32/458:53      1.389         {"mlm": 1.3602511943656401, "mse": 0.0}  0.4661
0         train   19700/102467 109:04/458:17      1.389         {"mlm": 1.3594887011666905, "mse": 0.0}  0.3922
0         train   19800/102467 109:37/457:41      1.389         {"mlm": 1.360288464644704, "mse": 0.0}  0.4768
0         train   19900/102467 110:09/457:04      1.389         {"mlm": 1.3600564191298645, "mse": 0.0}  0.6131
0         train   20000/102467 110:42/456:28      1.389         {"mlm": 1.3598750741304997, "mse": 0.0}  0.3912

10/05/2022 06:39:34 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter-64/0-step20000.pkl
0         valid   1/781        0:10/140:34        1.344           None
0         valid   101/781      0:25/ 2:52         1.263           None
0         valid   201/781      0:40/ 1:56         1.275           None
0         valid   301/781      0:55/ 1:28         1.270           None
0         valid   401/781      1:09/ 1:06         1.268           None
0         valid   501/781      1:24/ 0:47         1.267           None
0         valid   601/781      1:39/ 0:29         1.265           None
0         valid   701/781      1:54/ 0:13         1.265           None
0         valid   781/781      2:10/ 0:00         1.260         {"mlm": 1.259923175416133, "mse": 0.0, "train": 0.0}  None
0         train   20100/102467 113:25/464:46      1.389         {"mlm": 1.3649898231029511, "mse": 0.0}  0.4254
0         train   20200/102467 113:57/464:07      1.388         {"mlm": 1.349835624396801, "mse": 0.0}  0.4919
0         train   20300/102467 114:30/463:29      1.388         {"mlm": 1.3504796040058136, "mse": 0.0}  0.4569
0         train   20400/102467 115:03/462:50      1.388         {"mlm": 1.354314959049225, "mse": 0.0}  0.4229
0         train   20500/102467 115:35/462:11      1.388         {"mlm": 1.3531153433322907, "mse": 0.0}  0.4337
0         train   20600/102467 116:08/461:32      1.388         {"mlm": 1.35492365727822, "mse": 0.0}  0.4539
0         train   20700/102467 116:40/460:53      1.388         {"mlm": 1.3540028589963913, "mse": 0.0}  0.4269
0         train   20800/102467 117:13/460:14      1.387         {"mlm": 1.3503823801875114, "mse": 0.0}  0.4261
0         train   20900/102467 117:45/459:35      1.387         {"mlm": 1.3496649698416392, "mse": 0.0}  0.4252
0         train   21000/102467 118:18/458:56      1.387         {"mlm": 1.3487941964864731, "mse": 0.0}  0.4581
0         train   21100/102467 118:50/458:17      1.387         {"mlm": 1.347774668715217, "mse": 0.0}  0.4322
0         train   21200/102467 119:23/457:38      1.386         {"mlm": 1.3464367983241876, "mse": 0.0}  0.477
0         train   21300/102467 119:55/457:00      1.386         {"mlm": 1.3481676547802413, "mse": 0.0}  0.4567
0         train   21400/102467 120:28/456:21      1.386         {"mlm": 1.3473849833011626, "mse": 0.0}  0.4214
0         train   21500/102467 121:00/455:42      1.386         {"mlm": 1.3470190896987915, "mse": 0.0}  0.4506
0         train   21600/102467 121:33/455:04      1.386         {"mlm": 1.3483074463158846, "mse": 0.0}  0.4361
0         train   21700/102467 122:05/454:25      1.385         {"mlm": 1.3465869240550434, "mse": 0.0}  0.4677
0         train   21800/102467 122:38/453:47      1.385         {"mlm": 1.3471168014738295, "mse": 0.0}  0.4645
0         train   21900/102467 123:10/453:08      1.385         {"mlm": 1.34729057177117, "mse": 0.0}  0.4433
0         train   22000/102467 123:43/452:30      1.385         {"mlm": 1.3488032089471818, "mse": 0.0}  0.4114
0         train   22100/102467 124:15/451:52      1.385         {"mlm": 1.3650447973097213, "mse": 0.0}  0.4366
0         train   22200/102467 124:47/451:13      1.385         {"mlm": 1.3522326404125846, "mse": 0.0}  0.4242
0         train   22300/102467 125:20/450:35      1.385         {"mlm": 1.3492097948307178, "mse": 0.0}  0.4476
0         train   22400/102467 125:52/449:57      1.384         {"mlm": 1.3479203977680445, "mse": 0.0}  0.4238
0         train   22500/102467 126:25/449:19      1.384         {"mlm": 1.3480721660749706, "mse": 0.0}  0.4544
0         train   22600/102467 126:57/448:41      1.384         {"mlm": 1.3516017145823955, "mse": 0.0}  0.4246
0         train   22700/102467 127:30/448:02      1.384         {"mlm": 1.3487191583124523, "mse": 0.0}  0.4154
0         train   22800/102467 128:02/447:24      1.384         {"mlm": 1.3470782010218079, "mse": 0.0}  0.499
0         train   22900/102467 128:35/446:46      1.384         {"mlm": 1.3467038300994771, "mse": 0.0}  0.4003
0         train   23000/102467 129:07/446:09      1.383         {"mlm": 1.344453633726538, "mse": 0.0}  0.4261
0         train   23100/102467 129:40/445:31      1.383         {"mlm": 1.345032415355304, "mse": 0.0}  0.4717
0         train   23200/102467 130:12/444:53      1.383         {"mlm": 1.3448975541076629, "mse": 0.0}  0.4534
0         train   23300/102467 130:45/444:16      1.383         {"mlm": 1.347124111156816, "mse": 0.0}  0.4771
0         train   23400/102467 131:17/443:38      1.383         {"mlm": 1.347021696523226, "mse": 0.0}  0.4599
0         train   23500/102467 131:50/443:00      1.383         {"mlm": 1.3474765615991309, "mse": 0.0}  0.4535
0         train   23600/102467 132:22/442:23      1.382         {"mlm": 1.3466005411425406, "mse": 0.0}  0.4126
0         train   23700/102467 132:55/441:45      1.382         {"mlm": 1.3476111471758232, "mse": 0.0}  0.4361
0         train   23800/102467 133:27/441:08      1.382         {"mlm": 1.3473170815473665, "mse": 0.0}  0.4544
0         train   23900/102467 134:00/440:30      1.382         {"mlm": 1.3463050755404873, "mse": 0.0}  0.4346
0         train   24000/102467 134:32/439:53      1.382         {"mlm": 1.3459206068438252, "mse": 0.0}  0.4663
0         train   24100/102467 135:05/439:16      1.382         {"mlm": 1.3239092735611662, "mse": 0.0}  0.442
0         train   24200/102467 135:37/438:38      1.382         {"mlm": 1.3501154903811639, "mse": 0.0}  0.4482
0         train   24300/102467 136:10/438:01      1.382         {"mlm": 1.3563692511728145, "mse": 0.0}  0.4877
0         train   24400/102467 136:42/437:24      1.381         {"mlm": 1.3483977659263802, "mse": 0.0}  0.4884
0         train   24500/102467 137:15/436:46      1.381         {"mlm": 1.3466543318277382, "mse": 0.0}  0.4289
0         train   24600/102467 137:47/436:09      1.381         {"mlm": 1.3483034770026254, "mse": 0.0}  0.4775
0         train   24700/102467 138:20/435:32      1.381         {"mlm": 1.3522883096021363, "mse": 0.0}  0.4484
0         train   24800/102467 138:52/434:55      1.381         {"mlm": 1.3512284397182608, "mse": 0.0}  0.428
0         train   24900/102467 139:25/434:18      1.381         {"mlm": 1.3481331987343812, "mse": 0.0}  0.4265
0         train   25000/102467 139:57/433:41      1.380         {"mlm": 1.3469728592760817, "mse": 0.0}  0.4213
0         train   25100/102467 140:30/433:04      1.380         {"mlm": 1.3461094535005114, "mse": 0.0}  0.4386
0         train   25200/102467 141:02/432:27      1.380         {"mlm": 1.344450944950664, "mse": 0.0}  0.4461
0         train   25300/102467 141:34/431:50      1.380         {"mlm": 1.3432929626965926, "mse": 0.0}  0.4298
0         train   25400/102467 142:07/431:13      1.380         {"mlm": 1.3456564924236019, "mse": 0.0}  0.4273
0         train   25500/102467 142:39/430:36      1.380         {"mlm": 1.3453785305188717, "mse": 0.0}  0.4738
0         train   25600/102467 143:12/429:59      1.380         {"mlm": 1.3468851529164367, "mse": 0.0}  0.4576
0         train   25700/102467 143:44/429:22      1.380         {"mlm": 1.347079866626097, "mse": 0.0}  0.4101
0         train   25800/102467 144:17/428:46      1.379         {"mlm": 1.3461206491717508, "mse": 0.0}  0.4625
0         train   25900/102467 144:49/428:09      1.379         {"mlm": 1.3472284573837878, "mse": 0.0}  0.4412
0         train   26000/102467 145:22/427:33      1.379         {"mlm": 1.3469714457029336, "mse": 0.0}  0.4487
0         train   26100/102467 145:55/426:56      1.379         {"mlm": 1.3441045222823154, "mse": 0.0}  0.4552
0         train   26200/102467 146:27/426:20      1.379         {"mlm": 1.3464347145279048, "mse": 0.0}  0.4559
0         train   26300/102467 147:00/425:43      1.379         {"mlm": 1.3527691117440812, "mse": 0.0}  0.4732
0         train   26400/102467 147:32/425:07      1.379         {"mlm": 1.3573496862682948, "mse": 0.0}  0.4754
0         train   26500/102467 148:05/424:30      1.379         {"mlm": 1.3572637414788336, "mse": 0.0}  0.4701
0         train   26600/102467 148:37/423:54      1.379         {"mlm": 1.3577132766170916, "mse": 0.0}  0.4636
0         train   26700/102467 149:10/423:18      1.379         {"mlm": 1.3608780315992992, "mse": 0.0}  0.4827
0         train   26800/102467 149:42/422:41      1.379         {"mlm": 1.3604966879937999, "mse": 0.0}  0.4362
0         train   26900/102467 150:15/422:05      1.379         {"mlm": 1.3609438591045946, "mse": 0.0}  0.4563
0         train   27000/102467 150:47/421:29      1.378         {"mlm": 1.3575638043605456, "mse": 0.0}  0.5167
0         train   27100/102467 151:20/420:53      1.378         {"mlm": 1.3581846277063938, "mse": 0.0}  0.4202
0         train   27200/102467 151:52/420:17      1.378         {"mlm": 1.355314226526963, "mse": 0.0}  0.4701
0         train   27300/102467 152:25/419:40      1.378         {"mlm": 1.3560572316210915, "mse": 0.0}  0.5389
0         train   27400/102467 152:57/419:04      1.378         {"mlm": 1.3554017586458897, "mse": 0.0}  0.4919
0         train   27500/102467 153:30/418:28      1.378         {"mlm": 1.357224150347407, "mse": 0.0}  0.4409
0         train   27600/102467 154:02/417:52      1.378         {"mlm": 1.3588585680398482, "mse": 0.0}  0.4596
0         train   27700/102467 154:35/417:15      1.378         {"mlm": 1.3581848280108109, "mse": 0.0}  0.414
0         train   27800/102467 155:07/416:39      1.378         {"mlm": 1.3583026894875614, "mse": 0.0}  0.4518
0         train   27900/102467 155:40/416:03      1.378         {"mlm": 1.3570708080411398, "mse": 0.0}  0.4305
0         train   28000/102467 156:12/415:27      1.378         {"mlm": 1.3572557010111, "mse": 0.0}  0.4755
0         train   28100/102467 156:45/414:51      1.377         {"mlm": 1.3499935859193404, "mse": 0.0}  0.4156
0         train   28200/102467 157:17/414:15      1.377         {"mlm": 1.34840199594595, "mse": 0.0}  0.4827
0         train   28300/102467 157:50/413:39      1.377         {"mlm": 1.3403254775582134, "mse": 0.0}  0.4327
0         train   28400/102467 158:22/413:03      1.377         {"mlm": 1.3451635238498147, "mse": 0.0}  0.459
0         train   28500/102467 158:55/412:27      1.377         {"mlm": 1.3526456139741405, "mse": 0.0}  0.4572
0         train   28600/102467 159:27/411:51      1.377         {"mlm": 1.3578618146829158, "mse": 0.0}  0.4745
0         train   28700/102467 160:00/411:15      1.377         {"mlm": 1.3532415953860886, "mse": 0.0}  0.4643
0         train   28800/102467 160:32/410:39      1.377         {"mlm": 1.352373250764818, "mse": 0.0}  0.4514
0         train   28900/102467 161:05/410:03      1.377         {"mlm": 1.3524592446961574, "mse": 0.0}  0.5452
0         train   29000/102467 161:37/409:27      1.377         {"mlm": 1.3543596505280957, "mse": 0.0}  0.4695
0         train   29100/102467 162:10/408:52      1.377         {"mlm": 1.3556645239030358, "mse": 0.0}  0.4373
0         train   29200/102467 162:42/408:16      1.377         {"mlm": 1.3570976184562695, "mse": 0.0}  0.4568
0         train   29300/102467 163:15/407:40      1.377         {"mlm": 1.3566890134027711, "mse": 0.0}  0.4568
0         train   29400/102467 163:47/407:04      1.377         {"mlm": 1.3557214412016307, "mse": 0.0}  0.4357
0         train   29500/102467 164:20/406:29      1.376         {"mlm": 1.3558291875742337, "mse": 0.0}  0.4424
0         train   29600/102467 164:52/405:53      1.376         {"mlm": 1.3574885828677274, "mse": 0.0}  0.4746
0         train   29700/102467 165:25/405:17      1.376         {"mlm": 1.357022767816231, "mse": 0.0}  0.4713
0         train   29800/102467 165:57/404:42      1.376         {"mlm": 1.3571834711893098, "mse": 0.0}  0.4544
0         train   29900/102467 166:30/404:06      1.376         {"mlm": 1.355881254821387, "mse": 0.0}  0.4469
0         train   30000/102467 167:03/403:31      1.376         {"mlm": 1.3565988050911852, "mse": 0.0}  0.4393

10/05/2022 07:35:55 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter-64/0-step30000.pkl
0         valid   1/781        0:10/134:17        1.216           None
0         valid   101/781      0:25/ 2:49         1.242           None
0         valid   201/781      0:39/ 1:55         1.261           None
0         valid   301/781      0:54/ 1:27         1.259           None
0         valid   401/781      1:09/ 1:05         1.255           None
0         valid   501/781      1:24/ 0:47         1.259           None
0         valid   601/781      1:39/ 0:29         1.258           None
0         valid   701/781      1:54/ 0:13         1.258           None
0         valid   781/781      2:09/ 0:00         1.253         {"mlm": 1.253201024327785, "mse": 0.0, "train": 0.0}  None
0         train   30100/102467 169:45/408:07      1.376         {"mlm": 1.3610463750362396, "mse": 0.0}  0.4274
0         train   30200/102467 170:17/407:30      1.376         {"mlm": 1.352886182665825, "mse": 0.0}  0.45
0         train   30300/102467 170:50/406:53      1.376         {"mlm": 1.3641273434956869, "mse": 0.0}  0.5302
0         train   30400/102467 171:22/406:16      1.376         {"mlm": 1.3604102991521358, "mse": 0.0}  0.4794
0         train   30500/102467 171:55/405:39      1.376         {"mlm": 1.3584529329538346, "mse": 0.0}  0.452
0         train   30600/102467 172:27/405:02      1.376         {"mlm": 1.3571816713611284, "mse": 0.0}  0.4374
0         train   30700/102467 173:00/404:25      1.376         {"mlm": 1.3586479255131312, "mse": 0.0}  0.6012
0         train   30800/102467 173:32/403:48      1.376         {"mlm": 1.3576861245185137, "mse": 0.0}  0.4695
0         train   30900/102467 174:04/403:11      1.376         {"mlm": 1.355941926240921, "mse": 0.0}  0.4459
0         train   31000/102467 174:37/402:34      1.376         {"mlm": 1.3561134250164033, "mse": 0.0}  0.442
0         train   31100/102467 175:09/401:57      1.375         {"mlm": 1.357442226193168, "mse": 0.0}  0.4786
0         train   31200/102467 175:42/401:20      1.375         {"mlm": 1.3565326227247716, "mse": 0.0}  0.4493
0         train   31300/102467 176:14/400:44      1.375         {"mlm": 1.3569402300394497, "mse": 0.0}  0.4522
0         train   31400/102467 176:47/400:07      1.375         {"mlm": 1.3582558654035841, "mse": 0.0}  0.5036
0         train   31500/102467 177:19/399:30      1.375         {"mlm": 1.3559026541312535, "mse": 0.0}  0.4568
0         train   31600/102467 177:52/398:53      1.375         {"mlm": 1.3563864970579744, "mse": 0.0}  0.4824
0         train   31700/102467 178:24/398:17      1.375         {"mlm": 1.3582621934484034, "mse": 0.0}  0.7699
0         train   31800/102467 178:57/397:40      1.375         {"mlm": 1.3574951269891526, "mse": 0.0}  0.4409
0         train   31900/102467 179:29/397:04      1.375         {"mlm": 1.3571598775135842, "mse": 0.0}  0.4621
0         train   32000/102467 180:02/396:27      1.375         {"mlm": 1.3552377519011498, "mse": 0.0}  0.435
0         train   32100/102467 180:34/395:51      1.375         {"mlm": 1.3685426109969014, "mse": 0.0}  0.456
0         train   32200/102467 181:07/395:15      1.375         {"mlm": 1.3658680448580027, "mse": 0.0}  0.4577
0         train   32300/102467 181:40/394:38      1.375         {"mlm": 1.3599010207581281, "mse": 0.0}  0.427
0         train   32400/102467 182:12/394:02      1.375         {"mlm": 1.361106222434749, "mse": 0.0}  0.4968
0         train   32500/102467 182:45/393:25      1.375         {"mlm": 1.3585558690145643, "mse": 0.0}  0.4391
0         train   32600/102467 183:17/392:49      1.375         {"mlm": 1.3577253689152968, "mse": 0.0}  0.4711
0         train   32700/102467 183:49/392:12      1.374         {"mlm": 1.3560474399844975, "mse": 0.0}  0.4174
0         train   32800/102467 184:22/391:36      1.374         {"mlm": 1.3538997969281241, "mse": 0.0}  0.4375
0         train   32900/102467 184:54/391:00      1.374         {"mlm": 1.3556055740201567, "mse": 0.0}  0.4276
0         train   33000/102467 185:27/390:23      1.374         {"mlm": 1.359202791024018, "mse": 0.0}  0.4654
0         train   33100/102467 185:59/389:47      1.374         {"mlm": 1.358206692232231, "mse": 0.0}  0.4364
0         train   33200/102467 186:32/389:11      1.374         {"mlm": 1.3568981911760256, "mse": 0.0}  0.4391
0         train   33300/102467 187:04/388:34      1.374         {"mlm": 1.354873494288846, "mse": 0.0}  0.423
0         train   33400/102467 187:37/387:58      1.374         {"mlm": 1.3553691772242118, "mse": 0.0}  0.4206
0         train   33500/102467 188:09/387:22      1.374         {"mlm": 1.356289647514618, "mse": 0.0}  0.4694
0         train   33600/102467 188:42/386:45      1.374         {"mlm": 1.3556419328796334, "mse": 0.0}  0.4626
0         train   33700/102467 189:14/386:09      1.374         {"mlm": 1.3568015035073009, "mse": 0.0}  0.4359
0         train   33800/102467 189:47/385:33      1.374         {"mlm": 1.3582568411498417, "mse": 0.0}  0.9694
0         train   33900/102467 190:19/384:57      1.374         {"mlm": 1.3589742630327797, "mse": 0.0}  0.4568
0         train   34000/102467 190:52/384:21      1.374         {"mlm": 1.357201534399335, "mse": 0.0}  0.4297
0         train   34100/102467 191:24/383:45      1.374         {"mlm": 1.3361413192992309, "mse": 0.0}  0.4736
0         train   34200/102467 191:57/383:09      1.374         {"mlm": 1.351057775393881, "mse": 0.0}  0.4141
0         train   34300/102467 192:29/382:33      1.374         {"mlm": 1.3529736413651665, "mse": 0.0}  0.4532
0         train   34400/102467 193:01/381:57      1.374         {"mlm": 1.3574077949751562, "mse": 0.0}  0.5042
0         train   34500/102467 193:34/381:21      1.374         {"mlm": 1.3577487625510818, "mse": 0.0}  0.5649
0         train   34600/102467 194:06/380:44      1.373         {"mlm": 1.3513477015056738, "mse": 0.0}  0.4447
0         train   34700/102467 194:39/380:08      1.373         {"mlm": 1.3491494793092624, "mse": 0.0}  0.4558
0         train   34800/102467 195:11/379:32      1.373         {"mlm": 1.3500857942534568, "mse": 0.0}  0.424
0         train   34900/102467 195:44/378:56      1.373         {"mlm": 1.3515078751308085, "mse": 0.0}  0.4131
0         train   35000/102467 196:16/378:20      1.373         {"mlm": 1.3481924744670042, "mse": 0.0}  0.4479
0         train   35100/102467 196:49/377:45      1.373         {"mlm": 1.3494766710668749, "mse": 0.0}  0.4874
0         train   35200/102467 197:21/377:09      1.373         {"mlm": 1.3502248685328113, "mse": 0.0}  0.4211
0         train   35300/102467 197:54/376:33      1.373         {"mlm": 1.3508598646507792, "mse": 0.0}  0.4495
0         train   35400/102467 198:26/375:57      1.373         {"mlm": 1.3502589854730216, "mse": 0.0}  0.4568
0         train   35500/102467 198:59/375:22      1.373         {"mlm": 1.3510849356890042, "mse": 0.0}  0.4643
0         train   35600/102467 199:31/374:46      1.373         {"mlm": 1.3513483188402966, "mse": 0.0}  0.44
0         train   35700/102467 200:04/374:10      1.373         {"mlm": 1.3507351152537428, "mse": 0.0}  0.4536
0         train   35800/102467 200:36/373:35      1.373         {"mlm": 1.352571560490516, "mse": 0.0}  0.5028
0         train   35900/102467 201:09/372:59      1.373         {"mlm": 1.3536703200938203, "mse": 0.0}  0.4504
0         train   36000/102467 201:41/372:23      1.373         {"mlm": 1.354854246815881, "mse": 0.0}  0.4464
0         train   36100/102467 202:14/371:48      1.373         {"mlm": 1.3688268907291372, "mse": 0.0}  0.4254
0         train   36200/102467 202:46/371:12      1.373         {"mlm": 1.3663237415594498, "mse": 0.0}  0.4486
0         train   36300/102467 203:19/370:36      1.373         {"mlm": 1.361685777152026, "mse": 0.0}  0.4115
0         train   36400/102467 203:51/370:01      1.373         {"mlm": 1.364002348944282, "mse": 0.0}  0.4596
0         train   36500/102467 204:24/369:25      1.373         {"mlm": 1.361248861016402, "mse": 0.0}  0.4185
0         train   36600/102467 204:56/368:49      1.373         {"mlm": 1.3662647248512536, "mse": 0.0}  0.4531
0         train   36700/102467 205:29/368:14      1.373         {"mlm": 1.3666222617137038, "mse": 0.0}  0.4323
0         train   36800/102467 206:01/367:38      1.373         {"mlm": 1.3673401623777344, "mse": 0.0}  0.442
0         train   36900/102467 206:34/367:02      1.373         {"mlm": 1.3651646707900524, "mse": 0.0}  0.4399
0         train   37000/102467 207:06/366:27      1.373         {"mlm": 1.3642554945548775, "mse": 0.0}  0.4656
0         train   37100/102467 207:39/365:51      1.372         {"mlm": 1.3617762383158465, "mse": 0.0}  0.5322
0         train   37200/102467 208:11/365:16      1.372         {"mlm": 1.3615252235180753, "mse": 0.0}  0.4272
0         train   37300/102467 208:43/364:40      1.372         {"mlm": 1.36062419841909, "mse": 0.0}  0.435
0         train   37400/102467 209:16/364:05      1.372         {"mlm": 1.36345593879798, "mse": 0.0}  0.4467
0         train   37500/102467 209:48/363:29      1.372         {"mlm": 1.3631338051501003, "mse": 0.0}  0.4294
0         train   37600/102467 210:21/362:54      1.372         {"mlm": 1.3635292491766535, "mse": 0.0}  0.4471
0         train   37700/102467 210:53/362:18      1.372         {"mlm": 1.3626303217238234, "mse": 0.0}  0.4033
0         train   37800/102467 211:26/361:43      1.372         {"mlm": 1.3612607166577393, "mse": 0.0}  0.4445
0         train   37900/102467 211:58/361:08      1.372         {"mlm": 1.362696557592956, "mse": 0.0}  0.5405
0         train   38000/102467 212:31/360:32      1.372         {"mlm": 1.3622238015495782, "mse": 0.0}  0.4048
0         train   38100/102467 213:03/359:57      1.372         {"mlm": 1.3538348991423845, "mse": 0.0}  0.421
0         train   38200/102467 213:36/359:22      1.372         {"mlm": 1.34930241868204, "mse": 0.0}  0.4165
0         train   38300/102467 214:08/358:46      1.372         {"mlm": 1.3501619181117497, "mse": 0.0}  0.4383
0         train   38400/102467 214:41/358:11      1.372         {"mlm": 1.3486816256937355, "mse": 0.0}  0.4451
0         train   38500/102467 215:13/357:36      1.372         {"mlm": 1.3480431647791016, "mse": 0.0}  0.4547
0         train   38600/102467 215:46/357:01      1.372         {"mlm": 1.3509172036143757, "mse": 0.0}  3.0009
0         train   38700/102467 216:18/356:25      1.372         {"mlm": 1.3537433659550788, "mse": 0.0}  0.4246
0         train   38800/102467 216:51/355:50      1.372         {"mlm": 1.3552043650617551, "mse": 0.0}  0.4159
0         train   38900/102467 217:23/355:15      1.372         {"mlm": 1.3563979476291155, "mse": 0.0}  0.4637
0         train   39000/102467 217:56/354:39      1.372         {"mlm": 1.3547907826531842, "mse": 0.0}  0.442
0         train   39100/102467 218:28/354:04      1.372         {"mlm": 1.35484157127403, "mse": 0.0}  0.4078
0         train   39200/102467 219:01/353:29      1.372         {"mlm": 1.3552840443259497, "mse": 0.0}  0.421
0         train   39300/102467 219:33/352:54      1.372         {"mlm": 1.3572004435147031, "mse": 0.0}  0.4139
0         train   39400/102467 220:06/352:19      1.372         {"mlm": 1.3563734164039862, "mse": 0.0}  0.4605
0         train   39500/102467 220:38/351:44      1.372         {"mlm": 1.3559257369787299, "mse": 0.0}  0.4612
0         train   39600/102467 221:11/351:09      1.372         {"mlm": 1.356504358145826, "mse": 0.0}  0.5144
0         train   39700/102467 221:43/350:33      1.372         {"mlm": 1.3563522996275492, "mse": 0.0}  0.4181
0         train   39800/102467 222:16/349:58      1.371         {"mlm": 1.3565412930032459, "mse": 0.0}  0.4042
0         train   39900/102467 222:49/349:23      1.371         {"mlm": 1.3561314004886, "mse": 0.0}  0.4256
0         train   40000/102467 223:21/348:48      1.371         {"mlm": 1.356907884289841, "mse": 0.0}  0.4181

10/05/2022 08:32:14 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter-64/0-step40000.pkl
0         valid   1/781        0:15/198:42        1.318           None
0         valid   101/781      0:30/ 3:22         1.273           None
0         valid   201/781      0:44/ 2:09         1.286           None
0         valid   301/781      0:59/ 1:35         1.273           None
0         valid   401/781      1:14/ 1:10         1.266           None
0         valid   501/781      1:29/ 0:49         1.270           None
0         valid   601/781      1:44/ 0:31         1.266           None
0         valid   701/781      1:58/ 0:13         1.269           None
0         valid   781/781      2:14/ 0:00         1.265         {"mlm": 1.2645646606914211, "mse": 0.0, "train": 0.0}  None
0         train   40100/102467 226:08/351:43      1.371         {"mlm": 1.3860990417003631, "mse": 0.0}  0.4498
0         train   40200/102467 226:41/351:07      1.372         {"mlm": 1.3879973703622819, "mse": 0.0}  0.4776
0         train   40300/102467 227:13/350:31      1.371         {"mlm": 1.3820362236102421, "mse": 0.0}  0.4049
0         train   40400/102467 227:46/349:55      1.371         {"mlm": 1.3755712834000589, "mse": 0.0}  0.4124
0         train   40500/102467 228:18/349:20      1.371         {"mlm": 1.3684083876609803, "mse": 0.0}  0.4331
0         train   40600/102467 228:51/348:44      1.371         {"mlm": 1.3678442335128784, "mse": 0.0}  0.4276
0         train   40700/102467 229:24/348:08      1.371         {"mlm": 1.3607883466141564, "mse": 0.0}  0.4696
0         train   40800/102467 229:56/347:32      1.371         {"mlm": 1.3589273316413164, "mse": 0.0}  0.6225
0         train   40900/102467 230:29/346:57      1.371         {"mlm": 1.3617420548200607, "mse": 0.0}  0.429
0         train   41000/102467 231:01/346:21      1.371         {"mlm": 1.3593092151880264, "mse": 0.0}  0.4339
0         train   41100/102467 231:34/345:45      1.371         {"mlm": 1.3592319943688131, "mse": 0.0}  0.4324
0         train   41200/102467 232:06/345:09      1.371         {"mlm": 1.3590257124602794, "mse": 0.0}  0.405
0         train   41300/102467 232:39/344:34      1.371         {"mlm": 1.3596451798769145, "mse": 0.0}  0.412
0         train   41400/102467 233:11/343:58      1.371         {"mlm": 1.3582415814059121, "mse": 0.0}  0.4136
0         train   41500/102467 233:44/343:22      1.371         {"mlm": 1.3584103321234384, "mse": 0.0}  0.3889
0         train   41600/102467 234:16/342:47      1.371         {"mlm": 1.358430414572358, "mse": 0.0}  0.4173
0         train   41700/102467 234:49/342:11      1.371         {"mlm": 1.358808690975694, "mse": 0.0}  0.4145
0         train   41800/102467 235:21/341:35      1.371         {"mlm": 1.3583766638901498, "mse": 0.0}  0.4607
0         train   41900/102467 235:54/341:00      1.371         {"mlm": 1.358247843792564, "mse": 0.0}  0.4735
0         train   42000/102467 236:26/340:24      1.371         {"mlm": 1.3584407507777214, "mse": 0.0}  0.4181
0         train   42100/102467 236:59/339:48      1.371         {"mlm": 1.3401034010781183, "mse": 0.0}  0.4055
0         train   42200/102467 237:31/339:13      1.371         {"mlm": 1.3547381162643433, "mse": 0.0}  0.4325
0         train   42300/102467 238:04/338:37      1.371         {"mlm": 1.3710092190516034, "mse": 0.0}  0.421
0         train   42400/102467 238:36/338:02      1.371         {"mlm": 1.3753438481412137, "mse": 0.0}  0.4293
0         train   42500/102467 239:09/337:26      1.371         {"mlm": 1.3718094463816626, "mse": 0.0}  0.401
0         train   42600/102467 239:41/336:50      1.371         {"mlm": 1.3626320128249803, "mse": 0.0}  0.4051
0         train   42700/102467 240:14/336:15      1.371         {"mlm": 1.3605336429223482, "mse": 0.0}  0.403
0         train   42800/102467 240:46/335:39      1.371         {"mlm": 1.3609914983318505, "mse": 0.0}  0.4406
0         train   42900/102467 241:19/335:04      1.371         {"mlm": 1.3607888689826142, "mse": 0.0}  0.4144
0         train   43000/102467 241:51/334:28      1.371         {"mlm": 1.362500564591424, "mse": 0.0}  0.4267
0         train   43100/102467 242:24/333:53      1.371         {"mlm": 1.3611835333517837, "mse": 0.0}  0.8941
0         train   43200/102467 242:56/333:17      1.370         {"mlm": 1.3596532949812716, "mse": 0.0}  0.4062
0         train   43300/102467 243:29/332:42      1.370         {"mlm": 1.3601231597037018, "mse": 0.0}  0.4058
0         train   43400/102467 244:01/332:07      1.370         {"mlm": 1.36022130466513, "mse": 0.0}  0.4164
0         train   43500/102467 244:34/331:31      1.370         {"mlm": 1.3611268049641558, "mse": 0.0}  0.4171
0         train   43600/102467 245:06/330:56      1.370         {"mlm": 1.361545347436806, "mse": 0.0}  0.4219
0         train   43700/102467 245:39/330:21      1.370         {"mlm": 1.3612115100385722, "mse": 0.0}  0.4199
0         train   43800/102467 246:11/329:45      1.370         {"mlm": 1.362179679364347, "mse": 0.0}  0.4727
0         train   43900/102467 246:44/329:10      1.370         {"mlm": 1.3621103197351891, "mse": 0.0}  0.4087
0         train   44000/102467 247:17/328:35      1.370         {"mlm": 1.3623844895916262, "mse": 0.0}  0.38
0         train   44100/102467 247:49/328:00      1.370         {"mlm": 1.3488459112692852, "mse": 0.0}  0.834
0         train   44200/102467 248:22/327:24      1.370         {"mlm": 1.3534459841973854, "mse": 0.0}  0.9541
0         train   44300/102467 248:54/326:49      1.370         {"mlm": 1.3545255701013859, "mse": 0.0}  0.4304
0         train   44400/102467 249:27/326:14      1.370         {"mlm": 1.3532264347951017, "mse": 0.0}  0.4122
0         train   44500/102467 249:59/325:38      1.370         {"mlm": 1.3528168378584835, "mse": 0.0}  0.3849
0         train   44600/102467 250:32/325:03      1.370         {"mlm": 1.3566895807068484, "mse": 0.0}  0.3952
0         train   44700/102467 251:04/324:28      1.370         {"mlm": 1.3534516304475188, "mse": 0.0}  0.4379
0         train   44800/102467 251:37/323:53      1.370         {"mlm": 1.354342908339393, "mse": 0.0}  0.3942
0         train   44900/102467 252:09/323:18      1.370         {"mlm": 1.3535190914282555, "mse": 0.0}  0.3978
0         train   45000/102467 252:42/322:42      1.370         {"mlm": 1.355665170417759, "mse": 0.0}  0.398
0         train   45100/102467 253:14/322:07      1.370         {"mlm": 1.3567850177731018, "mse": 0.0}  0.886
0         train   45200/102467 253:47/321:32      1.370         {"mlm": 1.3590873589798285, "mse": 0.0}  0.4353
0         train   45300/102467 254:19/320:57      1.370         {"mlm": 1.361378452305801, "mse": 0.0}  0.3939
0         train   45400/102467 254:52/320:21      1.370         {"mlm": 1.3631384263990263, "mse": 0.0}  0.4002
0         train   45500/102467 255:24/319:46      1.370         {"mlm": 1.3605855615419125, "mse": 0.0}  0.3873
0         train   45600/102467 255:57/319:11      1.370         {"mlm": 1.3612804979048623, "mse": 0.0}  0.4072
0         train   45700/102467 256:29/318:36      1.370         {"mlm": 1.361278723104823, "mse": 0.0}  0.381
0         train   45800/102467 257:02/318:01      1.370         {"mlm": 1.3606533307055875, "mse": 0.0}  0.408
0         train   45900/102467 257:34/317:26      1.370         {"mlm": 1.359999000460883, "mse": 0.0}  1.7932
0         train   46000/102467 258:07/316:51      1.370         {"mlm": 1.361097686223917, "mse": 0.0}  0.4973
0         train   46100/102467 258:39/316:16      1.370         {"mlm": 1.3732528532903219, "mse": 0.0}  0.4453
0         train   46200/102467 259:12/315:41      1.370         {"mlm": 1.3638904627204547, "mse": 0.0}  0.4093
0         train   46300/102467 259:44/315:06      1.370         {"mlm": 1.3610800079223684, "mse": 0.0}  0.4117
0         train   46400/102467 260:17/314:30      1.370         {"mlm": 1.3647895027468127, "mse": 0.0}  0.449
0         train   46500/102467 260:49/313:55      1.370         {"mlm": 1.36312961170611, "mse": 0.0}  0.4937
0         train   46600/102467 261:22/313:20      1.370         {"mlm": 1.3631031407383418, "mse": 0.0}  0.4241
0         train   46700/102467 261:54/312:45      1.370         {"mlm": 1.3637030359833278, "mse": 0.0}  0.41
0         train   46800/102467 262:27/312:10      1.370         {"mlm": 1.3636393329430105, "mse": 0.0}  0.3902
0         train   46900/102467 262:59/311:35      1.370         {"mlm": 1.3610038144001062, "mse": 0.0}  0.4242
0         train   47000/102467 263:32/311:00      1.370         {"mlm": 1.3565715174737163, "mse": 0.0}  0.3857
0         train   47100/102467 264:04/310:25      1.370         {"mlm": 1.354676037130295, "mse": 0.0}  0.4194
0         train   47200/102467 264:37/309:50      1.370         {"mlm": 1.353361579907767, "mse": 0.0}  0.4269
0         train   47300/102467 265:09/309:15      1.370         {"mlm": 1.3535234569676031, "mse": 0.0}  0.3961
0         train   47400/102467 265:42/308:40      1.370         {"mlm": 1.353418066532337, "mse": 0.0}  0.3912
0         train   47500/102467 266:14/308:05      1.369         {"mlm": 1.3538766333717622, "mse": 0.0}  0.4056
0         train   47600/102467 266:47/307:31      1.369         {"mlm": 1.3539604524112, "mse": 0.0}  0.3971
0         train   47700/102467 267:19/306:56      1.369         {"mlm": 1.3543861139221056, "mse": 0.0}  0.4261
0         train   47800/102467 267:52/306:21      1.369         {"mlm": 1.3525501510204045, "mse": 0.0}  0.438
0         train   47900/102467 268:24/305:46      1.369         {"mlm": 1.3547639323965022, "mse": 0.0}  0.4092
0         train   48000/102467 268:57/305:11      1.369         {"mlm": 1.3543223717180441, "mse": 0.0}  0.4636
0         train   48100/102467 269:30/304:36      1.369         {"mlm": 1.3852751317123573, "mse": 0.0}  0.4401
0         train   48200/102467 270:02/304:02      1.369         {"mlm": 1.367807919273571, "mse": 0.0}  0.4235
0         train   48300/102467 270:35/303:27      1.369         {"mlm": 1.3558773772942054, "mse": 0.0}  0.3982
0         train   48400/102467 271:07/302:52      1.369         {"mlm": 1.352392766812835, "mse": 0.0}  0.4144
0         train   48500/102467 271:40/302:17      1.369         {"mlm": 1.351967070371874, "mse": 0.0}  0.3997
0         train   48600/102467 272:12/301:42      1.369         {"mlm": 1.3529334714348684, "mse": 0.0}  0.6069
0         train   48700/102467 272:45/301:08      1.369         {"mlm": 1.3533179752271751, "mse": 0.0}  0.4157
0         train   48800/102467 273:17/300:33      1.369         {"mlm": 1.3501838476364336, "mse": 0.0}  0.3845
0         train   48900/102467 273:50/299:58      1.369         {"mlm": 1.3518838304361063, "mse": 0.0}  0.5759
0         train   49000/102467 274:22/299:23      1.369         {"mlm": 1.3531269262952499, "mse": 0.0}  0.4598
0         train   49100/102467 274:55/298:48      1.369         {"mlm": 1.3501870823058768, "mse": 0.0}  0.3913
0         train   49200/102467 275:27/298:14      1.369         {"mlm": 1.3511073609757982, "mse": 0.0}  0.3625
0         train   49300/102467 276:00/297:39      1.369         {"mlm": 1.3503401776154835, "mse": 0.0}  0.4046
0         train   49400/102467 276:32/297:04      1.369         {"mlm": 1.3494170995530563, "mse": 0.0}  0.3726
0         train   49500/102467 277:05/296:29      1.369         {"mlm": 1.3488491598297567, "mse": 0.0}  0.4584
0         train   49600/102467 277:37/295:55      1.369         {"mlm": 1.3488566741011196, "mse": 0.0}  0.3712
0         train   49700/102467 278:10/295:20      1.369         {"mlm": 1.3509747279491626, "mse": 0.0}  0.3746
0         train   49800/102467 278:42/294:45      1.369         {"mlm": 1.351794621428031, "mse": 0.0}  0.489
0         train   49900/102467 279:15/294:10      1.369         {"mlm": 1.352182283620291, "mse": 0.0}  0.3931
0         train   50000/102467 279:47/293:36      1.369         {"mlm": 1.3512383525620004, "mse": 0.0}  0.375

10/05/2022 09:28:40 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter-64/0-step50000.pkl
0         valid   1/781        0:10/138:44        1.205           None
0         valid   101/781      0:25/ 2:51         1.250           None
0         valid   201/781      0:40/ 1:56         1.264           None
0         valid   301/781      0:55/ 1:27         1.262           None
0         valid   401/781      1:09/ 1:06         1.256           None
0         valid   501/781      1:24/ 0:47         1.259           None
0         valid   601/781      1:39/ 0:29         1.258           None
0         valid   701/781      1:54/ 0:13         1.262           None
0         valid   781/781      2:09/ 0:00         1.254         {"mlm": 1.2543213828425097, "mse": 0.0, "train": 0.0}  None
0         train   50100/102467 282:30/295:17      1.369         {"mlm": 1.328408260345459, "mse": 0.0}  0.3832
0         train   50200/102467 283:02/294:42      1.369         {"mlm": 1.3525082677602769, "mse": 0.0}  0.4019
0         train   50300/102467 283:35/294:06      1.369         {"mlm": 1.357448541323344, "mse": 0.0}  0.3623
0         train   50400/102467 284:07/293:31      1.369         {"mlm": 1.3598792791366576, "mse": 0.0}  0.4349
0         train   50500/102467 284:40/292:56      1.369         {"mlm": 1.3583347079753876, "mse": 0.0}  0.4925
0         train   50600/102467 285:12/292:21      1.369         {"mlm": 1.3608234868446987, "mse": 0.0}  0.4621
0         train   50700/102467 285:45/291:46      1.369         {"mlm": 1.3598775069202695, "mse": 0.0}  0.4148
0         train   50800/102467 286:18/291:11      1.368         {"mlm": 1.3585437196493149, "mse": 0.0}  0.5139
0         train   50900/102467 286:50/290:36      1.368         {"mlm": 1.3578606074386173, "mse": 0.0}  0.3938
0         train   51000/102467 287:23/290:01      1.368         {"mlm": 1.3582531292438507, "mse": 0.0}  0.3942
0         train   51100/102467 287:55/289:25      1.368         {"mlm": 1.3571210627122359, "mse": 0.0}  0.6094
0         train   51200/102467 288:28/288:50      1.368         {"mlm": 1.3590165635943412, "mse": 0.0}  0.3963
0         train   51300/102467 289:00/288:15      1.368         {"mlm": 1.3582503962516785, "mse": 0.0}  0.388
0         train   51400/102467 289:33/287:40      1.368         {"mlm": 1.3583482449395317, "mse": 0.0}  0.3829
0         train   51500/102467 290:05/287:05      1.368         {"mlm": 1.3575713452895481, "mse": 0.0}  0.4347
0         train   51600/102467 290:38/286:30      1.368         {"mlm": 1.35554690875113, "mse": 0.0}  0.7093
0         train   51700/102467 291:10/285:55      1.368         {"mlm": 1.3550197903899586, "mse": 0.0}  0.3964
0         train   51800/102467 291:42/285:20      1.368         {"mlm": 1.3537748103340468, "mse": 0.0}  0.425
0         train   51900/102467 292:15/284:45      1.368         {"mlm": 1.3555591845198682, "mse": 0.0}  0.6779
0         train   52000/102467 292:47/284:10      1.368         {"mlm": 1.355536764472723, "mse": 0.0}  0.3893
0         train   52100/102467 293:20/283:34      1.368         {"mlm": 1.366260411763432, "mse": 0.0}  0.5044
0         train   52200/102467 293:52/282:59      1.368         {"mlm": 1.3554346103165018, "mse": 0.0}  0.3579
0         train   52300/102467 294:25/282:24      1.368         {"mlm": 1.3571361845552323, "mse": 0.0}  0.4151
0         train   52400/102467 294:57/281:49      1.368         {"mlm": 1.3587727083598162, "mse": 0.0}  0.3913
0         train   52500/102467 295:30/281:15      1.368         {"mlm": 1.3495031586868729, "mse": 0.0}  0.3809
0         train   52600/102467 296:02/280:40      1.368         {"mlm": 1.349419550525525, "mse": 0.0}  0.3998
0         train   52700/102467 296:35/280:05      1.368         {"mlm": 1.3517528517392914, "mse": 0.0}  0.3799
0         train   52800/102467 297:07/279:30      1.368         {"mlm": 1.3505256056636386, "mse": 0.0}  0.375
0         train   52900/102467 297:40/278:55      1.368         {"mlm": 1.3534092392619115, "mse": 0.0}  0.3985
0         train   53000/102467 298:12/278:20      1.368         {"mlm": 1.3547948938948255, "mse": 0.0}  0.3805
0         train   53100/102467 298:45/277:45      1.368         {"mlm": 1.355132749668569, "mse": 0.0}  0.4682
0         train   53200/102467 299:17/277:10      1.368         {"mlm": 1.3554256704074328, "mse": 0.0}  0.3932
0         train   53300/102467 299:50/276:35      1.368         {"mlm": 1.3540160006243418, "mse": 0.0}  0.4148
0         train   53400/102467 300:22/276:00      1.368         {"mlm": 1.353220444819687, "mse": 0.0}  0.3669
0         train   53500/102467 300:55/275:25      1.368         {"mlm": 1.3527989938626535, "mse": 0.0}  0.3694
0         train   53600/102467 301:27/274:50      1.368         {"mlm": 1.353248053002611, "mse": 0.0}  0.3902
0         train   53700/102467 302:00/274:15      1.368         {"mlm": 1.3533822455218147, "mse": 0.0}  0.3787
0         train   53800/102467 302:33/273:41      1.368         {"mlm": 1.3520022261268632, "mse": 0.0}  0.3993
0         train   53900/102467 303:05/273:06      1.368         {"mlm": 1.3511754168592798, "mse": 0.0}  0.3728
0         train   54000/102467 303:38/272:31      1.367         {"mlm": 1.3510465743900717, "mse": 0.0}  0.3829
0         train   54100/102467 304:10/271:56      1.367         {"mlm": 1.3415552949418827, "mse": 0.0}  0.3937
0         train   54200/102467 304:43/271:21      1.367         {"mlm": 1.3350306225545479, "mse": 0.0}  0.3865
0         train   54300/102467 305:15/270:47      1.367         {"mlm": 1.3396282906100254, "mse": 0.0}  0.3752
0         train   54400/102467 305:48/270:12      1.367         {"mlm": 1.33895632924147, "mse": 0.0}  0.385
0         train   54500/102467 306:20/269:37      1.367         {"mlm": 1.342911166598998, "mse": 0.0}  0.376
0         train   54600/102467 306:53/269:02      1.367         {"mlm": 1.3414716558113544, "mse": 0.0}  0.5161
0         train   54700/102467 307:25/268:27      1.367         {"mlm": 1.3451575590751232, "mse": 0.0}  0.3737
0         train   54800/102467 307:58/267:53      1.367         {"mlm": 1.3472805157639927, "mse": 0.0}  0.3598
0         train   54900/102467 308:30/267:18      1.367         {"mlm": 1.3498688812776238, "mse": 0.0}  0.4165
0         train   55000/102467 309:03/266:43      1.367         {"mlm": 1.3498906262651953, "mse": 0.0}  0.4556
0         train   55100/102467 309:35/266:08      1.367         {"mlm": 1.3514667213071674, "mse": 0.0}  0.8969
0         train   55200/102467 310:08/265:33      1.367         {"mlm": 1.3526059025914123, "mse": 0.0}  0.3899
0         train   55300/102467 310:40/264:59      1.367         {"mlm": 1.3516781638693551, "mse": 0.0}  0.4085
0         train   55400/102467 311:13/264:24      1.367         {"mlm": 1.351665266508367, "mse": 0.0}  3.2451
0         train   55500/102467 311:45/263:49      1.367         {"mlm": 1.3508244347826979, "mse": 0.0}  0.3891
0         train   55600/102467 312:18/263:14      1.367         {"mlm": 1.3500809980945683, "mse": 0.0}  0.3861
0         train   55700/102467 312:50/262:40      1.367         {"mlm": 1.3492663232050177, "mse": 0.0}  0.655
0         train   55800/102467 313:22/262:05      1.367         {"mlm": 1.350189378250958, "mse": 0.0}  0.3834
0         train   55900/102467 313:55/261:30      1.367         {"mlm": 1.3507469032976473, "mse": 0.0}  0.3885
0         train   56000/102467 314:27/260:55      1.367         {"mlm": 1.351251491286733, "mse": 0.0}  0.3815
0         train   56100/102467 315:00/260:21      1.367         {"mlm": 1.3395467185482537, "mse": 0.0}  0.394
0         train   56200/102467 315:32/259:46      1.367         {"mlm": 1.3439971977078975, "mse": 0.0}  0.4747
0         train   56300/102467 316:05/259:11      1.367         {"mlm": 1.3447891586156004, "mse": 0.0}  0.3754
0         train   56400/102467 316:37/258:37      1.367         {"mlm": 1.3441863998357835, "mse": 0.0}  0.373
0         train   56500/102467 317:10/258:02      1.367         {"mlm": 1.3455781721972844, "mse": 0.0}  0.3939
0         train   56600/102467 317:42/257:27      1.367         {"mlm": 1.3469680331060834, "mse": 0.0}  0.4001
0         train   56700/102467 318:15/256:53      1.367         {"mlm": 1.3453244686639805, "mse": 0.0}  0.3906
0         train   56800/102467 318:47/256:18      1.367         {"mlm": 1.3456970764373148, "mse": 0.0}  0.3821
0         train   56900/102467 319:20/255:44      1.367         {"mlm": 1.344774454067915, "mse": 0.0}  0.4288
0         train   57000/102467 319:52/255:09      1.367         {"mlm": 1.3463025793149215, "mse": 0.0}  0.4106
0         train   57100/102467 320:25/254:34      1.366         {"mlm": 1.3454947841091383, "mse": 0.0}  0.3825
0         train   57200/102467 320:57/254:00      1.366         {"mlm": 1.3446108206870064, "mse": 0.0}  0.3763
0         train   57300/102467 321:30/253:25      1.366         {"mlm": 1.3430621748073521, "mse": 0.0}  0.3645
0         train   57400/102467 322:02/252:51      1.366         {"mlm": 1.3411359558296614, "mse": 0.0}  0.3767
0         train   57500/102467 322:35/252:16      1.366         {"mlm": 1.341083897776658, "mse": 0.0}  0.3625
0         train   57600/102467 323:07/251:42      1.366         {"mlm": 1.3405267112540242, "mse": 0.0}  0.3984
0         train   57700/102467 323:40/251:07      1.366         {"mlm": 1.3425433459040272, "mse": 0.0}  0.3869
0         train   57800/102467 324:12/250:32      1.366         {"mlm": 1.344034312596637, "mse": 0.0}  0.3701
0         train   57900/102467 324:45/249:58      1.366         {"mlm": 1.342189046289648, "mse": 0.0}  0.5372
0         train   58000/102467 325:17/249:23      1.366         {"mlm": 1.3409282097711406, "mse": 0.0}  0.3952
0         train   58100/102467 325:50/248:49      1.366         {"mlm": 1.3762806430459023, "mse": 0.0}  0.3839
0         train   58200/102467 326:22/248:14      1.366         {"mlm": 1.35057938585476, "mse": 0.0}  0.3744
0         train   58300/102467 326:55/247:39      1.366         {"mlm": 1.3544985176743687, "mse": 0.0}  0.4065
0         train   58400/102467 327:27/247:05      1.366         {"mlm": 1.3497455850093052, "mse": 0.0}  0.3614
0         train   58500/102467 327:59/246:30      1.366         {"mlm": 1.3450994860501058, "mse": 0.0}  0.4216
0         train   58600/102467 328:32/245:56      1.366         {"mlm": 1.3454626627616435, "mse": 0.0}  0.3842
0         train   58700/102467 329:04/245:21      1.366         {"mlm": 1.345262487472474, "mse": 0.0}  0.4012
0         train   58800/102467 329:37/244:47      1.366         {"mlm": 1.34356781698052, "mse": 0.0}  0.3898
0         train   58900/102467 330:09/244:12      1.366         {"mlm": 1.3392976618238859, "mse": 0.0}  0.3652
0         train   59000/102467 330:42/243:38      1.366         {"mlm": 1.341345155753764, "mse": 0.0}  0.3761
0         train   59100/102467 331:14/243:03      1.366         {"mlm": 1.3429576082930077, "mse": 0.0}  0.383
0         train   59200/102467 331:47/242:29      1.366         {"mlm": 1.3439726788164381, "mse": 0.0}  0.4065
0         train   59300/102467 332:19/241:54      1.366         {"mlm": 1.3450047118520294, "mse": 0.0}  1.6302
0         train   59400/102467 332:52/241:20      1.366         {"mlm": 1.3448180037104982, "mse": 0.0}  0.362
0         train   59500/102467 333:24/240:46      1.365         {"mlm": 1.3456948644974653, "mse": 0.0}  0.3473
0         train   59600/102467 333:57/240:11      1.365         {"mlm": 1.345917025529651, "mse": 0.0}  0.3852
0         train   59700/102467 334:29/239:37      1.365         {"mlm": 1.3475371985784117, "mse": 0.0}  0.3761
0         train   59800/102467 335:02/239:02      1.365         {"mlm": 1.346338425122019, "mse": 0.0}  0.3722
0         train   59900/102467 335:34/238:28      1.365         {"mlm": 1.345697127691301, "mse": 0.0}  0.4066
0         train   60000/102467 336:07/237:53      1.365         {"mlm": 1.3461175143778443, "mse": 0.0}  0.394

10/05/2022 10:24:59 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter-64/0-step60000.pkl
0         valid   1/781        0:10/141:08        1.189           None
0         valid   101/781      0:25/ 2:52         1.245           None
0         valid   201/781      0:40/ 1:56         1.262           None
0         valid   301/781      0:55/ 1:27         1.253           None
0         valid   401/781      1:09/ 1:06         1.250           None
0         valid   501/781      1:24/ 0:47         1.252           None
0         valid   601/781      1:39/ 0:29         1.251           None
0         valid   701/781      1:54/ 0:13         1.254           None
0         valid   781/781      2:10/ 0:00         1.246         {"mlm": 1.2459987195902689, "mse": 0.0, "train": 0.0}  None
0         train   60100/102467 338:49/238:51      1.365         {"mlm": 1.3436786341667175, "mse": 0.0}  0.3806
0         train   60200/102467 339:22/238:16      1.365         {"mlm": 1.3492191672325133, "mse": 0.0}  0.3792
0         train   60300/102467 339:54/237:41      1.365         {"mlm": 1.344625855088234, "mse": 0.0}  0.3873
0         train   60400/102467 340:27/237:07      1.365         {"mlm": 1.346572825908661, "mse": 0.0}  0.3852
0         train   60500/102467 340:59/236:32      1.365         {"mlm": 1.3455105917453767, "mse": 0.0}  0.386
0         train   60600/102467 341:32/235:57      1.365         {"mlm": 1.345492338736852, "mse": 0.0}  0.3647
0         train   60700/102467 342:04/235:23      1.365         {"mlm": 1.345261833667755, "mse": 0.0}  0.3765
0         train   60800/102467 342:37/234:48      1.365         {"mlm": 1.3448700494319201, "mse": 0.0}  0.3856
0         train   60900/102467 343:10/234:13      1.365         {"mlm": 1.3419804618755977, "mse": 0.0}  0.3533
0         train   61000/102467 343:42/233:38      1.365         {"mlm": 1.3431645832657815, "mse": 0.0}  0.4022
0         train   61100/102467 344:15/233:04      1.365         {"mlm": 1.3432638617537238, "mse": 0.0}  0.3689
0         train   61200/102467 344:47/232:29      1.365         {"mlm": 1.344073347747326, "mse": 0.0}  1.0597
0         train   61300/102467 345:20/231:54      1.365         {"mlm": 1.3441158121365768, "mse": 0.0}  1.4808
0         train   61400/102467 345:52/231:20      1.365         {"mlm": 1.344786943410124, "mse": 0.0}  0.3727
0         train   61500/102467 346:24/230:45      1.365         {"mlm": 1.3449253714084626, "mse": 0.0}  0.3858
0         train   61600/102467 346:57/230:10      1.365         {"mlm": 1.3436400624737144, "mse": 0.0}  0.388
0         train   61700/102467 347:29/229:36      1.365         {"mlm": 1.3427226209289886, "mse": 0.0}  0.3825
0         train   61800/102467 348:02/229:01      1.365         {"mlm": 1.3439608385827806, "mse": 0.0}  0.3837
0         train   61900/102467 348:34/228:26      1.365         {"mlm": 1.3456303822366815, "mse": 0.0}  0.389
0         train   62000/102467 349:07/227:52      1.365         {"mlm": 1.3456181070804596, "mse": 0.0}  0.3767
0         train   62100/102467 349:39/227:17      1.365         {"mlm": 1.3413215682964132, "mse": 0.0}  0.387
0         train   62200/102467 350:12/226:42      1.365         {"mlm": 1.3386366609951958, "mse": 0.0}  0.5696
0         train   62300/102467 350:44/226:08      1.365         {"mlm": 1.3367372797085688, "mse": 0.0}  0.3928
0         train   62400/102467 351:16/225:33      1.365         {"mlm": 1.3333296689174527, "mse": 0.0}  0.3647
0         train   62500/102467 351:49/224:58      1.364         {"mlm": 1.3301063615955666, "mse": 0.0}  0.3624
0         train   62600/102467 352:21/224:24      1.364         {"mlm": 1.3343096152569893, "mse": 0.0}  0.8706
0         train   62700/102467 352:54/223:49      1.364         {"mlm": 1.3347910505337095, "mse": 0.0}  0.384
0         train   62800/102467 353:26/223:15      1.364         {"mlm": 1.337812784840079, "mse": 0.0}  0.4145
0         train   62900/102467 353:59/222:40      1.364         {"mlm": 1.3411443112822077, "mse": 0.0}  0.366
0         train   63000/102467 354:31/222:05      1.364         {"mlm": 1.3407016774197598, "mse": 0.0}  0.3844
0         train   63100/102467 355:04/221:31      1.364         {"mlm": 1.339813960477154, "mse": 0.0}  0.3871
0         train   63200/102467 355:36/220:56      1.364         {"mlm": 1.3427507115762565, "mse": 0.0}  0.3608
0         train   63300/102467 356:09/220:22      1.364         {"mlm": 1.3438288603864146, "mse": 0.0}  0.3736
0         train   63400/102467 356:41/219:47      1.364         {"mlm": 1.3433500127846894, "mse": 0.0}  0.3695
0         train   63500/102467 357:14/219:13      1.364         {"mlm": 1.342318925243604, "mse": 0.0}  0.3655
0         train   63600/102467 357:47/218:38      1.364         {"mlm": 1.3425754776144116, "mse": 0.0}  0.4153
0         train   63700/102467 358:19/218:04      1.364         {"mlm": 1.3411619191733861, "mse": 0.0}  0.3576
0         train   63800/102467 358:52/217:29      1.364         {"mlm": 1.3413161709283443, "mse": 0.0}  0.3747
0         train   63900/102467 359:24/216:55      1.364         {"mlm": 1.340770434743169, "mse": 0.0}  0.3975
0         train   64000/102467 359:57/216:20      1.364         {"mlm": 1.3397624819442115, "mse": 0.0}  0.3802
0         train   64100/102467 360:29/215:46      1.364         {"mlm": 1.3390043073770952, "mse": 0.0}  0.3618
0         train   64200/102467 361:01/215:11      1.364         {"mlm": 1.3188641086371258, "mse": 0.0}  0.6523
0         train   64300/102467 361:34/214:37      1.364         {"mlm": 1.338930980271141, "mse": 0.0}  0.3696
0         train   64400/102467 362:06/214:02      1.364         {"mlm": 1.3388788679137302, "mse": 0.0}  0.4321
0         train   64500/102467 362:39/213:28      1.364         {"mlm": 1.3360906226567955, "mse": 0.0}  0.3595
0         train   64600/102467 363:11/212:53      1.364         {"mlm": 1.3342543186551352, "mse": 0.0}  0.3714
0         train   64700/102467 363:44/212:19      1.364         {"mlm": 1.3390812960941676, "mse": 0.0}  0.3684
0         train   64800/102467 364:16/211:44      1.364         {"mlm": 1.335228243268522, "mse": 0.0}  0.3651
0         train   64900/102467 364:49/211:10      1.364         {"mlm": 1.3371341672533072, "mse": 0.0}  0.3512
0         train   65000/102467 365:21/210:35      1.364         {"mlm": 1.3387876703648385, "mse": 0.0}  0.384
0         train   65100/102467 365:54/210:01      1.364         {"mlm": 1.339946563940882, "mse": 0.0}  0.395
0         train   65200/102467 366:26/209:27      1.363         {"mlm": 1.3380786696937128, "mse": 0.0}  0.3822
0         train   65300/102467 366:59/208:52      1.363         {"mlm": 1.3375364890083876, "mse": 0.0}  0.3891
0         train   65400/102467 367:31/208:18      1.363         {"mlm": 1.3383560075780352, "mse": 0.0}  0.391
0         train   65500/102467 368:04/207:43      1.363         {"mlm": 1.3360305854252406, "mse": 0.0}  0.3584
0         train   65600/102467 368:36/207:09      1.363         {"mlm": 1.3353064049394319, "mse": 0.0}  2.2505
0         train   65700/102467 369:08/206:34      1.363         {"mlm": 1.3368347077614007, "mse": 0.0}  0.4033
0         train   65800/102467 369:41/206:00      1.363         {"mlm": 1.3371835853220757, "mse": 0.0}  0.3723
0         train   65900/102467 370:13/205:26      1.363         {"mlm": 1.3368206467907346, "mse": 0.0}  0.8736
0         train   66000/102467 370:46/204:51      1.363         {"mlm": 1.3352999853359926, "mse": 0.0}  0.3612
0         train   66100/102467 371:18/204:17      1.363         {"mlm": 1.3470523381970592, "mse": 0.0}  0.3779
0         train   66200/102467 371:51/203:43      1.363         {"mlm": 1.3465183213882639, "mse": 0.0}  0.3792
0         train   66300/102467 372:24/203:08      1.363         {"mlm": 1.3441751535091335, "mse": 0.0}  0.3963
0         train   66400/102467 372:56/202:34      1.363         {"mlm": 1.3466303722383994, "mse": 0.0}  0.5314
0         train   66500/102467 373:29/202:00      1.363         {"mlm": 1.347312994406257, "mse": 0.0}  0.3758
0         train   66600/102467 374:01/201:25      1.363         {"mlm": 1.345785819605567, "mse": 0.0}  0.3757
0         train   66700/102467 374:34/200:51      1.363         {"mlm": 1.3493623530847612, "mse": 0.0}  0.3609
0         train   66800/102467 375:06/200:17      1.363         {"mlm": 1.349825842542062, "mse": 0.0}  0.3773
0         train   66900/102467 375:39/199:42      1.363         {"mlm": 1.350705866497363, "mse": 0.0}  0.3892
0         train   67000/102467 376:11/199:08      1.363         {"mlm": 1.3479495160916863, "mse": 0.0}  0.3816
0         train   67100/102467 376:44/198:34      1.363         {"mlm": 1.3479260420299382, "mse": 0.0}  0.3788
0         train   67200/102467 377:16/197:59      1.363         {"mlm": 1.3478997274050637, "mse": 0.0}  0.3984
0         train   67300/102467 377:49/197:25      1.363         {"mlm": 1.3470623861759925, "mse": 0.0}  0.4019
0         train   67400/102467 378:21/196:51      1.363         {"mlm": 1.3465168476104736, "mse": 0.0}  0.4146
0         train   67500/102467 378:54/196:16      1.363         {"mlm": 1.3473214322754599, "mse": 0.0}  0.3686
0         train   67600/102467 379:26/195:42      1.363         {"mlm": 1.3489945913450974, "mse": 0.0}  0.3735
0         train   67700/102467 379:59/195:08      1.363         {"mlm": 1.3480046259422618, "mse": 0.0}  0.3672
0         train   67800/102467 380:31/194:34      1.363         {"mlm": 1.3475842134582379, "mse": 0.0}  0.3939
0         train   67900/102467 381:04/193:59      1.363         {"mlm": 1.3470062234556293, "mse": 0.0}  0.3774
0         train   68000/102467 381:36/193:25      1.363         {"mlm": 1.3451202357776415, "mse": 0.0}  0.389
0         train   68100/102467 382:08/192:51      1.363         {"mlm": 1.374522415921092, "mse": 0.0}  0.394
0         train   68200/102467 382:41/192:16      1.363         {"mlm": 1.371210054475434, "mse": 0.0}  0.3847
0         train   68300/102467 383:13/191:42      1.363         {"mlm": 1.3526861981765643, "mse": 0.0}  0.4053
0         train   68400/102467 383:46/191:08      1.362         {"mlm": 1.342869785247427, "mse": 0.0}  0.3757
0         train   68500/102467 384:18/190:34      1.362         {"mlm": 1.3431983596134571, "mse": 0.0}  0.3659
0         train   68600/102467 384:51/190:00      1.362         {"mlm": 1.3365826878771685, "mse": 0.0}  0.3644
0         train   68700/102467 385:24/189:25      1.362         {"mlm": 1.3406445762549324, "mse": 0.0}  0.3669
0         train   68800/102467 385:56/188:51      1.362         {"mlm": 1.3388513856796762, "mse": 0.0}  0.4002
0         train   68900/102467 386:29/188:17      1.362         {"mlm": 1.3384404024109244, "mse": 0.0}  0.4101
0         train   69000/102467 387:01/187:43      1.362         {"mlm": 1.3406657973925273, "mse": 0.0}  0.3675
0         train   69100/102467 387:34/187:09      1.362         {"mlm": 1.3417492805823792, "mse": 0.0}  0.3792
0         train   69200/102467 388:06/186:34      1.362         {"mlm": 1.3396219174598771, "mse": 0.0}  0.3842
0         train   69300/102467 388:39/186:00      1.362         {"mlm": 1.3393617472898813, "mse": 0.0}  0.3672
0         train   69400/102467 389:11/185:26      1.362         {"mlm": 1.3403001094615903, "mse": 0.0}  0.3844
0         train   69500/102467 389:44/184:52      1.362         {"mlm": 1.3393397785842738, "mse": 0.0}  0.3581
0         train   69600/102467 390:16/184:18      1.362         {"mlm": 1.33721720899705, "mse": 0.0}  0.5357
0         train   69700/102467 390:49/183:43      1.362         {"mlm": 1.3375372786145165, "mse": 0.0}  0.3979
0         train   69800/102467 391:21/183:09      1.362         {"mlm": 1.3379026248155033, "mse": 0.0}  0.3936
0         train   69900/102467 391:54/182:35      1.362         {"mlm": 1.3381810298951868, "mse": 0.0}  0.3769
0         train   70000/102467 392:26/182:01      1.362         {"mlm": 1.3362445657501718, "mse": 0.0}  0.3681

10/05/2022 11:21:19 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter-64/0-step70000.pkl
0         valid   1/781        0:10/139:59        1.067           None
0         valid   101/781      0:25/ 2:51         1.241           None
0         valid   201/781      0:40/ 1:56         1.258           None
0         valid   301/781      0:55/ 1:27         1.254           None
0         valid   401/781      1:09/ 1:06         1.249           None
0         valid   501/781      1:24/ 0:47         1.247           None
0         valid   601/781      1:39/ 0:29         1.241           None
0         valid   701/781      1:54/ 0:13         1.246           None
0         valid   781/781      2:09/ 0:00         1.242         {"mlm": 1.2413572343149808, "mse": 0.0, "train": 0.0}  None
0         train   70100/102467 395:09/182:27      1.362         {"mlm": 1.3836372309923173, "mse": 0.0}  0.3979
0         train   70200/102467 395:41/181:52      1.362         {"mlm": 1.3461075815558434, "mse": 0.0}  0.3576
0         train   70300/102467 396:14/181:18      1.362         {"mlm": 1.337345778743426, "mse": 0.0}  0.5169
0         train   70400/102467 396:46/180:43      1.362         {"mlm": 1.340675623267889, "mse": 0.0}  0.3919
0         train   70500/102467 397:19/180:09      1.362         {"mlm": 1.3411796275377275, "mse": 0.0}  0.4218
0         train   70600/102467 397:51/179:34      1.362         {"mlm": 1.3395263654987017, "mse": 0.0}  0.3692
0         train   70700/102467 398:23/179:00      1.362         {"mlm": 1.342006419641631, "mse": 0.0}  0.3701
0         train   70800/102467 398:56/178:26      1.362         {"mlm": 1.3412973556667567, "mse": 0.0}  0.3599
0         train   70900/102467 399:28/177:51      1.362         {"mlm": 1.3404635994964176, "mse": 0.0}  0.3567
0         train   71000/102467 400:01/177:17      1.362         {"mlm": 1.3423697654008866, "mse": 0.0}  0.3516
0         train   71100/102467 400:33/176:42      1.361         {"mlm": 1.3402897775173188, "mse": 0.0}  0.356
0         train   71200/102467 401:06/176:08      1.361         {"mlm": 1.3372719948987166, "mse": 0.0}  0.3834
0         train   71300/102467 401:38/175:34      1.361         {"mlm": 1.335272502348973, "mse": 0.0}  0.3766
0         train   71400/102467 402:11/174:59      1.361         {"mlm": 1.335179129583495, "mse": 0.0}  0.3679
0         train   71500/102467 402:43/174:25      1.361         {"mlm": 1.3360823594729105, "mse": 0.0}  0.3773
0         train   71600/102467 403:16/173:50      1.361         {"mlm": 1.3368847895041107, "mse": 0.0}  0.3696
0         train   71700/102467 403:48/173:16      1.361         {"mlm": 1.336231788256589, "mse": 0.0}  0.3626
0         train   71800/102467 404:21/172:42      1.361         {"mlm": 1.3363630099097887, "mse": 0.0}  0.3794
0         train   71900/102467 404:53/172:07      1.361         {"mlm": 1.33726669622095, "mse": 0.0}  0.4
0         train   72000/102467 405:25/171:33      1.361         {"mlm": 1.3367392758727075, "mse": 0.0}  0.4174
0         train   72100/102467 405:58/170:59      1.361         {"mlm": 1.325415384889853, "mse": 0.0}  0.4303
0         train   72200/102467 406:31/170:24      1.361         {"mlm": 1.3281050150717921, "mse": 0.0}  0.3833
0         train   72300/102467 407:03/169:50      1.361         {"mlm": 1.3298988607416184, "mse": 0.0}  0.3918
0         train   72400/102467 407:36/169:16      1.361         {"mlm": 1.3283763258977044, "mse": 0.0}  0.3825
0         train   72500/102467 408:08/168:42      1.361         {"mlm": 1.3280181795179486, "mse": 0.0}  0.3667
0         train   72600/102467 408:41/168:07      1.361         {"mlm": 1.3309880736873225, "mse": 0.0}  0.43
0         train   72700/102467 409:13/167:33      1.361         {"mlm": 1.329765710793169, "mse": 0.0}  0.3587
0         train   72800/102467 409:45/166:59      1.361         {"mlm": 1.3333833061708824, "mse": 0.0}  0.371
0         train   72900/102467 410:18/166:24      1.361         {"mlm": 1.3314213506769683, "mse": 0.0}  0.37
0         train   73000/102467 410:50/165:50      1.361         {"mlm": 1.3326606563142351, "mse": 0.0}  0.395
0         train   73100/102467 411:23/165:16      1.361         {"mlm": 1.3362764552357633, "mse": 0.0}  0.3671
0         train   73200/102467 411:55/164:41      1.361         {"mlm": 1.3373734303272398, "mse": 0.0}  0.3841
0         train   73300/102467 412:28/164:07      1.361         {"mlm": 1.3386437433145888, "mse": 0.0}  0.4257
0         train   73400/102467 413:00/163:33      1.361         {"mlm": 1.339149086538428, "mse": 0.0}  0.3599
0         train   73500/102467 413:33/162:59      1.361         {"mlm": 1.3396879707359328, "mse": 0.0}  0.3741
0         train   73600/102467 414:05/162:24      1.361         {"mlm": 1.3402249072028967, "mse": 0.0}  0.3599
0         train   73700/102467 414:38/161:50      1.361         {"mlm": 1.3403890443732276, "mse": 0.0}  0.3976
0         train   73800/102467 415:10/161:16      1.361         {"mlm": 1.3396198533189634, "mse": 0.0}  0.3846
0         train   73900/102467 415:42/160:42      1.361         {"mlm": 1.3392039055319571, "mse": 0.0}  0.3891
0         train   74000/102467 416:15/160:07      1.360         {"mlm": 1.338674557692054, "mse": 0.0}  0.3581
0         train   74100/102467 416:47/159:33      1.360         {"mlm": 1.3085654274541505, "mse": 0.0}  0.4002
0         train   74200/102467 417:20/158:59      1.360         {"mlm": 1.326510349608431, "mse": 0.0}  0.3836
0         train   74300/102467 417:52/158:25      1.360         {"mlm": 1.325795608278889, "mse": 0.0}  0.3907
0         train   74400/102467 418:25/157:50      1.360         {"mlm": 1.3240869511012456, "mse": 0.0}  0.3671
0         train   74500/102467 418:57/157:16      1.360         {"mlm": 1.3260823885599773, "mse": 0.0}  0.3865
0         train   74600/102467 419:29/156:42      1.360         {"mlm": 1.3278015710438373, "mse": 0.0}  0.4236
0         train   74700/102467 420:02/156:08      1.360         {"mlm": 1.3285179521600292, "mse": 0.0}  0.3712
0         train   74800/102467 420:34/155:33      1.360         {"mlm": 1.3314045452533807, "mse": 0.0}  0.3659
0         train   74900/102467 421:07/154:59      1.360         {"mlm": 1.333869117405473, "mse": 0.0}  0.3466
0         train   75000/102467 421:39/154:25      1.360         {"mlm": 1.3356406058003765, "mse": 0.0}  0.3999
0         train   75100/102467 422:12/153:51      1.360         {"mlm": 1.3356594876513455, "mse": 0.0}  0.4215
0         train   75200/102467 422:44/153:17      1.360         {"mlm": 1.3349565882018095, "mse": 0.0}  0.3624
0         train   75300/102467 423:17/152:42      1.360         {"mlm": 1.336634239907992, "mse": 0.0}  0.3826
0         train   75400/102467 423:49/152:08      1.360         {"mlm": 1.3373969468862372, "mse": 0.0}  0.37
0         train   75500/102467 424:22/151:34      1.360         {"mlm": 1.3366186189476414, "mse": 0.0}  0.3755
0         train   75600/102467 424:54/151:00      1.360         {"mlm": 1.33374335104891, "mse": 0.0}  0.5053
0         train   75700/102467 425:27/150:26      1.360         {"mlm": 1.3342504993133748, "mse": 0.0}  0.3563
0         train   75800/102467 425:59/149:52      1.360         {"mlm": 1.333867687736125, "mse": 0.0}  0.6687
0         train   75900/102467 426:32/149:17      1.360         {"mlm": 1.3341250361205403, "mse": 0.0}  0.3634
0         train   76000/102467 427:04/148:43      1.360         {"mlm": 1.3356024846956656, "mse": 0.0}  0.6619
0         train   76100/102467 427:37/148:09      1.360         {"mlm": 1.336176637521724, "mse": 0.0}  0.4154
0         train   76200/102467 428:09/147:35      1.360         {"mlm": 1.342716885702259, "mse": 0.0}  0.3871
0         train   76300/102467 428:41/147:01      1.360         {"mlm": 1.3380053103572191, "mse": 0.0}  0.3786
0         train   76400/102467 429:14/146:27      1.360         {"mlm": 1.3369438125744875, "mse": 0.0}  0.3958
0         train   76500/102467 429:46/145:53      1.360         {"mlm": 1.3393906322523381, "mse": 0.0}  0.3578
0         train   76600/102467 430:19/145:18      1.360         {"mlm": 1.3365116385958302, "mse": 0.0}  0.3682
0         train   76700/102467 430:51/144:44      1.360         {"mlm": 1.3372688935682113, "mse": 0.0}  0.387
0         train   76800/102467 431:24/144:10      1.360         {"mlm": 1.33211538326486, "mse": 0.0}  0.3602
0         train   76900/102467 431:56/143:36      1.360         {"mlm": 1.3314182010116917, "mse": 0.0}  0.3768
0         train   77000/102467 432:29/143:02      1.359         {"mlm": 1.329303834658329, "mse": 0.0}  1.0018
0         train   77100/102467 433:01/142:28      1.359         {"mlm": 1.328338432496748, "mse": 0.0}  0.3945
0         train   77200/102467 433:34/141:54      1.359         {"mlm": 1.3288996610028005, "mse": 0.0}  1.8223
0         train   77300/102467 434:06/141:20      1.359         {"mlm": 1.328554679865826, "mse": 0.0}  0.3892
0         train   77400/102467 434:39/140:46      1.359         {"mlm": 1.3283134584522451, "mse": 0.0}  0.3759
0         train   77500/102467 435:11/140:11      1.359         {"mlm": 1.328404354627083, "mse": 0.0}  0.3565
0         train   77600/102467 435:43/139:37      1.359         {"mlm": 1.328118173532659, "mse": 0.0}  0.422
0         train   77700/102467 436:16/139:03      1.359         {"mlm": 1.3286822790529143, "mse": 0.0}  2.0429
0         train   77800/102467 436:48/138:29      1.359         {"mlm": 1.3294478771616236, "mse": 0.0}  0.3858
0         train   77900/102467 437:21/137:55      1.359         {"mlm": 1.3283016536194334, "mse": 0.0}  0.4018
0         train   78000/102467 437:53/137:21      1.359         {"mlm": 1.327796498327537, "mse": 0.0}  0.9236
0         train   78100/102467 438:26/136:47      1.359         {"mlm": 1.3416826756050189, "mse": 0.0}  0.3757
0         train   78200/102467 438:58/136:13      1.359         {"mlm": 1.3299153127840586, "mse": 0.0}  0.3784
0         train   78300/102467 439:31/135:39      1.359         {"mlm": 1.3298986656843006, "mse": 0.0}  0.4225
0         train   78400/102467 440:03/135:05      1.359         {"mlm": 1.325385140800717, "mse": 0.0}  0.3567
0         train   78500/102467 440:36/134:31      1.359         {"mlm": 1.32116201315676, "mse": 0.0}  0.3982
0         train   78600/102467 441:08/133:57      1.359         {"mlm": 1.3318310652003191, "mse": 0.0}  0.3638
0         train   78700/102467 441:41/133:23      1.359         {"mlm": 1.3355630411841404, "mse": 0.0}  0.3711
0         train   78800/102467 442:13/132:49      1.359         {"mlm": 1.3369304208300221, "mse": 0.0}  0.3657
0         train   78900/102467 442:45/132:15      1.359         {"mlm": 1.3377001594219888, "mse": 0.0}  0.353
0         train   79000/102467 443:18/131:41      1.359         {"mlm": 1.3374739636977513, "mse": 0.0}  0.5771
0         train   79100/102467 443:50/131:07      1.359         {"mlm": 1.3375127131703997, "mse": 0.0}  0.412
0         train   79200/102467 444:23/130:33      1.359         {"mlm": 1.3363376826446591, "mse": 0.0}  0.3753
0         train   79300/102467 444:55/129:58      1.359         {"mlm": 1.3371154324691972, "mse": 0.0}  0.3759
0         train   79400/102467 445:28/129:24      1.359         {"mlm": 1.3357164142466549, "mse": 0.0}  0.3878
0         train   79500/102467 446:00/128:50      1.359         {"mlm": 1.335554153484138, "mse": 0.0}  0.3722
0         train   79600/102467 446:33/128:16      1.359         {"mlm": 1.3339057587143173, "mse": 0.0}  0.3638
0         train   79700/102467 447:05/127:42      1.358         {"mlm": 1.3311223035924278, "mse": 0.0}  0.3596
0         train   79800/102467 447:37/127:08      1.358         {"mlm": 1.3319143796617576, "mse": 0.0}  0.4048
0         train   79900/102467 448:10/126:34      1.358         {"mlm": 1.331287568945925, "mse": 0.0}  0.3865
0         train   80000/102467 448:42/126:00      1.358         {"mlm": 1.3316818187255897, "mse": 0.0}  0.4054

10/05/2022 12:17:35 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter-64/0-step80000.pkl
0         valid   1/781        0:10/136:51        1.231           None
0         valid   101/781      0:25/ 2:49         1.246           None
0         valid   201/781      0:39/ 1:55         1.255           None
0         valid   301/781      0:54/ 1:27         1.248           None
0         valid   401/781      1:09/ 1:05         1.244           None
0         valid   501/781      1:24/ 0:47         1.242           None
0         valid   601/781      1:39/ 0:29         1.243           None
0         valid   701/781      1:54/ 0:13         1.244           None
0         valid   781/781      2:09/ 0:00         1.234         {"mlm": 1.2341549295774648, "mse": 0.0, "train": 0.0}  None
0         train   80100/102467 451:25/126:03      1.358         {"mlm": 1.3232342147827147, "mse": 0.0}  0.4028
0         train   80200/102467 451:57/125:29      1.358         {"mlm": 1.3275088781118394, "mse": 0.0}  0.3805
0         train   80300/102467 452:30/124:54      1.358         {"mlm": 1.3351457035541534, "mse": 0.0}  0.3514
0         train   80400/102467 453:02/124:20      1.358         {"mlm": 1.334106293618679, "mse": 0.0}  0.448
0         train   80500/102467 453:35/123:46      1.358         {"mlm": 1.3319788649082185, "mse": 0.0}  0.3906
0         train   80600/102467 454:07/123:12      1.358         {"mlm": 1.3284259667992593, "mse": 0.0}  0.3836
0         train   80700/102467 454:40/122:38      1.358         {"mlm": 1.3275388452836445, "mse": 0.0}  0.4019
0         train   80800/102467 455:12/122:04      1.358         {"mlm": 1.3276551500707865, "mse": 0.0}  0.3778
0         train   80900/102467 455:45/121:29      1.358         {"mlm": 1.3272362316317028, "mse": 0.0}  0.3853
0         train   81000/102467 456:17/120:55      1.358         {"mlm": 1.3259245360493659, "mse": 0.0}  0.3717
0         train   81100/102467 456:50/120:21      1.358         {"mlm": 1.3267409558729693, "mse": 0.0}  0.5087
0         train   81200/102467 457:22/119:47      1.358         {"mlm": 1.3294680247704187, "mse": 0.0}  0.3831
0         train   81300/102467 457:55/119:13      1.358         {"mlm": 1.3298775175443063, "mse": 0.0}  0.3794
0         train   81400/102467 458:27/118:39      1.358         {"mlm": 1.3295503222090856, "mse": 0.0}  0.4067
0         train   81500/102467 459:00/118:05      1.358         {"mlm": 1.3293482565085093, "mse": 0.0}  0.418
0         train   81600/102467 459:32/117:30      1.358         {"mlm": 1.3302548559010028, "mse": 0.0}  0.5062
0         train   81700/102467 460:05/116:56      1.358         {"mlm": 1.3314334492122426, "mse": 0.0}  0.3685
0         train   81800/102467 460:37/116:22      1.358         {"mlm": 1.3326255775491396, "mse": 0.0}  0.3634
0         train   81900/102467 461:10/115:48      1.358         {"mlm": 1.331377280542725, "mse": 0.0}  0.349
0         train   82000/102467 461:42/115:14      1.358         {"mlm": 1.3305648539960384, "mse": 0.0}  0.427
0         train   82100/102467 462:15/114:40      1.358         {"mlm": 1.363093052247558, "mse": 0.0}  0.3763
0         train   82200/102467 462:47/114:06      1.358         {"mlm": 1.3546275140053063, "mse": 0.0}  0.3884
0         train   82300/102467 463:20/113:32      1.358         {"mlm": 1.3516437578759464, "mse": 0.0}  0.3726
0         train   82400/102467 463:52/112:58      1.358         {"mlm": 1.3443458270010793, "mse": 0.0}  0.3655
0         train   82500/102467 464:25/112:24      1.358         {"mlm": 1.3430785928556102, "mse": 0.0}  0.4126
0         train   82600/102467 464:57/111:49      1.358         {"mlm": 1.3418152928352356, "mse": 0.0}  0.4008
0         train   82700/102467 465:30/111:15      1.357         {"mlm": 1.3387643118443577, "mse": 0.0}  0.4344
0         train   82800/102467 466:02/110:41      1.357         {"mlm": 1.3366763489565652, "mse": 0.0}  0.3908
0         train   82900/102467 466:35/110:07      1.357         {"mlm": 1.3355950587848668, "mse": 0.0}  0.4175
0         train   83000/102467 467:07/109:33      1.357         {"mlm": 1.332176431461617, "mse": 0.0}  0.3391
0         train   83100/102467 467:40/108:59      1.357         {"mlm": 1.334523712536549, "mse": 0.0}  0.4962
0         train   83200/102467 468:12/108:25      1.357         {"mlm": 1.3357943560403023, "mse": 0.0}  0.4251
0         train   83300/102467 468:45/107:51      1.357         {"mlm": 1.3375834977746102, "mse": 0.0}  0.3972
0         train   83400/102467 469:17/107:17      1.357         {"mlm": 1.3398622411161427, "mse": 0.0}  0.3617
0         train   83500/102467 469:50/106:43      1.357         {"mlm": 1.3389828895790883, "mse": 0.0}  0.4397
0         train   83600/102467 470:22/106:09      1.357         {"mlm": 1.3386814811961811, "mse": 0.0}  0.4254
0         train   83700/102467 470:55/105:35      1.357         {"mlm": 1.3374133190033786, "mse": 0.0}  0.3851
0         train   83800/102467 471:27/105:01      1.357         {"mlm": 1.3374236221112032, "mse": 0.0}  0.3994
0         train   83900/102467 472:00/104:27      1.357         {"mlm": 1.336919330495982, "mse": 0.0}  0.4101
0         train   84000/102467 472:32/103:53      1.357         {"mlm": 1.3356538676750427, "mse": 0.0}  0.3716
0         train   84100/102467 473:05/103:19      1.357         {"mlm": 1.3534133008548193, "mse": 0.0}  0.422
0         train   84200/102467 473:37/102:45      1.357         {"mlm": 1.3397246827982894, "mse": 0.0}  0.3683
0         train   84300/102467 474:09/102:11      1.357         {"mlm": 1.3421745316294216, "mse": 0.0}  0.3821
0         train   84400/102467 474:42/101:37      1.357         {"mlm": 1.3406284923229985, "mse": 0.0}  0.3697
0         train   84500/102467 475:14/101:03      1.357         {"mlm": 1.3380332568802509, "mse": 0.0}  0.3829
0         train   84600/102467 475:47/100:29      1.357         {"mlm": 1.337742219302168, "mse": 0.0}  0.3564
0         train   84700/102467 476:19/99:55       1.357         {"mlm": 1.334984575182114, "mse": 0.0}  0.3804
0         train   84800/102467 476:52/99:20       1.357         {"mlm": 1.3318694554325332, "mse": 0.0}  0.3706
0         train   84900/102467 477:24/98:47       1.357         {"mlm": 1.329689973645858, "mse": 0.0}  0.3598
0         train   85000/102467 477:57/98:13       1.357         {"mlm": 1.3307220816373346, "mse": 0.0}  0.3824
0         train   85100/102467 478:29/97:39       1.357         {"mlm": 1.3303535588777782, "mse": 0.0}  0.465
0         train   85200/102467 479:02/97:05       1.357         {"mlm": 1.3324694575272338, "mse": 0.0}  0.3761
0         train   85300/102467 479:34/96:31       1.357         {"mlm": 1.3327444752302302, "mse": 0.0}  0.371
0         train   85400/102467 480:07/95:57       1.357         {"mlm": 1.3314436062296402, "mse": 0.0}  0.5109
0         train   85500/102467 480:39/95:23       1.357         {"mlm": 1.3301299545013698, "mse": 0.0}  0.393
0         train   85600/102467 481:12/94:49       1.357         {"mlm": 1.329272731485892, "mse": 0.0}  0.3827
0         train   85700/102467 481:44/94:15       1.357         {"mlm": 1.3294730575050988, "mse": 0.0}  0.3843
0         train   85800/102467 482:17/93:41       1.357         {"mlm": 1.329626225921283, "mse": 0.0}  0.3843
0         train   85900/102467 482:49/93:07       1.357         {"mlm": 1.3297999669239318, "mse": 0.0}  0.4364
0         train   86000/102467 483:22/92:33       1.356         {"mlm": 1.330082958793497, "mse": 0.0}  0.3501
0         train   86100/102467 483:54/91:59       1.356         {"mlm": 1.3610444916892297, "mse": 0.0}  0.3834
0         train   86200/102467 484:27/91:25       1.356         {"mlm": 1.3446440012926983, "mse": 0.0}  0.393
0         train   86300/102467 484:59/90:51       1.356         {"mlm": 1.3401804100784789, "mse": 0.0}  0.3804
0         train   86400/102467 485:32/90:17       1.356         {"mlm": 1.3367384807288796, "mse": 0.0}  0.368
0         train   86500/102467 486:04/89:43       1.356         {"mlm": 1.3323186685141903, "mse": 0.0}  0.352
0         train   86600/102467 486:37/89:09       1.356         {"mlm": 1.3325709237325531, "mse": 0.0}  0.3589
0         train   86700/102467 487:09/88:35       1.356         {"mlm": 1.3325911945044737, "mse": 0.0}  0.3837
0         train   86800/102467 487:42/88:01       1.356         {"mlm": 1.33598359398441, "mse": 0.0}  0.3802
0         train   86900/102467 488:14/87:27       1.356         {"mlm": 1.3330507936950777, "mse": 0.0}  0.3841
0         train   87000/102467 488:47/86:53       1.356         {"mlm": 1.3319874267759868, "mse": 0.0}  0.3814
0         train   87100/102467 489:19/86:19       1.356         {"mlm": 1.3346366641623602, "mse": 0.0}  0.3754
0         train   87200/102467 489:51/85:45       1.356         {"mlm": 1.3328417607815741, "mse": 0.0}  0.3622
0         train   87300/102467 490:24/85:12       1.356         {"mlm": 1.333570851531503, "mse": 0.0}  0.3836
0         train   87400/102467 490:56/84:38       1.356         {"mlm": 1.332464739008298, "mse": 0.0}  0.3918
0         train   87500/102467 491:29/84:04       1.356         {"mlm": 1.33204950622184, "mse": 0.0}  0.3683
0         train   87600/102467 492:01/83:30       1.356         {"mlm": 1.3315432529264342, "mse": 0.0}  0.3706
0         train   87700/102467 492:34/82:56       1.356         {"mlm": 1.3330897989171915, "mse": 0.0}  0.3739
0         train   87800/102467 493:06/82:22       1.356         {"mlm": 1.3305963406512389, "mse": 0.0}  0.3656
0         train   87900/102467 493:39/81:48       1.356         {"mlm": 1.3300709155401431, "mse": 0.0}  0.3863
0         train   88000/102467 494:11/81:14       1.356         {"mlm": 1.3300481061369525, "mse": 0.0}  0.3765
0         train   88100/102467 494:44/80:40       1.356         {"mlm": 1.3229455091059208, "mse": 0.0}  0.4207
0         train   88200/102467 495:16/80:06       1.356         {"mlm": 1.3202112754996942, "mse": 0.0}  0.387
0         train   88300/102467 495:49/79:33       1.356         {"mlm": 1.3351270685324799, "mse": 0.0}  0.3988
0         train   88400/102467 496:21/78:59       1.356         {"mlm": 1.3272437632384926, "mse": 0.0}  0.358
0         train   88500/102467 496:54/78:25       1.356         {"mlm": 1.3267452453653659, "mse": 0.0}  0.417
0         train   88600/102467 497:26/77:51       1.356         {"mlm": 1.326754785824142, "mse": 0.0}  0.3719
0         train   88700/102467 497:59/77:17       1.356         {"mlm": 1.3305429179092934, "mse": 0.0}  0.3665
0         train   88800/102467 498:31/76:43       1.356         {"mlm": 1.332106400674312, "mse": 0.0}  0.3637
0         train   88900/102467 499:04/76:09       1.356         {"mlm": 1.3319832469070596, "mse": 0.0}  0.3949
0         train   89000/102467 499:36/75:35       1.356         {"mlm": 1.3287095635171875, "mse": 0.0}  0.3567
0         train   89100/102467 500:09/75:02       1.356         {"mlm": 1.3288936682426147, "mse": 0.0}  0.3678
0         train   89200/102467 500:41/74:28       1.356         {"mlm": 1.3272933959462572, "mse": 0.0}  0.3764
0         train   89300/102467 501:13/73:54       1.355         {"mlm": 1.3262374293932944, "mse": 0.0}  0.3606
0         train   89400/102467 501:46/73:20       1.355         {"mlm": 1.3275062359421164, "mse": 0.0}  0.3671
0         train   89500/102467 502:18/72:46       1.355         {"mlm": 1.327640239209096, "mse": 0.0}  0.3711
0         train   89600/102467 502:51/72:12       1.355         {"mlm": 1.3283817682573968, "mse": 0.0}  0.3804
0         train   89700/102467 503:23/71:38       1.355         {"mlm": 1.3285846361222695, "mse": 0.0}  0.4033
0         train   89800/102467 503:56/71:05       1.355         {"mlm": 1.328237717328199, "mse": 0.0}  0.381
0         train   89900/102467 504:28/70:31       1.355         {"mlm": 1.3290374498767188, "mse": 0.0}  0.4154
0         train   90000/102467 505:01/69:57       1.355         {"mlm": 1.3286768937098956, "mse": 0.0}  0.3999

10/05/2022 13:13:53 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter-64/0-step90000.pkl
0         valid   1/781        0:10/137:45        0.905           None
0         valid   101/781      0:25/ 2:50         1.236           None
0         valid   201/781      0:40/ 1:55         1.244           None
0         valid   301/781      0:54/ 1:27         1.232           None
0         valid   401/781      1:09/ 1:06         1.234           None
0         valid   501/781      1:24/ 0:47         1.233           None
0         valid   601/781      1:39/ 0:29         1.232           None
0         valid   701/781      1:54/ 0:13         1.237           None
0         valid   781/781      2:09/ 0:00         1.233         {"mlm": 1.232394366197183, "mse": 0.0, "train": 0.0}  None
0         train   90100/102467 507:43/69:41       1.355         {"mlm": 1.3297709995508193, "mse": 0.0}  0.4005
0         train   90200/102467 508:15/69:07       1.355         {"mlm": 1.336771153509617, "mse": 0.0}  0.3974
0         train   90300/102467 508:48/68:33       1.355         {"mlm": 1.334061210155487, "mse": 0.0}  0.5288
0         train   90400/102467 509:21/67:59       1.355         {"mlm": 1.333051886409521, "mse": 0.0}  0.4034
0         train   90500/102467 509:53/67:25       1.355         {"mlm": 1.3296683547496795, "mse": 0.0}  0.3652
0         train   90600/102467 510:25/66:51       1.355         {"mlm": 1.3339926773309707, "mse": 0.0}  0.3774
0         train   90700/102467 510:58/66:17       1.355         {"mlm": 1.3331079609053476, "mse": 0.0}  0.3375
0         train   90800/102467 511:30/65:43       1.355         {"mlm": 1.3308935194462537, "mse": 0.0}  0.3798
0         train   90900/102467 512:03/65:09       1.355         {"mlm": 1.331654391421212, "mse": 0.0}  0.6591
0         train   91000/102467 512:35/64:35       1.355         {"mlm": 1.3315782054066658, "mse": 0.0}  0.3929
0         train   91100/102467 513:08/64:01       1.355         {"mlm": 1.3305956463922153, "mse": 0.0}  0.3788
0         train   91200/102467 513:40/63:27       1.355         {"mlm": 1.3265105958779653, "mse": 0.0}  0.3914
0         train   91300/102467 514:13/62:53       1.355         {"mlm": 1.3258863239104932, "mse": 0.0}  0.3587
0         train   91400/102467 514:45/62:19       1.355         {"mlm": 1.3281349867582322, "mse": 0.0}  0.3888
0         train   91500/102467 515:18/61:45       1.355         {"mlm": 1.330274163365364, "mse": 0.0}  0.3495
0         train   91600/102467 515:50/61:11       1.355         {"mlm": 1.3312814012914895, "mse": 0.0}  0.3576
0         train   91700/102467 516:23/60:37       1.355         {"mlm": 1.330426926051869, "mse": 0.0}  0.3402
0         train   91800/102467 516:55/60:03       1.355         {"mlm": 1.329961687988705, "mse": 0.0}  0.4096
0         train   91900/102467 517:28/59:30       1.355         {"mlm": 1.3309492369701987, "mse": 0.0}  0.3958
0         train   92000/102467 518:00/58:56       1.355         {"mlm": 1.3302036439180374, "mse": 0.0}  0.3738
0         train   92100/102467 518:32/58:22       1.355         {"mlm": 1.3395897168101687, "mse": 0.0}  0.3854
0         train   92200/102467 519:05/57:48       1.355         {"mlm": 1.3344530415295357, "mse": 0.0}  0.4368
0         train   92300/102467 519:37/57:14       1.355         {"mlm": 1.3225690774295642, "mse": 0.0}  0.3864
0         train   92400/102467 520:10/56:40       1.355         {"mlm": 1.3334610526424302, "mse": 0.0}  0.3659
0         train   92500/102467 520:42/56:06       1.355         {"mlm": 1.335571934082704, "mse": 0.0}  1.8138
0         train   92600/102467 521:15/55:32       1.355         {"mlm": 1.3331071942397867, "mse": 0.0}  0.3682
0         train   92700/102467 521:47/54:58       1.355         {"mlm": 1.3333180631179156, "mse": 0.0}  0.3788
0         train   92800/102467 522:20/54:24       1.355         {"mlm": 1.3338863449788958, "mse": 0.0}  0.3786
0         train   92900/102467 522:52/53:50       1.355         {"mlm": 1.3335376666968604, "mse": 0.0}  0.3602
0         train   93000/102467 523:25/53:16       1.355         {"mlm": 1.3321691752076745, "mse": 0.0}  0.3904
0         train   93100/102467 523:57/52:43       1.354         {"mlm": 1.3312257822260627, "mse": 0.0}  0.3606
0         train   93200/102467 524:30/52:09       1.354         {"mlm": 1.3275856352826771, "mse": 0.0}  0.3572
0         train   93300/102467 525:02/51:35       1.354         {"mlm": 1.3293495402141569, "mse": 0.0}  0.3686
0         train   93400/102467 525:35/51:01       1.354         {"mlm": 1.3286976204589915, "mse": 0.0}  0.4374
0         train   93500/102467 526:07/50:27       1.354         {"mlm": 1.329994555351494, "mse": 0.0}  0.4396
0         train   93600/102467 526:40/49:53       1.354         {"mlm": 1.3295978936573503, "mse": 0.0}  0.3767
0         train   93700/102467 527:12/49:19       1.354         {"mlm": 1.3295734664353993, "mse": 0.0}  0.3544
0         train   93800/102467 527:45/48:45       1.354         {"mlm": 1.3293413834614247, "mse": 0.0}  0.3698
0         train   93900/102467 528:17/48:11       1.354         {"mlm": 1.3302991456769528, "mse": 0.0}  0.394
0         train   94000/102467 528:50/47:38       1.354         {"mlm": 1.3309898547913923, "mse": 0.0}  0.3709
0         train   94100/102467 529:22/47:04       1.354         {"mlm": 1.342369726725987, "mse": 0.0}  0.377
0         train   94200/102467 529:55/46:30       1.354         {"mlm": 1.3335682704593197, "mse": 0.0}  0.4268
0         train   94300/102467 530:27/45:56       1.354         {"mlm": 1.3332538034691908, "mse": 0.0}  0.3491
0         train   94400/102467 531:00/45:22       1.354         {"mlm": 1.317220327243134, "mse": 0.0}  0.388
0         train   94500/102467 531:32/44:48       1.354         {"mlm": 1.3188554146682403, "mse": 0.0}  0.3653
0         train   94600/102467 532:04/44:14       1.354         {"mlm": 1.3212271332740784, "mse": 0.0}  0.395
0         train   94700/102467 532:37/43:41       1.354         {"mlm": 1.32100616297271, "mse": 0.0}  0.4313
0         train   94800/102467 533:09/43:07       1.354         {"mlm": 1.3206868328546222, "mse": 0.0}  0.3905
0         train   94900/102467 533:42/42:33       1.354         {"mlm": 1.3199438462013127, "mse": 0.0}  0.4082
0         train   95000/102467 534:14/41:59       1.354         {"mlm": 1.3215949746793163, "mse": 0.0}  0.3785
0         train   95100/102467 534:47/41:25       1.354         {"mlm": 1.3192050369059454, "mse": 0.0}  0.3672
0         train   95200/102467 535:19/40:51       1.354         {"mlm": 1.3192508387346698, "mse": 0.0}  0.3826
0         train   95300/102467 535:52/40:17       1.354         {"mlm": 1.3209626941908674, "mse": 0.0}  0.5526
0         train   95400/102467 536:24/39:44       1.354         {"mlm": 1.3220239171228012, "mse": 0.0}  0.3948
0         train   95500/102467 536:57/39:10       1.354         {"mlm": 1.3219938563488831, "mse": 0.0}  0.419
0         train   95600/102467 537:29/38:36       1.354         {"mlm": 1.3220469433166209, "mse": 0.0}  0.3629
0         train   95700/102467 538:02/38:02       1.354         {"mlm": 1.3225276389096736, "mse": 0.0}  0.385
0         train   95800/102467 538:34/37:28       1.354         {"mlm": 1.324628840696825, "mse": 0.0}  0.4631
0         train   95900/102467 539:07/36:55       1.354         {"mlm": 1.3230456248852174, "mse": 0.0}  0.3818
0         train   96000/102467 539:39/36:21       1.354         {"mlm": 1.322214505514941, "mse": 0.0}  0.4268
0         train   96100/102467 540:12/35:47       1.354         {"mlm": 1.3183026762352776, "mse": 0.0}  0.3541
0         train   96200/102467 540:44/35:13       1.353         {"mlm": 1.31622258660757, "mse": 0.0}  0.408
0         train   96300/102467 541:17/34:39       1.353         {"mlm": 1.3101447529664345, "mse": 0.0}  0.3929
0         train   96400/102467 541:49/34:06       1.353         {"mlm": 1.3026873599972473, "mse": 0.0}  0.3827
0         train   96500/102467 542:22/33:32       1.353         {"mlm": 1.3032717842691142, "mse": 0.0}  0.3834
0         train   96600/102467 542:54/32:58       1.353         {"mlm": 1.3113170703451837, "mse": 0.0}  0.3736
0         train   96700/102467 543:27/32:24       1.353         {"mlm": 1.312680087010864, "mse": 0.0}  0.3519
0         train   96800/102467 543:59/31:50       1.353         {"mlm": 1.3093690945482912, "mse": 0.0}  0.423
0         train   96900/102467 544:32/31:17       1.353         {"mlm": 1.3119081024873749, "mse": 0.0}  0.3598
0         train   97000/102467 545:04/30:43       1.353         {"mlm": 1.312163102770282, "mse": 0.0}  0.3597
0         train   97100/102467 545:37/30:09       1.353         {"mlm": 1.312796853951356, "mse": 0.0}  0.4009
0         train   97200/102467 546:09/29:35       1.353         {"mlm": 1.3129853239732676, "mse": 0.0}  0.3719
0         train   97300/102467 546:42/29:01       1.353         {"mlm": 1.3137143852624327, "mse": 0.0}  0.3432
0         train   97400/102467 547:14/28:28       1.353         {"mlm": 1.313825219506951, "mse": 0.0}  0.3794
0         train   97500/102467 547:47/27:54       1.353         {"mlm": 1.313943174813856, "mse": 0.0}  0.3896
0         train   97600/102467 548:19/27:20       1.353         {"mlm": 1.3149698259461724, "mse": 0.0}  0.4102
0         train   97700/102467 548:52/26:46       1.353         {"mlm": 1.3144389253612399, "mse": 0.0}  0.3707
0         train   97800/102467 549:24/26:13       1.353         {"mlm": 1.3154137764430802, "mse": 0.0}  0.4019
0         train   97900/102467 549:57/25:39       1.353         {"mlm": 1.3176638196503796, "mse": 0.0}  0.3914
0         train   98000/102467 550:29/25:05       1.353         {"mlm": 1.3179532903459708, "mse": 0.0}  0.391
0         train   98100/102467 551:01/24:31       1.353         {"mlm": 1.340687761704127, "mse": 0.0}  0.3842
0         train   98200/102467 551:34/23:58       1.353         {"mlm": 1.333994053456248, "mse": 0.0}  0.3881
0         train   98300/102467 552:06/23:24       1.353         {"mlm": 1.3278835576933783, "mse": 0.0}  0.3741
0         train   98400/102467 552:39/22:50       1.353         {"mlm": 1.335612947290594, "mse": 0.0}  0.3846
0         train   98500/102467 553:11/22:16       1.353         {"mlm": 1.3312645386303625, "mse": 0.0}  0.3788
0         train   98600/102467 553:44/21:43       1.353         {"mlm": 1.33095171687587, "mse": 0.0}  0.3822
0         train   98700/102467 554:16/21:09       1.353         {"mlm": 1.3285932456967475, "mse": 0.0}  0.4023
0         train   98800/102467 554:49/20:35       1.353         {"mlm": 1.3281815114183042, "mse": 0.0}  0.368
0         train   98900/102467 555:21/20:01       1.353         {"mlm": 1.3270013081575078, "mse": 0.0}  0.3932
0         train   99000/102467 555:54/19:28       1.353         {"mlm": 1.3261458280694534, "mse": 0.0}  0.3996
0         train   99100/102467 556:27/18:54       1.353         {"mlm": 1.325700034027117, "mse": 0.0}  0.3714
0         train   99200/102467 556:59/18:20       1.353         {"mlm": 1.3252142332369667, "mse": 0.0}  0.3867
0         train   99300/102467 557:32/17:46       1.352         {"mlm": 1.3257414877506686, "mse": 0.0}  0.3809
0         train   99400/102467 558:04/17:13       1.352         {"mlm": 1.3252212850958707, "mse": 0.0}  0.3644
0         train   99500/102467 558:37/16:39       1.352         {"mlm": 1.3241256451543002, "mse": 0.0}  0.3446
0         train   99600/102467 559:09/16:05       1.352         {"mlm": 1.3238466615440851, "mse": 0.0}  0.368
0         train   99700/102467 559:42/15:32       1.352         {"mlm": 1.3238350861828845, "mse": 0.0}  0.3726
0         train   99800/102467 560:14/14:58       1.352         {"mlm": 1.3252068670064145, "mse": 0.0}  0.3505
0         train   99900/102467 560:47/14:24       1.352         {"mlm": 1.3244436118233054, "mse": 0.0}  0.4287
0         train   100000/102467 561:19/13:50      1.352         {"mlm": 1.3239396944612205, "mse": 0.0}  0.3799

10/05/2022 14:10:12 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter-64/0-step100000.pkl
0         valid   1/781        0:10/135:10        1.197           None
0         valid   101/781      0:25/ 2:49         1.232           None
0         valid   201/781      0:40/ 1:55         1.251           None
0         valid   301/781      0:54/ 1:27         1.240           None
0         valid   401/781      1:09/ 1:06         1.236           None
0         valid   501/781      1:24/ 0:47         1.239           None
0         valid   601/781      1:39/ 0:29         1.235           None
0         valid   701/781      1:54/ 0:13         1.238           None
0         valid   781/781      2:09/ 0:00         1.231         {"mlm": 1.2309539052496798, "mse": 0.0, "train": 0.0}  None
0         train   100100/102467 564:02/13:20      1.352         {"mlm": 1.366535745859146, "mse": 0.0}  0.4828
