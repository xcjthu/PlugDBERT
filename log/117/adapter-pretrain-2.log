/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
/data3/private/xcj/DomainPlugin/DomainPlugin
None
read config from config/pre-train/adapter.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
/data3/private/xcj/DomainPlugin/DomainPlugin
None
read config from config/pre-train/adapter.config
/data3/private/xcj/DomainPlugin/DomainPlugin
None
read config from config/pre-train/adapter.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
/data3/private/xcj/DomainPlugin/DomainPlugin
read config from config/pre-train/adapter.config
None
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 6
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 5
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 7
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 4
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/04/2022 14:26:14 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
10/04/2022 14:26:14 - INFO - __main__ -   CUDA available: True
10/04/2022 14:26:14 - INFO - tools.init_tool -   Begin to initialize dataset and formatter...
formatter roberta-base
10/04/2022 14:26:14 - INFO - __main__ -   CUDA available: True
10/04/2022 14:26:14 - INFO - __main__ -   CUDA available: True
10/04/2022 14:26:14 - INFO - __main__ -   CUDA available: True
10/04/2022 14:26:14 - INFO - __main__ -   CUDA available: True
10/04/2022 14:26:14 - INFO - __main__ -   CUDA available: True
10/04/2022 14:26:14 - INFO - __main__ -   CUDA available: True
10/04/2022 14:26:14 - INFO - __main__ -   CUDA available: True
formatterformatterformatter  roberta-base roberta-baseroberta-base


formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
formatter roberta-base
10/04/2022 14:26:23 - INFO - tools.init_tool -   Begin to initialize models...
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-04 14:26:28,001 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-10-04 14:26:28,001 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-10-04 14:26:28,001 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-04 14:26:28,267 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-10-04 14:26:28,267 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-10-04 14:26:28,267 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|(OpenDelta)basemodel:675]2022-10-04 14:26:28,273 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-10-04 14:26:28,273 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-10-04 14:26:28,273 >> Static Memory 0.00 GB, Max Memory 0.00 GB
all parameters are turned
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-04 14:26:28,466 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-10-04 14:26:28,466 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-10-04 14:26:28,466 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-04 14:26:30,151 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-10-04 14:26:30,152 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-10-04 14:26:30,152 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-04 14:26:30,436 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-10-04 14:26:30,437 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-10-04 14:26:30,437 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-04 14:26:30,452 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-10-04 14:26:30,452 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-10-04 14:26:30,452 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
root
├── roberta (RobertaModel)
│   ├── embeddings (RobertaEmbeddings)
│   │   ├── word_embeddings (Embedding) weight:[50265, 768]
│   │   ├── position_embeddings (Embedding) weight:[514, 768]
│   │   ├── token_type_embeddings (Embedding) weight:[1, 768]
│   │   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│   └── encoder (RobertaEncoder)
│       └── layer (ModuleList)
│           └── 0-11(RobertaLayer)
│               ├── attention (RobertaAttention)
│               │   ├── self (RobertaSelfAttention)
│               │   │   └── query,key,value(Linear) weight:[768, 768] bias:[768]
│               │   └── output (RobertaSelfOutput)
│               │       ├── dense (Linear) weight:[768, 768] bias:[768]
│               │       └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│               │           └── adapter (AdapterLayer)
│               │               └── modulelist (Sequential)
│               │                   ├── down_proj (Linear) weight:[256, 768] bias:[256]
│               │                   └── up_proj (Linear) weight:[768, 256] bias:[768]
│               ├── intermediate (RobertaIntermediate)
│               │   └── dense (Linear) weight:[3072, 768] bias:[3072]
│               └── output (RobertaOutput)
│                   ├── dense (Linear) weight:[768, 3072] bias:[768]
│                   └── LayerNorm (LayerNorm) weight:[768] bias:[768]
│                       └── adapter (AdapterLayer)
│                           └── modulelist (Sequential)
│                               ├── down_proj (Linear) weight:[256, 768] bias:[256]
│                               └── up_proj (Linear) weight:[768, 256] bias:[768]
└── lm_head (RobertaLMHead) bias:[50265]
    ├── dense (Linear) weight:[768, 768] bias:[768]
    ├── layer_norm (LayerNorm) weight:[768] bias:[768]
    └── decoder (Linear) weight:[50265, 768] bias:[50265]
[INFO|(OpenDelta)basemodel:675]2022-10-04 14:26:30,946 >> Trainable Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:677]2022-10-04 14:26:30,946 >> Delta Parameter Ratio: 7.052636%
[INFO|(OpenDelta)basemodel:679]2022-10-04 14:26:30,946 >> Static Memory 0.00 GB, Max Memory 0.00 GB
/data/disk1/private/xcj/env/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
odict_keys(['roberta.encoder.layer.0.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.0.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.0.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.0.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.0.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.1.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.1.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.1.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.1.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.2.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.2.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.2.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.2.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.3.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.3.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.3.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.3.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.4.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.4.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.4.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.4.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.5.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.5.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.5.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.5.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.6.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.6.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.6.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.6.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.7.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.7.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.7.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.7.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.8.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.8.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.8.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.8.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.9.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.9.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.9.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.9.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.10.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.10.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.10.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.10.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.adapter.modulelist.up_proj.bias', 'roberta.encoder.layer.11.output.LayerNorm.adapter.modulelist.down_proj.weight', 'roberta.encoder.layer.11.output.LayerNorm.adapter.modulelist.down_proj.bias', 'roberta.encoder.layer.11.output.LayerNorm.adapter.modulelist.up_proj.weight', 'roberta.encoder.layer.11.output.LayerNorm.adapter.modulelist.up_proj.bias'])
10/04/2022 14:26:36 - INFO - tools.init_tool -   Begin to load checkpoint... from None
10/04/2022 14:26:36 - WARNING - tools.init_tool -   Cannot load checkpoint file with error 'NoneType' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.
10/04/2022 14:26:36 - INFO - tools.init_tool -   Initialize done.
[train]
epoch: 20
batch_size: 8
shuffle: True
reader_num: 4
optimizer: AdamW
learning_rate: 1e-3
weight_decay: 1e-5
step_size: 1
lr_multiplier: 1
doc_len: 512
warmup_steps: 20000
training_steps: 200000
max_grad_norm: 1.0
fp16: False
mlm_prob: 0.15
valid_mode: step
step_epoch: 5000
bottleneck_dim: 256
grad_accumulate: 2
========
[eval]
batch_size: 8
shuffle: False
reader_num: 4
========
[distributed]
use: True
backend: nccl
local_rank: 0
gpu_num: 8
========
[data]
train_dataset_type: kara
train_formatter_type: vallina
train_data_path: /data3/private/lirun/biomed/kara
train_kara_namespace: bio
train_kara_dataset: train
train_kara_version: latest
valid_dataset_type: kara
valid_formatter_type: vallina
valid_data_path: /data3/private/lirun/biomed/kara
valid_kara_namespace: bio
valid_kara_dataset: valid
valid_kara_version: latest
========
[model]
model_name: vallina_delta
pretrained_model: roberta-base
domain_plugin_path: None
========
[output]
output_time: 100
test_time: 1
model_path: checkpoints/
model_name: BERT-Adapter
output_function: avgloss
load_from_path: False
========
10/04/2022 14:26:36 - WARNING - tools.train_tool -   Output path exists, check whether need to change a name of model
valid_mode step no_valid False
step_epoch 5000
10/04/2022 14:26:36 - INFO - tools.train_tool -   Start training
Epoch  Stage  Iterations  Time Usage    Loss    Output Information
training from epoch: 0 step: 0
10/04/2022 14:26:48 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/04/2022 14:26:48 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/04/2022 14:26:48 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/04/2022 14:26:48 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/04/2022 14:26:48 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/04/2022 14:26:48 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/04/2022 14:26:48 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
10/04/2022 14:26:48 - INFO - root -   Reducer buckets have been rebuilt in this iteration.
0         train   100/102467   0:46/790:14        1.483         {"mlm": 1.4827176451683044, "mse": 0.0}  0.8263
0         train   200/102467   1:21/694:29        1.479         {"mlm": 1.4792643284797669, "mse": 0.0}  0.5358
0         train   300/102467   1:57/664:32        1.481         {"mlm": 1.4811715686321258, "mse": 0.0}  0.5796
0         train   400/102467   2:33/651:02        1.488         {"mlm": 1.4877898508310319, "mse": 0.0}  0.4844
0         train   500/102467   3:09/643:04        1.481         {"mlm": 1.4806157946586609, "mse": 0.0}  0.5457
0         train   600/102467   3:45/637:42        1.478         {"mlm": 1.4784894661108652, "mse": 0.0}  0.5325
0         train   700/102467   4:21/633:49        1.475         {"mlm": 1.4751481832776752, "mse": 0.0}  0.5278
0         train   800/102467   4:57/630:35        1.476         {"mlm": 1.4756753544509411, "mse": 0.0}  0.5456
0         train   900/102467   5:33/627:59        1.472         {"mlm": 1.4721139069398244, "mse": 0.0}  0.5381
0         train   1000/102467  6:10/625:47        1.469         {"mlm": 1.4685865139961243, "mse": 0.0}  0.508
0         train   1100/102467  6:46/623:43        1.466         {"mlm": 1.4658352127942171, "mse": 0.0}  0.5337
0         train   1200/102467  7:22/621:53        1.463         {"mlm": 1.4634324644009273, "mse": 0.0}  0.6189
0         train   1300/102467  7:58/620:19        1.463         {"mlm": 1.463466813656, "mse": 0.0}  0.5129
0         train   1400/102467  8:34/618:53        1.462         {"mlm": 1.4615123284714562, "mse": 0.0}  0.4853
0         train   1500/102467  9:10/617:34        1.461         {"mlm": 1.4605729851722717, "mse": 0.0}  0.4961
0         train   1600/102467  9:46/616:17        1.459         {"mlm": 1.4594912353903056, "mse": 0.0}  0.4965
0         train   1700/102467 10:22/615:06        1.460         {"mlm": 1.4601461387381833, "mse": 0.0}  0.4853
0         train   1800/102467 10:58/613:58        1.458         {"mlm": 1.458181330230501, "mse": 0.0}  0.5182
0         train   1900/102467 11:34/612:55        1.457         {"mlm": 1.4573190851587998, "mse": 0.0}  0.4864
0         train   2000/102467 12:10/612:00        1.457         {"mlm": 1.457211877465248, "mse": 0.0}  0.4604
0         train   2100/102467 12:47/611:01        1.457         {"mlm": 1.441899881218419, "mse": 0.0}  0.4818
0         train   2200/102467 13:23/610:04        1.455         {"mlm": 1.4339184802980278, "mse": 0.0}  0.4876
0         train   2300/102467 13:59/609:07        1.454         {"mlm": 1.4354208241338315, "mse": 0.0}  0.4797
0         train   2400/102467 14:35/608:14        1.454         {"mlm": 1.438978930164997, "mse": 0.0}  0.5375
0         train   2500/102467 15:11/607:27        1.454         {"mlm": 1.4397301212819162, "mse": 0.0}  0.6486
0         train   2600/102467 15:47/606:36        1.452         {"mlm": 1.435571446442644, "mse": 0.0}  0.5092
0         train   2700/102467 16:23/605:48        1.450         {"mlm": 1.4307770077592143, "mse": 0.0}  0.4517
0         train   2800/102467 16:59/605:03        1.450         {"mlm": 1.4314555980087968, "mse": 0.0}  0.4656
0         train   2900/102467 17:36/604:17        1.448         {"mlm": 1.4286225017106307, "mse": 0.0}  0.4463
0         train   3000/102467 18:12/603:33        1.447         {"mlm": 1.4275000048829272, "mse": 0.0}  0.4503
0         train   3100/102467 18:48/602:51        1.446         {"mlm": 1.425772905892085, "mse": 0.0}  0.4613
0         train   3200/102467 19:24/602:08        1.446         {"mlm": 1.4262192908975857, "mse": 0.0}  0.4434
0         train   3300/102467 20:00/601:27        1.444         {"mlm": 1.4238449667764683, "mse": 0.0}  0.4408
0         train   3400/102467 20:37/600:46        1.443         {"mlm": 1.423107250789986, "mse": 0.0}  0.4266
0         train   3500/102467 21:13/600:06        1.442         {"mlm": 1.421292057309332, "mse": 0.0}  0.4149
0         train   3600/102467 21:49/599:27        1.441         {"mlm": 1.420618968393148, "mse": 0.0}  0.4543
0         train   3700/102467 22:25/598:49        1.440         {"mlm": 1.4186084347307295, "mse": 0.0}  0.5786
0         train   3800/102467 23:02/598:08        1.438         {"mlm": 1.4175201086881362, "mse": 0.0}  0.5563
0         train   3900/102467 23:38/597:30        1.438         {"mlm": 1.4180132323908392, "mse": 0.0}  0.4562
0         train   4000/102467 24:14/596:50        1.437         {"mlm": 1.4173675589468433, "mse": 0.0}  0.4534
0         train   4100/102467 24:50/596:10        1.437         {"mlm": 1.4145314304196104, "mse": 0.0}  0.4572
0         train   4200/102467 25:27/595:29        1.436         {"mlm": 1.40813378312371, "mse": 0.0}  0.4673
0         train   4300/102467 26:03/594:49        1.435         {"mlm": 1.4068989957739042, "mse": 0.0}  0.4389
0         train   4400/102467 26:39/594:10        1.434         {"mlm": 1.3995177977947733, "mse": 0.0}  0.4332
0         train   4500/102467 27:15/593:29        1.433         {"mlm": 1.3987539669117295, "mse": 0.0}  0.4127
0         train   4600/102467 27:51/592:49        1.432         {"mlm": 1.3975750267704992, "mse": 0.0}  0.4577
0         train   4700/102467 28:28/592:11        1.431         {"mlm": 1.39695911233268, "mse": 0.0}  0.419
0         train   4800/102467 29:04/591:33        1.431         {"mlm": 1.3995462887567984, "mse": 0.0}  0.4659
0         train   4900/102467 29:40/590:56        1.430         {"mlm": 1.3990230244358293, "mse": 0.0}  0.433
0         train   5000/102467 30:16/590:17        1.430         {"mlm": 1.3986738866460109, "mse": 0.0}  0.4021
0         train   5100/102467 30:53/589:38        1.429         {"mlm": 1.39980215724483, "mse": 0.0}  0.4232
0         train   5200/102467 31:29/589:00        1.429         {"mlm": 1.4010361436014382, "mse": 0.0}  0.428
0         train   5300/102467 32:05/588:22        1.429         {"mlm": 1.401650980275658, "mse": 0.0}  0.4224
0         train   5400/102467 32:41/587:44        1.428         {"mlm": 1.4005379588136686, "mse": 0.0}  0.4227
0         train   5500/102467 33:18/587:06        1.427         {"mlm": 1.3994132800000372, "mse": 0.0}  0.4293
0         train   5600/102467 33:54/586:29        1.427         {"mlm": 1.400069886699636, "mse": 0.0}  0.5091
0         train   5700/102467 34:30/585:52        1.426         {"mlm": 1.3995373814911949, "mse": 0.0}  0.4393
0         train   5800/102467 35:06/585:15        1.426         {"mlm": 1.399541648595829, "mse": 0.0}  0.4288
0         train   5900/102467 35:43/584:37        1.425         {"mlm": 1.3988370691888325, "mse": 0.0}  0.4495
0         train   6000/102467 36:19/584:00        1.425         {"mlm": 1.3992464698709406, "mse": 0.0}  0.4229
0         train   6100/102467 36:55/583:23        1.424         {"mlm": 1.382175361372761, "mse": 0.0}  0.4474
0         train   6200/102467 37:31/582:45        1.423         {"mlm": 1.3838073671771791, "mse": 0.0}  0.4582
0         train   6300/102467 38:08/582:08        1.423         {"mlm": 1.3914226799300222, "mse": 0.0}  0.4485
0         train   6400/102467 38:44/581:32        1.423         {"mlm": 1.3938290132983806, "mse": 0.0}  0.4577
0         train   6500/102467 39:20/580:55        1.422         {"mlm": 1.3973295116568476, "mse": 0.0}  0.4559
0         train   6600/102467 39:57/580:19        1.422         {"mlm": 1.3995765877928166, "mse": 0.0}  0.5038
0         train   6700/102467 40:33/579:43        1.422         {"mlm": 1.3963664080012306, "mse": 0.0}  0.4353
0         train   6800/102467 41:09/579:07        1.420         {"mlm": 1.3888511178334952, "mse": 0.0}  0.4941
0         train   6900/102467 41:46/578:31        1.420         {"mlm": 1.3864383866954408, "mse": 0.0}  0.4461
0         train   7000/102467 42:22/577:54        1.419         {"mlm": 1.387407364135042, "mse": 0.0}  0.466
0         train   7100/102467 42:58/577:18        1.419         {"mlm": 1.387392149638782, "mse": 0.0}  0.4816
0         train   7200/102467 43:35/576:40        1.418         {"mlm": 1.3869668712890835, "mse": 0.0}  0.4519
0         train   7300/102467 44:11/576:02        1.418         {"mlm": 1.3860448286821958, "mse": 0.0}  0.446
0         train   7400/102467 44:47/575:25        1.417         {"mlm": 1.385301800310057, "mse": 0.0}  0.448
0         train   7500/102467 45:23/574:49        1.417         {"mlm": 1.384276358939523, "mse": 0.0}  0.905
0         train   7600/102467 46:00/574:12        1.416         {"mlm": 1.383181519521798, "mse": 0.0}  0.4952
0         train   7700/102467 46:36/573:35        1.415         {"mlm": 1.3815490360392075, "mse": 0.0}  0.4681
0         train   7800/102467 47:12/572:59        1.415         {"mlm": 1.381535071595616, "mse": 0.0}  0.444
0         train   7900/102467 47:48/572:22        1.414         {"mlm": 1.3819183460900704, "mse": 0.0}  0.5126
0         train   8000/102467 48:25/571:46        1.413         {"mlm": 1.3798139622107588, "mse": 0.0}  0.4503
0         train   8100/102467 49:01/571:09        1.413         {"mlm": 1.3567228522151709, "mse": 0.0}  0.4784
0         train   8200/102467 49:37/570:33        1.412         {"mlm": 1.3711948379570125, "mse": 0.0}  0.4996
0         train   8300/102467 50:14/569:56        1.412         {"mlm": 1.3673925870979153, "mse": 0.0}  0.4629
0         train   8400/102467 50:50/569:19        1.411         {"mlm": 1.3720375468032528, "mse": 0.0}  0.4987
0         train   8500/102467 51:26/568:42        1.411         {"mlm": 1.370364564080392, "mse": 0.0}  0.4837
0         train   8600/102467 52:03/568:06        1.411         {"mlm": 1.3736255232919783, "mse": 0.0}  0.4406
0         train   8700/102467 52:39/567:31        1.410         {"mlm": 1.374926213217878, "mse": 0.0}  0.5539
0         train   8800/102467 53:15/566:55        1.410         {"mlm": 1.3757879110287183, "mse": 0.0}  0.4641
0         train   8900/102467 53:51/566:18        1.410         {"mlm": 1.378506085502782, "mse": 0.0}  0.485
0         train   9000/102467 54:28/565:41        1.409         {"mlm": 1.3776356711325397, "mse": 0.0}  0.482
0         train   9100/102467 55:04/565:05        1.409         {"mlm": 1.3794274457397253, "mse": 0.0}  0.4881
0         train   9200/102467 55:40/564:29        1.409         {"mlm": 1.379084752355531, "mse": 0.0}  0.5514
0         train   9300/102467 56:17/563:53        1.409         {"mlm": 1.379227515677979, "mse": 0.0}  0.452
0         train   9400/102467 56:53/563:17        1.408         {"mlm": 1.3779664453770164, "mse": 0.0}  0.4526
0         train   9500/102467 57:29/562:41        1.408         {"mlm": 1.3767572330998228, "mse": 0.0}  0.5293
0         train   9600/102467 58:06/562:04        1.407         {"mlm": 1.375880269067628, "mse": 0.0}  0.5611
0         train   9700/102467 58:42/561:28        1.407         {"mlm": 1.3751433869858958, "mse": 0.0}  0.5225
0         train   9800/102467 59:18/560:51        1.407         {"mlm": 1.3758754244692342, "mse": 0.0}  0.5203
0         train   9900/102467 59:55/560:15        1.406         {"mlm": 1.3760625593018432, "mse": 0.0}  0.4794
0         train   10000/102467 60:31/559:39       1.406         {"mlm": 1.376389274825314, "mse": 0.0}  0.4983

10/04/2022 15:27:08 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step10000.pkl
0         valid   1/781        0:10/139:45        1.445           None
0         valid   101/781      0:26/ 2:56         1.276           None
0         valid   201/781      0:41/ 2:00         1.288           None
0         valid   301/781      0:57/ 1:31         1.279           None
0         valid   401/781      1:12/ 1:08         1.276           None
0         valid   501/781      1:28/ 0:49         1.276           None
0         valid   601/781      1:43/ 0:31         1.272           None
0         valid   701/781      1:59/ 0:13         1.274           None
0         valid   781/781      2:15/ 0:00         1.272         {"mlm": 1.2719270166453265, "mse": 0.0, "train": 0.0}  None
0         train   10100/102467 63:23/579:47       1.405         {"mlm": 1.3487612718343736, "mse": 0.0}  0.6082
0         train   10200/102467 63:59/578:54       1.405         {"mlm": 1.3576038366556167, "mse": 0.0}  0.4886
0         train   10300/102467 64:35/578:01       1.405         {"mlm": 1.3688124012947083, "mse": 0.0}  0.5069
0         train   10400/102467 65:11/577:06       1.404         {"mlm": 1.361994363963604, "mse": 0.0}  0.4893
0         train   10500/102467 65:47/576:14       1.404         {"mlm": 1.3609986174106599, "mse": 0.0}  0.5161
0         train   10600/102467 66:23/575:21       1.403         {"mlm": 1.3616897306839626, "mse": 0.0}  0.5474
0         train   10700/102467 66:59/574:29       1.403         {"mlm": 1.3607236426217215, "mse": 0.0}  0.4814
0         train   10800/102467 67:35/573:38       1.403         {"mlm": 1.3646491734683515, "mse": 0.0}  0.5141
0         train   10900/102467 68:11/572:47       1.402         {"mlm": 1.3634965731700261, "mse": 0.0}  0.4913
0         train   11000/102467 68:46/571:56       1.402         {"mlm": 1.363774216592312, "mse": 0.0}  1.1988
0         train   11100/102467 69:22/571:06       1.402         {"mlm": 1.3618247749046846, "mse": 0.0}  0.5354
0         train   11200/102467 69:58/570:16       1.402         {"mlm": 1.3640771768987179, "mse": 0.0}  0.4957
0         train   11300/102467 70:34/569:25       1.401         {"mlm": 1.3637974902758232, "mse": 0.0}  0.487
0         train   11400/102467 71:10/568:35       1.401         {"mlm": 1.363690589581217, "mse": 0.0}  0.5251
0         train   11500/102467 71:46/567:45       1.400         {"mlm": 1.362346184571584, "mse": 0.0}  0.5087
0         train   11600/102467 72:22/566:55       1.400         {"mlm": 1.362571718171239, "mse": 0.0}  0.5024
0         train   11700/102467 72:58/566:06       1.400         {"mlm": 1.3616828241769006, "mse": 0.0}  0.4909
0         train   11800/102467 73:34/565:17       1.399         {"mlm": 1.3613174438476563, "mse": 0.0}  0.4994
0         train   11900/102467 74:10/564:28       1.398         {"mlm": 1.3579789262696316, "mse": 0.0}  0.5515
0         train   12000/102467 74:45/563:39       1.398         {"mlm": 1.3580322505235671, "mse": 0.0}  0.4903
0         train   12100/102467 75:21/562:50       1.398         {"mlm": 1.3786640058864246, "mse": 0.0}  0.5431
0         train   12200/102467 75:57/562:02       1.397         {"mlm": 1.3657363512408194, "mse": 0.0}  0.5212
0         train   12300/102467 76:33/561:13       1.397         {"mlm": 1.3614418600315235, "mse": 0.0}  0.5289
0         train   12400/102467 77:09/560:26       1.397         {"mlm": 1.3559047488640423, "mse": 0.0}  0.5743
0         train   12500/102467 77:45/559:39       1.397         {"mlm": 1.3627589014106858, "mse": 0.0}  0.6574
0         train   12600/102467 78:21/558:52       1.396         {"mlm": 1.3614312926795527, "mse": 0.0}  0.5297
0         train   12700/102467 78:57/558:05       1.396         {"mlm": 1.3567401033443784, "mse": 0.0}  0.5147
0         train   12800/102467 79:33/557:18       1.395         {"mlm": 1.3537820983440318, "mse": 0.0}  0.5135
0         train   12900/102467 80:09/556:31       1.395         {"mlm": 1.3538992109898067, "mse": 0.0}  0.507
0         train   13000/102467 80:45/555:44       1.395         {"mlm": 1.3550436577281437, "mse": 0.0}  0.53
0         train   13100/102467 81:21/554:58       1.395         {"mlm": 1.3582591512075222, "mse": 0.0}  0.5295
0         train   13200/102467 81:56/554:11       1.395         {"mlm": 1.3596240348672748, "mse": 0.0}  0.5246
0         train   13300/102467 82:32/553:25       1.394         {"mlm": 1.3588297699762364, "mse": 0.0}  0.6201
0         train   13400/102467 83:08/552:39       1.394         {"mlm": 1.3602658249464437, "mse": 0.0}  0.5084
0         train   13500/102467 83:44/551:54       1.394         {"mlm": 1.3606637360415035, "mse": 0.0}  0.5696
0         train   13600/102467 84:20/551:09       1.394         {"mlm": 1.3611031850104485, "mse": 0.0}  0.5481
0         train   13700/102467 84:56/550:24       1.394         {"mlm": 1.3630269614649633, "mse": 0.0}  0.4917
0         train   13800/102467 85:32/549:39       1.394         {"mlm": 1.364120564472682, "mse": 0.0}  0.5722
0         train   13900/102467 86:08/548:54       1.393         {"mlm": 1.3644337327998837, "mse": 0.0}  0.6929
0         train   14000/102467 86:44/548:10       1.393         {"mlm": 1.3648353152838035, "mse": 0.0}  0.5368
0         train   14100/102467 87:20/547:25       1.393         {"mlm": 1.366277004991259, "mse": 0.0}  0.5172
0         train   14200/102467 87:56/546:41       1.393         {"mlm": 1.347994155956037, "mse": 0.0}  0.5197
0         train   14300/102467 88:32/545:57       1.393         {"mlm": 1.358971341344334, "mse": 0.0}  0.5334
0         train   14400/102467 89:08/545:12       1.392         {"mlm": 1.3603059637486634, "mse": 0.0}  0.5929
0         train   14500/102467 89:44/544:28       1.392         {"mlm": 1.3643033023340156, "mse": 0.0}  0.5529
0         train   14600/102467 90:20/543:44       1.392         {"mlm": 1.3605169968860205, "mse": 0.0}  0.5577
0         train   14700/102467 90:56/543:00       1.392         {"mlm": 1.3591213994859623, "mse": 0.0}  0.5142
0         train   14800/102467 91:32/542:16       1.391         {"mlm": 1.35939778092511, "mse": 0.0}  0.5598
0         train   14900/102467 92:08/541:32       1.391         {"mlm": 1.3545199918720399, "mse": 0.0}  0.5577
0         train   15000/102467 92:44/540:48       1.391         {"mlm": 1.354283481896043, "mse": 0.0}  0.5568
0         train   15100/102467 93:20/540:04       1.390         {"mlm": 1.3541351742431766, "mse": 0.0}  0.6257
0         train   15200/102467 93:56/539:21       1.390         {"mlm": 1.3536243021786711, "mse": 0.0}  0.5311
0         train   15300/102467 94:32/538:37       1.390         {"mlm": 1.3552545969401744, "mse": 0.0}  0.5395
0         train   15400/102467 95:08/537:53       1.390         {"mlm": 1.3543703132262386, "mse": 0.0}  0.5517
0         train   15500/102467 95:44/537:10       1.390         {"mlm": 1.3551695412882816, "mse": 0.0}  0.5952
0         train   15600/102467 96:20/536:27       1.389         {"mlm": 1.3558296722002112, "mse": 0.0}  0.5971
0         train   15700/102467 96:56/535:44       1.389         {"mlm": 1.355690496865094, "mse": 0.0}  0.5885
0         train   15800/102467 97:32/535:01       1.389         {"mlm": 1.3558643043571639, "mse": 0.0}  0.5622
0         train   15900/102467 98:08/534:17       1.389         {"mlm": 1.357148470284439, "mse": 0.0}  0.5337
0         train   16000/102467 98:44/533:34       1.389         {"mlm": 1.3566228150426447, "mse": 0.0}  0.5494
0         train   16100/102467 99:19/532:51       1.388         {"mlm": 1.3386232773053277, "mse": 0.0}  0.5821
0         train   16200/102467 99:55/532:09       1.388         {"mlm": 1.3346661241526532, "mse": 0.0}  0.5507
0         train   16300/102467 100:31/531:26      1.388         {"mlm": 1.3494949250510244, "mse": 0.0}  0.6088
0         train   16400/102467 101:07/530:44      1.388         {"mlm": 1.34754991726551, "mse": 0.0}  0.589
0         train   16500/102467 101:43/530:02      1.388         {"mlm": 1.349642889960191, "mse": 0.0}  0.6025
0         train   16600/102467 102:20/529:20      1.387         {"mlm": 1.3526422768781332, "mse": 0.0}  0.5502
0         train   16700/102467 102:56/528:39      1.387         {"mlm": 1.3526159083655096, "mse": 0.0}  0.5501
0         train   16800/102467 103:32/527:57      1.387         {"mlm": 1.3531077538559697, "mse": 0.0}  0.5747
0         train   16900/102467 104:08/527:15      1.387         {"mlm": 1.3541615398033806, "mse": 0.0}  0.5761
0         train   17000/102467 104:44/526:33      1.387         {"mlm": 1.355127419189082, "mse": 0.0}  0.5966
0         train   17100/102467 105:20/525:52      1.387         {"mlm": 1.355993368077952, "mse": 0.0}  0.5462
0         train   17200/102467 105:56/525:10      1.386         {"mlm": 1.3549563957435844, "mse": 0.0}  0.5622
0         train   17300/102467 106:32/524:28      1.386         {"mlm": 1.3532902550127475, "mse": 0.0}  0.5402
0         train   17400/102467 107:08/523:47      1.386         {"mlm": 1.3527411033873398, "mse": 0.0}  0.5755
0         train   17500/102467 107:44/523:05      1.386         {"mlm": 1.3542647674709618, "mse": 0.0}  0.5791
0         train   17600/102467 108:20/522:23      1.385         {"mlm": 1.3523150086029665, "mse": 0.0}  0.6067
0         train   17700/102467 108:56/521:41      1.385         {"mlm": 1.3537043939285862, "mse": 0.0}  0.6814
0         train   17800/102467 109:32/521:00      1.385         {"mlm": 1.356131540382313, "mse": 0.0}  0.7351
0         train   17900/102467 110:08/520:19      1.385         {"mlm": 1.3580507109148854, "mse": 0.0}  0.605
0         train   18000/102467 110:44/519:37      1.385         {"mlm": 1.3582534748253132, "mse": 0.0}  0.5207
0         train   18100/102467 111:20/518:56      1.385         {"mlm": 1.3618036583065987, "mse": 0.0}  0.5895
0         train   18200/102467 111:55/518:15      1.385         {"mlm": 1.3569458388552373, "mse": 0.0}  0.9487
0         train   18300/102467 112:31/517:33      1.385         {"mlm": 1.3493264176555582, "mse": 0.0}  0.8243
0         train   18400/102467 113:07/516:53      1.385         {"mlm": 1.3495007313863197, "mse": 0.0}  0.5818
0         train   18500/102467 113:43/516:11      1.384         {"mlm": 1.3528928166675953, "mse": 0.0}  0.5532
0         train   18600/102467 114:19/515:30      1.384         {"mlm": 1.3578688607319889, "mse": 0.0}  0.5831
0         train   18700/102467 114:55/514:50      1.384         {"mlm": 1.3574164696465965, "mse": 0.0}  0.5808
0         train   18800/102467 115:31/514:09      1.384         {"mlm": 1.3568167073193507, "mse": 0.0}  0.5627
0         train   18900/102467 116:07/513:28      1.384         {"mlm": 1.3580917489847966, "mse": 0.0}  0.5952
0         train   19000/102467 116:44/512:48      1.384         {"mlm": 1.3594462882323437, "mse": 0.0}  0.6069
0         train   19100/102467 117:20/512:08      1.384         {"mlm": 1.3606297394437512, "mse": 0.0}  0.581
0         train   19200/102467 117:56/511:28      1.384         {"mlm": 1.35980301307994, "mse": 0.0}  0.5733
0         train   19300/102467 118:32/510:47      1.384         {"mlm": 1.3604117077257898, "mse": 0.0}  0.6257
0         train   19400/102467 119:08/510:07      1.383         {"mlm": 1.359622559694301, "mse": 0.0}  0.5949
0         train   19500/102467 119:44/509:27      1.383         {"mlm": 1.3598323552844358, "mse": 0.0}  0.5734
0         train   19600/102467 120:20/508:46      1.383         {"mlm": 1.3595919222535944, "mse": 0.0}  0.6058
0         train   19700/102467 120:56/508:06      1.383         {"mlm": 1.3590852677611247, "mse": 0.0}  0.5559
0         train   19800/102467 121:32/507:25      1.383         {"mlm": 1.358972296384236, "mse": 0.0}  0.6076
0         train   19900/102467 122:08/506:45      1.383         {"mlm": 1.3584114587571048, "mse": 0.0}  0.6246
0         train   20000/102467 122:44/506:05      1.383         {"mlm": 1.3578157891371925, "mse": 0.0}  1.1323

10/04/2022 16:29:20 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step20000.pkl
0         valid   1/781        0:10/141:35        1.136           None
0         valid   101/781      0:26/ 2:57         1.261           None
0         valid   201/781      0:41/ 2:00         1.274           None
0         valid   301/781      0:57/ 1:31         1.270           None
0         valid   401/781      1:12/ 1:08         1.263           None
0         valid   501/781      1:28/ 0:49         1.262           None
0         valid   601/781      1:43/ 0:31         1.258           None
0         valid   701/781      1:59/ 0:13         1.262           None
0         valid   781/781      2:15/ 0:00         1.254         {"mlm": 1.2538412291933418, "mse": 0.0, "train": 0.0}  None
0         train   20100/102467 125:36/514:41      1.383         {"mlm": 1.3670734679698944, "mse": 0.0}  0.6391
0         train   20200/102467 126:11/513:57      1.382         {"mlm": 1.360379786491394, "mse": 0.0}  0.5865
0         train   20300/102467 126:47/513:13      1.382         {"mlm": 1.3495378037293753, "mse": 0.0}  0.6561
0         train   20400/102467 127:23/512:29      1.382         {"mlm": 1.3497383384406567, "mse": 0.0}  0.5817
0         train   20500/102467 127:59/511:45      1.382         {"mlm": 1.3505473420619964, "mse": 0.0}  0.6143
0         train   20600/102467 128:35/511:01      1.382         {"mlm": 1.3524635555346807, "mse": 0.0}  0.5995
0         train   20700/102467 129:10/510:16      1.382         {"mlm": 1.3504152166843415, "mse": 0.0}  0.5914
0         train   20800/102467 129:46/509:33      1.381         {"mlm": 1.3500966116786004, "mse": 0.0}  0.7597
0         train   20900/102467 130:22/508:49      1.381         {"mlm": 1.350397516489029, "mse": 0.0}  0.5982
0         train   21000/102467 130:58/508:05      1.381         {"mlm": 1.3524280905127526, "mse": 0.0}  0.5857
0         train   21100/102467 131:34/507:22      1.381         {"mlm": 1.3538796542449432, "mse": 0.0}  0.626
0         train   21200/102467 132:10/506:38      1.381         {"mlm": 1.3510935664176942, "mse": 0.0}  0.5713
0         train   21300/102467 132:45/505:55      1.381         {"mlm": 1.3535039421228263, "mse": 0.0}  0.6599
0         train   21400/102467 133:21/505:12      1.381         {"mlm": 1.3536392217023032, "mse": 0.0}  0.5958
0         train   21500/102467 133:57/504:29      1.381         {"mlm": 1.353664155681928, "mse": 0.0}  0.6197
0         train   21600/102467 134:33/503:45      1.380         {"mlm": 1.3538126320764423, "mse": 0.0}  0.5987
0         train   21700/102467 135:09/503:02      1.380         {"mlm": 1.3529993288657245, "mse": 0.0}  0.6338
0         train   21800/102467 135:45/502:19      1.380         {"mlm": 1.3527734937270481, "mse": 0.0}  0.5986
0         train   21900/102467 136:20/501:36      1.380         {"mlm": 1.3511904063977693, "mse": 0.0}  0.5962
0         train   22000/102467 136:56/500:53      1.380         {"mlm": 1.3517853232622146, "mse": 0.0}  0.5942
0         train   22100/102467 137:32/500:10      1.380         {"mlm": 1.3705525030993453, "mse": 0.0}  0.6012
0         train   22200/102467 138:08/499:27      1.380         {"mlm": 1.3514193431216868, "mse": 0.0}  0.6192
0         train   22300/102467 138:43/498:43      1.380         {"mlm": 1.3613341378926433, "mse": 0.0}  0.6087
0         train   22400/102467 139:19/498:00      1.379         {"mlm": 1.3568599558713144, "mse": 0.0}  0.726
0         train   22500/102467 139:55/497:17      1.379         {"mlm": 1.3575112095815625, "mse": 0.0}  0.6044
0         train   22600/102467 140:31/496:35      1.379         {"mlm": 1.3578799070420369, "mse": 0.0}  0.6078
0         train   22700/102467 141:06/495:52      1.379         {"mlm": 1.362478974465819, "mse": 0.0}  0.6943
0         train   22800/102467 141:42/495:09      1.379         {"mlm": 1.361208681394818, "mse": 0.0}  0.6501
0         train   22900/102467 142:18/494:27      1.379         {"mlm": 1.3601922150846317, "mse": 0.0}  0.5829
0         train   23000/102467 142:54/493:44      1.379         {"mlm": 1.3582200311325692, "mse": 0.0}  0.6405
0         train   23100/102467 143:29/493:01      1.379         {"mlm": 1.3589285766677057, "mse": 0.0}  0.9506
0         train   23200/102467 144:05/492:19      1.379         {"mlm": 1.3583712710550768, "mse": 0.0}  0.6021
0         train   23300/102467 144:41/491:36      1.379         {"mlm": 1.3589210976904589, "mse": 0.0}  0.6232
0         train   23400/102467 145:17/490:54      1.379         {"mlm": 1.359235490408005, "mse": 0.0}  0.6727
0         train   23500/102467 145:52/490:12      1.379         {"mlm": 1.3596743199330636, "mse": 0.0}  0.618
0         train   23600/102467 146:28/489:30      1.378         {"mlm": 1.35953757112812, "mse": 0.0}  0.6058
0         train   23700/102467 147:04/488:48      1.378         {"mlm": 1.3602805622470175, "mse": 0.0}  0.6298
0         train   23800/102467 147:40/488:06      1.378         {"mlm": 1.35987729572998, "mse": 0.0}  0.5751
0         train   23900/102467 148:16/487:24      1.378         {"mlm": 1.3591371804491479, "mse": 0.0}  0.6353
0         train   24000/102467 148:52/486:42      1.378         {"mlm": 1.3585892637829116, "mse": 0.0}  0.647
0         train   24100/102467 149:27/486:01      1.378         {"mlm": 1.3221336809956297, "mse": 0.0}  0.6205
0         train   24200/102467 150:03/485:19      1.378         {"mlm": 1.3603529978280116, "mse": 0.0}  0.6461
0         train   24300/102467 150:39/484:37      1.378         {"mlm": 1.3666945327848397, "mse": 0.0}  0.6462
0         train   24400/102467 151:15/483:55      1.378         {"mlm": 1.3663797782893157, "mse": 0.0}  0.6497
0         train   24500/102467 151:50/483:13      1.378         {"mlm": 1.3615459720772434, "mse": 0.0}  0.6175
0         train   24600/102467 152:26/482:32      1.378         {"mlm": 1.3629232615132794, "mse": 0.0}  0.7618
0         train   24700/102467 153:02/481:50      1.378         {"mlm": 1.3669109651898927, "mse": 0.0}  0.9097
0         train   24800/102467 153:38/481:08      1.378         {"mlm": 1.3625392204239255, "mse": 0.0}  0.5836
0         train   24900/102467 154:13/480:27      1.377         {"mlm": 1.3607305744310265, "mse": 0.0}  0.5948
0         train   25000/102467 154:49/479:45      1.377         {"mlm": 1.3596717747514377, "mse": 0.0}  1.359
0         train   25100/102467 155:25/479:03      1.377         {"mlm": 1.3589112553983005, "mse": 0.0}  0.6421
0         train   25200/102467 156:01/478:22      1.377         {"mlm": 1.357377947720144, "mse": 0.0}  0.6381
0         train   25300/102467 156:36/477:41      1.377         {"mlm": 1.3572127946719919, "mse": 0.0}  0.6537
0         train   25400/102467 157:12/476:59      1.377         {"mlm": 1.3599908582130726, "mse": 0.0}  0.6271
0         train   25500/102467 157:48/476:18      1.377         {"mlm": 1.3598804883708622, "mse": 0.0}  0.619
0         train   25600/102467 158:24/475:37      1.377         {"mlm": 1.3620091521173008, "mse": 0.0}  0.6323
0         train   25700/102467 159:00/474:56      1.377         {"mlm": 1.3631005597128605, "mse": 0.0}  0.644
0         train   25800/102467 159:35/474:15      1.377         {"mlm": 1.3645202155505722, "mse": 0.0}  0.891
0         train   25900/102467 160:11/473:34      1.377         {"mlm": 1.3646368754299474, "mse": 0.0}  0.6498
0         train   26000/102467 160:47/472:53      1.377         {"mlm": 1.3632980150443774, "mse": 0.0}  0.6416
0         train   26100/102467 161:23/472:13      1.377         {"mlm": 1.3431140626828695, "mse": 0.0}  0.7223
0         train   26200/102467 161:59/471:32      1.377         {"mlm": 1.3461068021464468, "mse": 0.0}  0.6421
0         train   26300/102467 162:34/470:51      1.377         {"mlm": 1.3476060941965893, "mse": 0.0}  0.6056
0         train   26400/102467 163:10/470:10      1.377         {"mlm": 1.3599479387629243, "mse": 0.0}  1.0159
0         train   26500/102467 163:46/469:29      1.377         {"mlm": 1.3620395557261569, "mse": 0.0}  0.6134
0         train   26600/102467 164:22/468:48      1.376         {"mlm": 1.3594665383573752, "mse": 0.0}  0.6599
0         train   26700/102467 164:58/468:07      1.377         {"mlm": 1.3672546363800464, "mse": 0.0}  0.6262
0         train   26800/102467 165:33/467:27      1.377         {"mlm": 1.3676129148775242, "mse": 0.0}  0.6204
0         train   26900/102467 166:09/466:46      1.377         {"mlm": 1.3661471953493032, "mse": 0.0}  0.6302
0         train   27000/102467 166:45/466:05      1.376         {"mlm": 1.3620027049613692, "mse": 0.0}  0.5937
0         train   27100/102467 167:21/465:25      1.376         {"mlm": 1.365424294143562, "mse": 0.0}  0.6203
0         train   27200/102467 167:56/464:44      1.376         {"mlm": 1.3660952542459557, "mse": 0.0}  0.6494
0         train   27300/102467 168:32/464:03      1.376         {"mlm": 1.365947378030996, "mse": 0.0}  0.6369
0         train   27400/102467 169:08/463:23      1.376         {"mlm": 1.3653532508107364, "mse": 0.0}  0.5958
0         train   27500/102467 169:44/462:42      1.376         {"mlm": 1.3670899090801945, "mse": 0.0}  0.6249
0         train   27600/102467 170:19/462:02      1.376         {"mlm": 1.367522993010137, "mse": 0.0}  0.6305
0         train   27700/102467 170:55/461:21      1.376         {"mlm": 1.368056247005339, "mse": 0.0}  2.3643
0         train   27800/102467 171:31/460:41      1.376         {"mlm": 1.3682766965713247, "mse": 0.0}  0.6461
0         train   27900/102467 172:07/460:01      1.376         {"mlm": 1.3681177046026252, "mse": 0.0}  0.6676
0         train   28000/102467 172:42/459:20      1.376         {"mlm": 1.3687430700780632, "mse": 0.0}  0.8643
0         train   28100/102467 173:18/458:40      1.376         {"mlm": 1.3717050577203433, "mse": 0.0}  0.6438
0         train   28200/102467 173:54/458:00      1.376         {"mlm": 1.3566322010390612, "mse": 0.0}  0.6602
0         train   28300/102467 174:30/457:19      1.376         {"mlm": 1.3594176356454153, "mse": 0.0}  0.6707
0         train   28400/102467 175:06/456:39      1.376         {"mlm": 1.3635317052554603, "mse": 0.0}  0.6504
0         train   28500/102467 175:41/455:59      1.376         {"mlm": 1.369303085270428, "mse": 0.0}  0.6566
0         train   28600/102467 176:17/455:19      1.376         {"mlm": 1.3715842271211163, "mse": 0.0}  0.8346
0         train   28700/102467 176:53/454:40      1.376         {"mlm": 1.3700153480829864, "mse": 0.0}  0.6206
0         train   28800/102467 177:29/454:00      1.376         {"mlm": 1.3701684176173043, "mse": 0.0}  0.6001
0         train   28900/102467 178:05/453:20      1.376         {"mlm": 1.3685333095490932, "mse": 0.0}  0.5952
0         train   29000/102467 178:41/452:40      1.376         {"mlm": 1.3710725141098221, "mse": 0.0}  0.6319
0         train   29100/102467 179:17/452:00      1.376         {"mlm": 1.3701883764380085, "mse": 0.0}  0.7331
0         train   29200/102467 179:52/451:20      1.376         {"mlm": 1.3702670582959484, "mse": 0.0}  0.6196
0         train   29300/102467 180:28/450:41      1.376         {"mlm": 1.369224298423455, "mse": 0.0}  0.6389
0         train   29400/102467 181:04/450:01      1.376         {"mlm": 1.367885773238275, "mse": 0.0}  0.6303
0         train   29500/102467 181:40/449:21      1.376         {"mlm": 1.367962807297388, "mse": 0.0}  0.669
0         train   29600/102467 182:16/448:41      1.376         {"mlm": 1.3706813791117871, "mse": 0.0}  0.7445
0         train   29700/102467 182:51/448:01      1.376         {"mlm": 1.3708662535854668, "mse": 0.0}  0.6811
0         train   29800/102467 183:27/447:22      1.376         {"mlm": 1.3710432722666748, "mse": 0.0}  0.6575
0         train   29900/102467 184:03/446:42      1.376         {"mlm": 1.371251543111439, "mse": 0.0}  0.6466
0         train   30000/102467 184:39/446:02      1.376         {"mlm": 1.3723342280409379, "mse": 0.0}  0.638

10/04/2022 17:31:15 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step30000.pkl
0         valid   1/781        0:10/142:44        1.219           None
0         valid   101/781      0:26/ 2:57         1.269           None
0         valid   201/781      0:41/ 2:00         1.287           None
0         valid   301/781      0:57/ 1:31         1.284           None
0         valid   401/781      1:12/ 1:08         1.282           None
0         valid   501/781      1:28/ 0:49         1.286           None
0         valid   601/781      1:43/ 0:31         1.284           None
0         valid   701/781      1:59/ 0:13         1.285           None
0         valid   781/781      2:15/ 0:00         1.274         {"mlm": 1.2741677336747759, "mse": 0.0, "train": 0.0}  None
0         train   30100/102467 187:30/450:48      1.376         {"mlm": 1.380627019405365, "mse": 0.0}  0.6769
0         train   30200/102467 188:06/450:07      1.376         {"mlm": 1.3777939477562904, "mse": 0.0}  0.6488
0         train   30300/102467 188:42/449:26      1.376         {"mlm": 1.390224884947141, "mse": 0.0}  0.5965
0         train   30400/102467 189:17/448:45      1.376         {"mlm": 1.383552044481039, "mse": 0.0}  0.6376
0         train   30500/102467 189:53/448:04      1.376         {"mlm": 1.3807808866500855, "mse": 0.0}  0.6129
0         train   30600/102467 190:29/447:22      1.376         {"mlm": 1.3801038386424382, "mse": 0.0}  0.6149
0         train   30700/102467 191:05/446:41      1.376         {"mlm": 1.3811313526970999, "mse": 0.0}  0.669
0         train   30800/102467 191:40/446:00      1.376         {"mlm": 1.378176720365882, "mse": 0.0}  0.5992
0         train   30900/102467 192:16/445:19      1.376         {"mlm": 1.376562695701917, "mse": 0.0}  0.6153
0         train   31000/102467 192:52/444:39      1.376         {"mlm": 1.375073187172413, "mse": 0.0}  0.6011
0         train   31100/102467 193:28/443:58      1.376         {"mlm": 1.3738094355301425, "mse": 0.0}  0.6126
0         train   31200/102467 194:04/443:17      1.376         {"mlm": 1.372925223906835, "mse": 0.0}  0.6296
0         train   31300/102467 194:40/442:37      1.376         {"mlm": 1.3740058933771573, "mse": 0.0}  0.6583
0         train   31400/102467 195:15/441:56      1.376         {"mlm": 1.3737780146939413, "mse": 0.0}  0.6391
0         train   31500/102467 195:51/441:15      1.376         {"mlm": 1.3714414726495743, "mse": 0.0}  0.6331
0         train   31600/102467 196:27/440:34      1.376         {"mlm": 1.374175258539617, "mse": 0.0}  0.5972
0         train   31700/102467 197:03/439:54      1.376         {"mlm": 1.3745301776072558, "mse": 0.0}  0.6389
0         train   31800/102467 197:38/439:13      1.376         {"mlm": 1.3735659006237984, "mse": 0.0}  0.6286
0         train   31900/102467 198:14/438:32      1.376         {"mlm": 1.3735085794800206, "mse": 0.0}  0.6692
0         train   32000/102467 198:50/437:52      1.376         {"mlm": 1.372309724509716, "mse": 0.0}  1.7778
0         train   32100/102467 199:26/437:11      1.376         {"mlm": 1.3969805758408826, "mse": 0.0}  0.6707
0         train   32200/102467 200:02/436:30      1.376         {"mlm": 1.3981945316995208, "mse": 0.0}  0.6407
0         train   32300/102467 200:37/435:50      1.376         {"mlm": 1.3905807143469717, "mse": 0.0}  0.6302
0         train   32400/102467 201:13/435:09      1.376         {"mlm": 1.387645092823153, "mse": 0.0}  0.6167
0         train   32500/102467 201:49/434:29      1.376         {"mlm": 1.3820487279930191, "mse": 0.0}  0.6683
0         train   32600/102467 202:25/433:48      1.376         {"mlm": 1.3820286698651831, "mse": 0.0}  0.615
0         train   32700/102467 203:00/433:08      1.376         {"mlm": 1.3802267477577166, "mse": 0.0}  0.6273
0         train   32800/102467 203:36/432:27      1.376         {"mlm": 1.3795055525472972, "mse": 0.0}  0.6538
0         train   32900/102467 204:12/431:47      1.376         {"mlm": 1.3808454459448147, "mse": 0.0}  0.5978
0         train   33000/102467 204:48/431:07      1.376         {"mlm": 1.3840002092393908, "mse": 0.0}  0.6713
0         train   33100/102467 205:23/430:26      1.376         {"mlm": 1.3838603523669186, "mse": 0.0}  0.6257
0         train   33200/102467 205:59/429:46      1.376         {"mlm": 1.3814804691190616, "mse": 0.0}  0.6821
0         train   33300/102467 206:35/429:06      1.376         {"mlm": 1.3802912003999495, "mse": 0.0}  0.6433
0         train   33400/102467 207:11/428:26      1.376         {"mlm": 1.3801962904967606, "mse": 0.0}  0.6324
0         train   33500/102467 207:46/427:45      1.376         {"mlm": 1.3817378379568568, "mse": 0.0}  0.6615
0         train   33600/102467 208:22/427:05      1.376         {"mlm": 1.378901382250067, "mse": 0.0}  0.6023
0         train   33700/102467 208:58/426:25      1.376         {"mlm": 1.379072439817908, "mse": 0.0}  0.5683
0         train   33800/102467 209:34/425:45      1.376         {"mlm": 1.3791295360233335, "mse": 0.0}  0.5798
0         train   33900/102467 210:10/425:05      1.376         {"mlm": 1.3796541447260307, "mse": 0.0}  0.5891
0         train   34000/102467 210:45/424:25      1.376         {"mlm": 1.3786633811991713, "mse": 0.0}  0.6843
0         train   34100/102467 211:21/423:45      1.376         {"mlm": 1.349055717186052, "mse": 0.0}  0.6048
0         train   34200/102467 211:57/423:05      1.376         {"mlm": 1.3589628760260766, "mse": 0.0}  0.6063
0         train   34300/102467 212:33/422:25      1.376         {"mlm": 1.3662386424589477, "mse": 0.0}  0.6664
0         train   34400/102467 213:09/421:45      1.376         {"mlm": 1.3751978493815091, "mse": 0.0}  0.612
0         train   34500/102467 213:44/421:05      1.376         {"mlm": 1.3812463735959617, "mse": 0.0}  0.6183
0         train   34600/102467 214:20/420:25      1.376         {"mlm": 1.3785414138565892, "mse": 0.0}  0.5751
0         train   34700/102467 214:56/419:45      1.376         {"mlm": 1.378550686347792, "mse": 0.0}  0.6273
0         train   34800/102467 215:32/419:05      1.376         {"mlm": 1.3775210579983275, "mse": 0.0}  0.6483
0         train   34900/102467 216:07/418:26      1.376         {"mlm": 1.3741429178645723, "mse": 0.0}  0.6309
0         train   35000/102467 216:43/417:46      1.376         {"mlm": 1.3727549578479392, "mse": 0.0}  0.5791
0         train   35100/102467 217:19/417:06      1.376         {"mlm": 1.3742635836909594, "mse": 0.0}  0.7289
0         train   35200/102467 217:55/416:26      1.376         {"mlm": 1.3728782968947009, "mse": 0.0}  0.5745
0         train   35300/102467 218:30/415:46      1.376         {"mlm": 1.3726965202756214, "mse": 0.0}  0.5975
0         train   35400/102467 219:06/415:06      1.376         {"mlm": 1.3717428228374202, "mse": 0.0}  0.6004
0         train   35500/102467 219:42/414:27      1.376         {"mlm": 1.3722425068570712, "mse": 0.0}  0.6302
0         train   35600/102467 220:18/413:47      1.376         {"mlm": 1.372460143848116, "mse": 0.0}  0.5863
0         train   35700/102467 220:53/413:07      1.376         {"mlm": 1.3723607123319337, "mse": 0.0}  0.5959
0         train   35800/102467 221:29/412:28      1.376         {"mlm": 1.3746753285604802, "mse": 0.0}  0.6429
0         train   35900/102467 222:05/411:48      1.376         {"mlm": 1.3759815395192427, "mse": 0.0}  0.5756
0         train   36000/102467 222:41/411:08      1.376         {"mlm": 1.376389689571984, "mse": 0.0}  0.5921
0         train   36100/102467 223:17/410:29      1.376         {"mlm": 1.3994785903655376, "mse": 0.0}  0.6039
0         train   36200/102467 223:52/409:50      1.376         {"mlm": 1.3776803615734663, "mse": 0.0}  0.578
0         train   36300/102467 224:28/409:10      1.376         {"mlm": 1.3792807377548733, "mse": 0.0}  0.6107
0         train   36400/102467 225:04/408:31      1.376         {"mlm": 1.3814001545797967, "mse": 0.0}  0.6694
0         train   36500/102467 225:40/407:51      1.376         {"mlm": 1.379528911420757, "mse": 0.0}  0.5416
0         train   36600/102467 226:16/407:12      1.376         {"mlm": 1.3832806844008427, "mse": 0.0}  0.5752
0         train   36700/102467 226:52/406:32      1.376         {"mlm": 1.3850112612996588, "mse": 0.0}  0.605
0         train   36800/102467 227:27/405:53      1.376         {"mlm": 1.384390003109816, "mse": 0.0}  0.6434
0         train   36900/102467 228:03/405:14      1.376         {"mlm": 1.3815108136322718, "mse": 0.0}  0.6474
0         train   37000/102467 228:39/404:35      1.376         {"mlm": 1.3788815644703274, "mse": 0.0}  1.0132
0         train   37100/102467 229:15/403:56      1.376         {"mlm": 1.3762071992227352, "mse": 0.0}  0.5947
0         train   37200/102467 229:51/403:16      1.376         {"mlm": 1.3770227692181007, "mse": 0.0}  0.5506
0         train   37300/102467 230:27/402:37      1.376         {"mlm": 1.3749510956527458, "mse": 0.0}  0.5371
0         train   37400/102467 231:02/401:58      1.376         {"mlm": 1.3749376255098888, "mse": 0.0}  0.6241
0         train   37500/102467 231:38/401:19      1.376         {"mlm": 1.375108024201237, "mse": 0.0}  0.6558
0         train   37600/102467 232:14/400:39      1.376         {"mlm": 1.3764862289784026, "mse": 0.0}  0.5531
0         train   37700/102467 232:50/400:00      1.376         {"mlm": 1.3758080759045652, "mse": 0.0}  0.5688
0         train   37800/102467 233:26/399:21      1.376         {"mlm": 1.3740382818759118, "mse": 0.0}  0.5651
0         train   37900/102467 234:01/398:41      1.376         {"mlm": 1.375772363271095, "mse": 0.0}  0.5361
0         train   38000/102467 234:37/398:02      1.376         {"mlm": 1.3750157787552701, "mse": 0.0}  0.5784
0         train   38100/102467 235:13/397:23      1.376         {"mlm": 1.3752440108607213, "mse": 0.0}  0.5731
0         train   38200/102467 235:49/396:44      1.376         {"mlm": 1.3849183889676113, "mse": 0.0}  0.5624
0         train   38300/102467 236:24/396:05      1.376         {"mlm": 1.378219173566715, "mse": 0.0}  1.7708
0         train   38400/102467 237:00/395:25      1.376         {"mlm": 1.372547917143263, "mse": 0.0}  0.5949
0         train   38500/102467 237:36/394:46      1.376         {"mlm": 1.3719910028721056, "mse": 0.0}  0.6275
0         train   38600/102467 238:12/394:07      1.376         {"mlm": 1.3715034242244375, "mse": 0.0}  0.677
0         train   38700/102467 238:47/393:28      1.376         {"mlm": 1.3730220480383128, "mse": 0.0}  0.5425
0         train   38800/102467 239:23/392:49      1.376         {"mlm": 1.3730102474965042, "mse": 0.0}  0.5441
0         train   38900/102467 239:59/392:10      1.376         {"mlm": 1.3753025918932897, "mse": 0.0}  0.5853
0         train   39000/102467 240:35/391:31      1.376         {"mlm": 1.378081259358839, "mse": 0.0}  0.6065
0         train   39100/102467 241:10/390:52      1.376         {"mlm": 1.3776449849353218, "mse": 0.0}  0.5702
0         train   39200/102467 241:46/390:13      1.376         {"mlm": 1.3786286316986467, "mse": 0.0}  0.5705
0         train   39300/102467 242:22/389:34      1.376         {"mlm": 1.3774723897193686, "mse": 0.0}  0.5387
0         train   39400/102467 242:58/388:55      1.376         {"mlm": 1.3757301404602549, "mse": 0.0}  0.543
0         train   39500/102467 243:33/388:16      1.376         {"mlm": 1.3740458658592587, "mse": 0.0}  0.5755
0         train   39600/102467 244:09/387:37      1.376         {"mlm": 1.3739266722348698, "mse": 0.0}  0.5733
0         train   39700/102467 244:45/386:58      1.376         {"mlm": 1.3728199577626754, "mse": 0.0}  0.5798
0         train   39800/102467 245:21/386:19      1.376         {"mlm": 1.373323028861547, "mse": 0.0}  0.565
0         train   39900/102467 245:57/385:40      1.376         {"mlm": 1.373073403562423, "mse": 0.0}  0.8568
0         train   40000/102467 246:33/385:01      1.376         {"mlm": 1.3728223363299648, "mse": 0.0}  0.5067

10/04/2022 18:33:09 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step40000.pkl
0         valid   1/781        0:11/143:17        1.095           None
0         valid   101/781      0:26/ 2:58         1.267           None
0         valid   201/781      0:42/ 2:01         1.283           None
0         valid   301/781      0:57/ 1:31         1.283           None
0         valid   401/781      1:12/ 1:09         1.272           None
0         valid   501/781      1:28/ 0:49         1.272           None
0         valid   601/781      1:44/ 0:31         1.269           None
0         valid   701/781      1:59/ 0:13         1.269           None
0         valid   781/781      2:15/ 0:00         1.265         {"mlm": 1.2647247119078104, "mse": 0.0, "train": 0.0}  None
0         train   40100/102467 249:24/387:54      1.376         {"mlm": 1.3678784418106078, "mse": 0.0}  0.5921
0         train   40200/102467 250:00/387:14      1.376         {"mlm": 1.3772195893526078, "mse": 0.0}  0.5626
0         train   40300/102467 250:36/386:34      1.376         {"mlm": 1.3758590064446132, "mse": 0.0}  0.5281
0         train   40400/102467 251:11/385:55      1.376         {"mlm": 1.3701951967179775, "mse": 0.0}  0.5616
0         train   40500/102467 251:47/385:15      1.376         {"mlm": 1.364945319533348, "mse": 0.0}  0.8315
0         train   40600/102467 252:23/384:35      1.376         {"mlm": 1.367663789987564, "mse": 0.0}  0.6383
0         train   40700/102467 252:58/383:55      1.376         {"mlm": 1.3632364341190883, "mse": 0.0}  0.5524
0         train   40800/102467 253:34/383:15      1.376         {"mlm": 1.3634814175218344, "mse": 0.0}  3.1947
0         train   40900/102467 254:10/382:36      1.376         {"mlm": 1.3659471353557375, "mse": 0.0}  0.6566
0         train   41000/102467 254:45/381:56      1.376         {"mlm": 1.3647124832868576, "mse": 0.0}  0.5324
0         train   41100/102467 255:21/381:17      1.376         {"mlm": 1.366454689340158, "mse": 0.0}  0.5278
0         train   41200/102467 255:57/380:37      1.376         {"mlm": 1.3662691804269949, "mse": 0.0}  0.5206
0         train   41300/102467 256:33/379:58      1.376         {"mlm": 1.368012571655787, "mse": 0.0}  0.4968
0         train   41400/102467 257:08/379:18      1.376         {"mlm": 1.3676358429023199, "mse": 0.0}  0.5429
0         train   41500/102467 257:44/378:38      1.376         {"mlm": 1.368820039431254, "mse": 0.0}  0.5354
0         train   41600/102467 258:20/377:59      1.375         {"mlm": 1.3676076213270425, "mse": 0.0}  0.7071
0         train   41700/102467 258:56/377:19      1.375         {"mlm": 1.366076593539294, "mse": 0.0}  0.5581
0         train   41800/102467 259:31/376:40      1.375         {"mlm": 1.3656539843479791, "mse": 0.0}  1.0537
0         train   41900/102467 260:07/376:00      1.375         {"mlm": 1.3661225679359938, "mse": 0.0}  0.5188
0         train   42000/102467 260:43/375:21      1.375         {"mlm": 1.3661868238151074, "mse": 0.0}  0.5174
0         train   42100/102467 261:19/374:42      1.375         {"mlm": 1.3566511845347857, "mse": 0.0}  8.2369
0         train   42200/102467 261:55/374:03      1.375         {"mlm": 1.3777155972006332, "mse": 0.0}  0.5552
0         train   42300/102467 262:30/373:23      1.375         {"mlm": 1.3783582803796366, "mse": 0.0}  0.4921
0         train   42400/102467 263:06/372:44      1.375         {"mlm": 1.3850163128740507, "mse": 0.0}  0.5582
0         train   42500/102467 263:42/372:05      1.375         {"mlm": 1.3759134291169162, "mse": 0.0}  0.5307
0         train   42600/102467 264:18/371:26      1.375         {"mlm": 1.3684309114000832, "mse": 0.0}  0.5009
0         train   42700/102467 264:54/370:47      1.375         {"mlm": 1.3702064503586513, "mse": 0.0}  0.5453
0         train   42800/102467 265:30/370:07      1.375         {"mlm": 1.3694453550518977, "mse": 0.0}  0.4936
0         train   42900/102467 266:05/369:28      1.375         {"mlm": 1.3676372250009565, "mse": 0.0}  0.6972
0         train   43000/102467 266:41/368:49      1.375         {"mlm": 1.3676991107108238, "mse": 0.0}  0.5602
0         train   43100/102467 267:17/368:10      1.375         {"mlm": 1.3663700531568173, "mse": 0.0}  0.4885
0         train   43200/102467 267:53/367:31      1.375         {"mlm": 1.3645559497631223, "mse": 0.0}  0.5761
0         train   43300/102467 268:28/366:51      1.375         {"mlm": 1.365184806327438, "mse": 0.0}  0.549
0         train   43400/102467 269:04/366:12      1.375         {"mlm": 1.3641262682869744, "mse": 0.0}  0.5513
0         train   43500/102467 269:40/365:33      1.375         {"mlm": 1.362946962379789, "mse": 0.0}  0.5601
0         train   43600/102467 270:16/364:54      1.375         {"mlm": 1.3631890101534192, "mse": 0.0}  2.4532
0         train   43700/102467 270:51/364:15      1.375         {"mlm": 1.363221039836585, "mse": 0.0}  0.4924
0         train   43800/102467 271:27/363:35      1.375         {"mlm": 1.364060343570879, "mse": 0.0}  0.5629
0         train   43900/102467 272:03/362:56      1.375         {"mlm": 1.3641292015272044, "mse": 0.0}  0.5272
0         train   44000/102467 272:38/362:17      1.375         {"mlm": 1.363874870070581, "mse": 0.0}  0.5578
0         train   44100/102467 273:14/361:38      1.375         {"mlm": 1.3866309292462407, "mse": 0.0}  0.9831
0         train   44200/102467 273:50/360:59      1.375         {"mlm": 1.3773880543732884, "mse": 0.0}  0.528
0         train   44300/102467 274:26/360:20      1.375         {"mlm": 1.3759176357080471, "mse": 0.0}  0.5059
0         train   44400/102467 275:02/359:41      1.375         {"mlm": 1.3812029249404543, "mse": 0.0}  0.9948
0         train   44500/102467 275:37/359:02      1.375         {"mlm": 1.3746670531221183, "mse": 0.0}  0.503
0         train   44600/102467 276:13/358:23      1.375         {"mlm": 1.3764890750315675, "mse": 0.0}  0.5047
0         train   44700/102467 276:49/357:45      1.375         {"mlm": 1.3729453916877594, "mse": 0.0}  0.6557
0         train   44800/102467 277:25/357:06      1.375         {"mlm": 1.3719658782159476, "mse": 0.0}  0.5393
0         train   44900/102467 278:01/356:27      1.375         {"mlm": 1.3686042262079456, "mse": 0.0}  1.1862
0         train   45000/102467 278:37/355:48      1.375         {"mlm": 1.3692177417044171, "mse": 0.0}  1.3793
0         train   45100/102467 279:12/355:09      1.375         {"mlm": 1.3683825052175365, "mse": 0.0}  0.6221
0         train   45200/102467 279:48/354:30      1.375         {"mlm": 1.3672588433268074, "mse": 0.0}  0.6185
0         train   45300/102467 280:24/353:51      1.375         {"mlm": 1.3691847812045703, "mse": 0.0}  1.4283
0         train   45400/102467 281:00/353:12      1.375         {"mlm": 1.3699301752836066, "mse": 0.0}  0.5024
0         train   45500/102467 281:35/352:34      1.375         {"mlm": 1.3677928735003453, "mse": 0.0}  0.4807
0         train   45600/102467 282:11/351:55      1.375         {"mlm": 1.368565682662145, "mse": 0.0}  0.5074
0         train   45700/102467 282:47/351:16      1.375         {"mlm": 1.368478687477898, "mse": 0.0}  0.6892
0         train   45800/102467 283:23/350:37      1.375         {"mlm": 1.3676494386490514, "mse": 0.0}  0.4778
0         train   45900/102467 283:58/349:58      1.374         {"mlm": 1.3668518123875428, "mse": 0.0}  0.5611
0         train   46000/102467 284:34/349:19      1.374         {"mlm": 1.3664862353343505, "mse": 0.0}  0.4967
0         train   46100/102467 285:10/348:41      1.374         {"mlm": 1.3613562473316783, "mse": 0.0}  0.5803
0         train   46200/102467 285:46/348:02      1.374         {"mlm": 1.3500021093993018, "mse": 0.0}  0.5019
0         train   46300/102467 286:21/347:23      1.374         {"mlm": 1.3444764258885624, "mse": 0.0}  0.4894
0         train   46400/102467 286:57/346:44      1.374         {"mlm": 1.3423134387290148, "mse": 0.0}  0.4939
0         train   46500/102467 287:33/346:06      1.374         {"mlm": 1.342684380845045, "mse": 0.0}  0.7466
0         train   46600/102467 288:09/345:27      1.374         {"mlm": 1.3403358358633977, "mse": 0.0}  0.5344
0         train   46700/102467 288:45/344:48      1.374         {"mlm": 1.3441725449377357, "mse": 0.0}  0.5454
0         train   46800/102467 289:20/344:10      1.374         {"mlm": 1.3457437085836117, "mse": 0.0}  0.4895
0         train   46900/102467 289:56/343:31      1.374         {"mlm": 1.3436724041882433, "mse": 0.0}  0.5232
0         train   47000/102467 290:32/342:53      1.374         {"mlm": 1.3423754033018855, "mse": 0.0}  0.5113
0         train   47100/102467 291:08/342:14      1.374         {"mlm": 1.3414871017826397, "mse": 0.0}  0.4918
0         train   47200/102467 291:44/341:36      1.374         {"mlm": 1.3408863495664987, "mse": 0.0}  0.5754
0         train   47300/102467 292:20/340:57      1.373         {"mlm": 1.3398576520366126, "mse": 0.0}  0.4949
0         train   47400/102467 292:55/340:18      1.373         {"mlm": 1.3398234053603564, "mse": 0.0}  0.5075
0         train   47500/102467 293:31/339:40      1.373         {"mlm": 1.3397464961629753, "mse": 0.0}  0.4984
0         train   47600/102467 294:07/339:01      1.373         {"mlm": 1.339354694486784, "mse": 0.0}  0.4874
0         train   47700/102467 294:43/338:22      1.373         {"mlm": 1.3404076139436585, "mse": 0.0}  2.197
0         train   47800/102467 295:18/337:44      1.373         {"mlm": 1.340470914450631, "mse": 0.0}  2.016
0         train   47900/102467 295:54/337:05      1.373         {"mlm": 1.3420135466500716, "mse": 0.0}  0.6092
0         train   48000/102467 296:30/336:27      1.373         {"mlm": 1.3410929248341334, "mse": 0.0}  0.6002
0         train   48100/102467 297:06/335:48      1.373         {"mlm": 1.3474037026365597, "mse": 0.0}  0.537
0         train   48200/102467 297:41/335:10      1.373         {"mlm": 1.3406819251118873, "mse": 0.0}  0.5425
0         train   48300/102467 298:17/334:31      1.373         {"mlm": 1.3389181506794852, "mse": 0.0}  0.7393
0         train   48400/102467 298:53/333:52      1.373         {"mlm": 1.34210001850369, "mse": 0.0}  1.5768
0         train   48500/102467 299:29/333:14      1.373         {"mlm": 1.343700636899279, "mse": 0.0}  0.5839
0         train   48600/102467 300:04/332:35      1.373         {"mlm": 1.3409677049857658, "mse": 0.0}  0.5429
0         train   48700/102467 300:40/331:57      1.373         {"mlm": 1.3428351613639415, "mse": 0.0}  3.1983
0         train   48800/102467 301:16/331:19      1.373         {"mlm": 1.345357406978032, "mse": 0.0}  0.7675
0         train   48900/102467 301:51/330:40      1.372         {"mlm": 1.3437940488303346, "mse": 0.0}  0.6521
0         train   49000/102467 302:27/330:02      1.372         {"mlm": 1.3438706593700203, "mse": 0.0}  0.5193
0         train   49100/102467 303:03/329:23      1.372         {"mlm": 1.3409013623630044, "mse": 0.0}  0.5046
0         train   49200/102467 303:39/328:45      1.372         {"mlm": 1.3449466008207072, "mse": 0.0}  0.5634
0         train   49300/102467 304:15/328:06      1.372         {"mlm": 1.3430187155343132, "mse": 0.0}  0.5252
0         train   49400/102467 304:50/327:28      1.372         {"mlm": 1.3442446789290639, "mse": 0.0}  0.4919
0         train   49500/102467 305:26/326:50      1.372         {"mlm": 1.3441441845208566, "mse": 0.0}  4.3275
0         train   49600/102467 306:02/326:11      1.372         {"mlm": 1.344367105709879, "mse": 0.0}  1.0105
0         train   49700/102467 306:38/325:33      1.372         {"mlm": 1.344941949577264, "mse": 0.0}  1.4508
0         train   49800/102467 307:14/324:55      1.372         {"mlm": 1.3440710316123834, "mse": 0.0}  0.5098
0         train   49900/102467 307:49/324:16      1.372         {"mlm": 1.3447655501622189, "mse": 0.0}  0.9214
0         train   50000/102467 308:25/323:38      1.372         {"mlm": 1.3453869327215966, "mse": 0.0}  0.5066

10/04/2022 19:35:02 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step50000.pkl
0         valid   1/781        0:12/156:50        1.120           None
0         valid   101/781      0:27/ 3:05         1.254           None
0         valid   201/781      0:42/ 2:03         1.262           None
0         valid   301/781      0:58/ 1:33         1.253           None
0         valid   401/781      1:13/ 1:10         1.251           None
0         valid   501/781      1:29/ 0:49         1.252           None
0         valid   601/781      1:44/ 0:31         1.251           None
0         valid   701/781      2:00/ 0:13         1.253           None
0         valid   781/781      2:16/ 0:00         1.249         {"mlm": 1.249359795134443, "mse": 0.0, "train": 0.0}  None
0         train   50100/102467 311:18/325:23      1.372         {"mlm": 1.3279817700386047, "mse": 0.0}  0.6482
0         train   50200/102467 311:54/324:44      1.372         {"mlm": 1.3520675566792488, "mse": 0.0}  1.2185
0         train   50300/102467 312:30/324:06      1.372         {"mlm": 1.355959544380506, "mse": 0.0}  0.5263
0         train   50400/102467 313:05/323:27      1.372         {"mlm": 1.358959001749754, "mse": 0.0}  0.7819
0         train   50500/102467 313:41/322:48      1.372         {"mlm": 1.358528982758522, "mse": 0.0}  0.5458
0         train   50600/102467 314:17/322:09      1.372         {"mlm": 1.3581839285294215, "mse": 0.0}  1.0149
0         train   50700/102467 314:53/321:30      1.372         {"mlm": 1.353695358633995, "mse": 0.0}  0.4849
0         train   50800/102467 315:29/320:52      1.372         {"mlm": 1.354196889102459, "mse": 0.0}  0.4972
0         train   50900/102467 316:04/320:13      1.372         {"mlm": 1.3524604264232847, "mse": 0.0}  0.776
0         train   51000/102467 316:40/319:34      1.372         {"mlm": 1.3521595745682717, "mse": 0.0}  0.4724
0         train   51100/102467 317:16/318:55      1.371         {"mlm": 1.3501678022471342, "mse": 0.0}  0.519
0         train   51200/102467 317:52/318:17      1.371         {"mlm": 1.35147670874993, "mse": 0.0}  0.7393
0         train   51300/102467 318:28/317:38      1.371         {"mlm": 1.3506062732293056, "mse": 0.0}  0.5664
0         train   51400/102467 319:03/316:59      1.371         {"mlm": 1.3498379269668035, "mse": 0.0}  0.5243
0         train   51500/102467 319:39/316:21      1.371         {"mlm": 1.3484641735553742, "mse": 0.0}  0.469
0         train   51600/102467 320:15/315:42      1.371         {"mlm": 1.3470034551620484, "mse": 0.0}  0.4934
0         train   51700/102467 320:51/315:04      1.371         {"mlm": 1.3462233947305118, "mse": 0.0}  0.492
0         train   51800/102467 321:27/314:25      1.371         {"mlm": 1.3458352069722281, "mse": 0.0}  0.4774
0         train   51900/102467 322:03/313:46      1.371         {"mlm": 1.3459004752259505, "mse": 0.0}  0.4933
0         train   52000/102467 322:39/313:08      1.371         {"mlm": 1.3464983416199685, "mse": 0.0}  0.5811
0         train   52100/102467 323:14/312:29      1.371         {"mlm": 1.3575188631963249, "mse": 0.0}  0.5152
0         train   52200/102467 323:50/311:51      1.371         {"mlm": 1.3522385793115625, "mse": 0.0}  0.4697
0         train   52300/102467 324:26/311:12      1.371         {"mlm": 1.3471440373854493, "mse": 0.0}  0.5454
0         train   52400/102467 325:02/310:33      1.371         {"mlm": 1.3493560134318836, "mse": 0.0}  0.5959
0         train   52500/102467 325:37/309:55      1.371         {"mlm": 1.3470273822964074, "mse": 0.0}  0.4997
0         train   52600/102467 326:13/309:16      1.371         {"mlm": 1.3452863000669144, "mse": 0.0}  1.9005
0         train   52700/102467 326:49/308:38      1.371         {"mlm": 1.3431563808853193, "mse": 0.0}  0.5082
0         train   52800/102467 327:25/307:59      1.371         {"mlm": 1.3458756457580643, "mse": 0.0}  0.5505
0         train   52900/102467 328:00/307:20      1.371         {"mlm": 1.3482907281834238, "mse": 0.0}  0.5259
0         train   53000/102467 328:36/306:42      1.371         {"mlm": 1.349035795565482, "mse": 0.0}  0.474
0         train   53100/102467 329:12/306:03      1.370         {"mlm": 1.3479568216885296, "mse": 0.0}  0.5238
0         train   53200/102467 329:48/305:25      1.370         {"mlm": 1.3486456597616914, "mse": 0.0}  0.5098
0         train   53300/102467 330:24/304:46      1.370         {"mlm": 1.347878773089461, "mse": 0.0}  0.6002
0         train   53400/102467 330:59/304:08      1.370         {"mlm": 1.348275584182712, "mse": 0.0}  0.5717
0         train   53500/102467 331:35/303:29      1.370         {"mlm": 1.3464317669305426, "mse": 0.0}  0.4931
0         train   53600/102467 332:11/302:51      1.370         {"mlm": 1.3459218907087873, "mse": 0.0}  0.5408
0         train   53700/102467 332:47/302:12      1.370         {"mlm": 1.345848842358996, "mse": 0.0}  0.595
0         train   53800/102467 333:22/301:34      1.370         {"mlm": 1.3457107850748542, "mse": 0.0}  0.5504
0         train   53900/102467 333:58/300:55      1.370         {"mlm": 1.3442103732504802, "mse": 0.0}  0.5459
0         train   54000/102467 334:34/300:17      1.370         {"mlm": 1.3439354592707826, "mse": 0.0}  0.5097
0         train   54100/102467 335:10/299:39      1.370         {"mlm": 1.3218686501590573, "mse": 0.0}  0.5565
0         train   54200/102467 335:46/299:00      1.370         {"mlm": 1.3222783006200887, "mse": 0.0}  0.516
0         train   54300/102467 336:21/298:22      1.370         {"mlm": 1.3313020227739476, "mse": 0.0}  0.5362
0         train   54400/102467 336:57/297:43      1.370         {"mlm": 1.3310111343261584, "mse": 0.0}  0.5414
0         train   54500/102467 337:33/297:05      1.370         {"mlm": 1.337265099267883, "mse": 0.0}  0.5118
0         train   54600/102467 338:09/296:27      1.370         {"mlm": 1.3370992585169432, "mse": 0.0}  0.4758
0         train   54700/102467 338:45/295:48      1.370         {"mlm": 1.336138243661569, "mse": 0.0}  0.564
0         train   54800/102467 339:20/295:10      1.369         {"mlm": 1.3378880989730806, "mse": 0.0}  0.5939
0         train   54900/102467 339:56/294:32      1.369         {"mlm": 1.3374708076495105, "mse": 0.0}  0.9697
0         train   55000/102467 340:32/293:54      1.369         {"mlm": 1.337343717266419, "mse": 0.0}  0.5613
0         train   55100/102467 341:08/293:15      1.369         {"mlm": 1.3393307669675198, "mse": 0.0}  0.4592
0         train   55200/102467 341:44/292:37      1.369         {"mlm": 1.338917878165667, "mse": 0.0}  0.5026
0         train   55300/102467 342:20/291:59      1.369         {"mlm": 1.3387610893495645, "mse": 0.0}  0.5132
0         train   55400/102467 342:55/291:21      1.369         {"mlm": 1.3385057339340831, "mse": 0.0}  0.5221
0         train   55500/102467 343:31/290:42      1.369         {"mlm": 1.3364968268432031, "mse": 0.0}  1.1601
0         train   55600/102467 344:07/290:04      1.369         {"mlm": 1.336823836696312, "mse": 0.0}  0.5066
0         train   55700/102467 344:43/289:26      1.369         {"mlm": 1.3368716677661778, "mse": 0.0}  0.4839
0         train   55800/102467 345:18/288:47      1.369         {"mlm": 1.3364210206356408, "mse": 0.0}  0.5233
0         train   55900/102467 345:54/288:09      1.369         {"mlm": 1.3366438409174204, "mse": 0.0}  0.5117
0         train   56000/102467 346:30/287:31      1.369         {"mlm": 1.3366357157001265, "mse": 0.0}  0.5129
0         train   56100/102467 347:06/286:53      1.369         {"mlm": 1.3293714265233463, "mse": 0.0}  0.5146
0         train   56200/102467 347:41/286:14      1.369         {"mlm": 1.3314034118870188, "mse": 0.0}  0.5636
0         train   56300/102467 348:17/285:36      1.369         {"mlm": 1.3256691505611946, "mse": 0.0}  0.6691
0         train   56400/102467 348:53/284:58      1.368         {"mlm": 1.3297054254137899, "mse": 0.0}  0.5121
0         train   56500/102467 349:29/284:19      1.368         {"mlm": 1.3332121367425747, "mse": 0.0}  0.5109
0         train   56600/102467 350:04/283:41      1.368         {"mlm": 1.3365717793828877, "mse": 0.0}  0.5415
0         train   56700/102467 350:40/283:03      1.368         {"mlm": 1.3347910349304057, "mse": 0.0}  0.6327
0         train   56800/102467 351:16/282:25      1.368         {"mlm": 1.3342317300877877, "mse": 0.0}  0.5243
0         train   56900/102467 351:52/281:47      1.368         {"mlm": 1.3332622101075673, "mse": 0.0}  0.4677
0         train   57000/102467 352:28/281:09      1.368         {"mlm": 1.3314289927123901, "mse": 0.0}  0.4987
0         train   57100/102467 353:03/280:30      1.368         {"mlm": 1.330811185736817, "mse": 0.0}  0.511
0         train   57200/102467 353:39/279:52      1.368         {"mlm": 1.3302124759607148, "mse": 0.0}  0.8829
0         train   57300/102467 354:15/279:14      1.368         {"mlm": 1.3286420150545808, "mse": 0.0}  0.6401
0         train   57400/102467 354:51/278:36      1.368         {"mlm": 1.3255658935203498, "mse": 0.0}  0.5097
0         train   57500/102467 355:27/277:58      1.368         {"mlm": 1.3260275073344499, "mse": 0.0}  6.5299
0         train   57600/102467 356:03/277:20      1.368         {"mlm": 1.3257698129696927, "mse": 0.0}  0.6237
0         train   57700/102467 356:38/276:42      1.368         {"mlm": 1.3270381261266957, "mse": 0.0}  0.633
0         train   57800/102467 357:14/276:04      1.368         {"mlm": 1.329162124235496, "mse": 0.0}  0.6554
0         train   57900/102467 357:50/275:26      1.367         {"mlm": 1.3298465523772574, "mse": 0.0}  1.1465
0         train   58000/102467 358:26/274:48      1.367         {"mlm": 1.330182175816568, "mse": 0.0}  0.6497
0         train   58100/102467 359:02/274:10      1.367         {"mlm": 1.3923517037183046, "mse": 0.0}  0.5815
0         train   58200/102467 359:37/273:32      1.367         {"mlm": 1.3624227360195043, "mse": 0.0}  0.4958
0         train   58300/102467 360:13/272:54      1.367         {"mlm": 1.3547561970111486, "mse": 0.0}  0.5171
0         train   58400/102467 360:49/272:16      1.367         {"mlm": 1.3470813228626444, "mse": 0.0}  0.6273
0         train   58500/102467 361:25/271:37      1.367         {"mlm": 1.3420529053095849, "mse": 0.0}  1.1944
0         train   58600/102467 362:00/270:59      1.368         {"mlm": 1.4199940126214252, "mse": 0.0}  152.936
0         train   58700/102467 362:36/270:21      1.374         {"mlm": 1.8881346311034828, "mse": 0.0}  36.5821
0         train   58800/102467 363:12/269:43      1.375         {"mlm": 1.8944853662545957, "mse": 0.0}  4.6732
0         train   58900/102467 363:48/269:05      1.375         {"mlm": 1.8429576316183167, "mse": 0.0}  1.6146
0         train   59000/102467 364:23/268:27      1.375         {"mlm": 1.8117306243224316, "mse": 0.0}  26.6206
0         train   59100/102467 364:59/267:49      1.375         {"mlm": 1.7863444921961666, "mse": 0.0}  1.8325
0         train   59200/102467 365:35/267:11      1.375         {"mlm": 1.7653549610571717, "mse": 0.0}  6.8161
0         train   59300/102467 366:11/266:33      1.381         {"mlm": 1.9861998030065018, "mse": 0.0}  0.3035
0         train   59400/102467 366:46/265:55      1.392         {"mlm": 2.3977655092078156, "mse": 0.0}  0.1913
0         train   59500/102467 367:22/265:17      1.402         {"mlm": 2.7549533851962678, "mse": 0.0}  0.1976
0         train   59600/102467 367:58/264:39      1.413         {"mlm": 3.066148509507191, "mse": 0.0}  0.149
0         train   59700/102467 368:33/264:01      1.423         {"mlm": 3.3411321329339496, "mse": 0.0}  0.1266
0         train   59800/102467 369:09/263:23      1.434         {"mlm": 3.585923744867533, "mse": 0.0}  0.1098
0         train   59900/102467 369:45/262:45      1.445         {"mlm": 3.804254322741102, "mse": 0.0}  0.1263
0         train   60000/102467 370:21/262:07      1.455         {"mlm": 4.0006369114638805, "mse": 0.0}  0.1174

10/04/2022 20:36:57 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step60000.pkl
0         valid   1/781        0:10/137:57        7.673           None
0         valid   101/781      0:25/ 2:53         7.706           None
0         valid   201/781      0:41/ 1:58         7.719           None
0         valid   301/781      0:56/ 1:29         7.726           None
0         valid   401/781      1:11/ 1:07         7.726           None
0         valid   501/781      1:26/ 0:48         7.725           None
0         valid   601/781      1:41/ 0:30         7.724           None
0         valid   701/781      1:57/ 0:13         7.725           None
0         valid   781/781      2:12/ 0:00         7.724         {"mlm": 7.7234314980793854, "mse": 0.0, "train": 0.0}  None
0         train   60100/102467 373:09/263:03      1.465         {"mlm": 7.725113024711609, "mse": 0.0}  0.1083
0         train   60200/102467 373:45/262:25      1.476         {"mlm": 7.719300618171692, "mse": 0.0}  0.115
0         train   60300/102467 374:21/261:46      1.486         {"mlm": 7.716240475972493, "mse": 0.0}  0.1127
0         train   60400/102467 374:56/261:08      1.496         {"mlm": 7.717906997203827, "mse": 0.0}  0.0964
0         train   60500/102467 375:32/260:29      1.507         {"mlm": 7.721939390182495, "mse": 0.0}  0.0969
0         train   60600/102467 376:07/259:51      1.517         {"mlm": 7.720853806336721, "mse": 0.0}  0.0969
0         train   60700/102467 376:43/259:13      1.527         {"mlm": 7.720859061649867, "mse": 0.0}  0.1022
0         train   60800/102467 377:18/258:34      1.537         {"mlm": 7.7209205228090285, "mse": 0.0}  0.0993
0         train   60900/102467 377:54/257:56      1.548         {"mlm": 7.721439150174459, "mse": 0.0}  0.0899
0         train   61000/102467 378:30/257:18      1.558         {"mlm": 7.720098306179047, "mse": 0.0}  0.1048
0         train   61100/102467 379:05/256:39      1.568         {"mlm": 7.71760475765575, "mse": 0.0}  0.114
0         train   61200/102467 379:41/256:01      1.578         {"mlm": 7.719988114833832, "mse": 0.0}  0.0989
0         train   61300/102467 380:16/255:22      1.588         {"mlm": 7.720065330358652, "mse": 0.0}  0.1061
0         train   61400/102467 380:52/254:44      1.598         {"mlm": 7.720728354794638, "mse": 0.0}  0.0816
0         train   61500/102467 381:27/254:06      1.608         {"mlm": 7.720786057472229, "mse": 0.0}  0.098
0         train   61600/102467 382:03/253:27      1.618         {"mlm": 7.7219478395581245, "mse": 0.0}  0.0804
0         train   61700/102467 382:39/252:49      1.628         {"mlm": 7.721872979332419, "mse": 0.0}  0.1015
0         train   61800/102467 383:14/252:11      1.638         {"mlm": 7.722320708963606, "mse": 0.0}  0.081
0         train   61900/102467 383:50/251:33      1.647         {"mlm": 7.722975238247922, "mse": 0.0}  0.1056
0         train   62000/102467 384:25/250:54      1.657         {"mlm": 7.7221339194774625, "mse": 0.0}  0.0873
0         train   62100/102467 385:01/250:16      1.667         {"mlm": 7.72780140481814, "mse": 0.0}  0.2329
0         train   62200/102467 385:37/249:38      1.677         {"mlm": 7.727773541781171, "mse": 0.0}  0.1159
0         train   62300/102467 386:12/249:00      1.686         {"mlm": 7.721242510754129, "mse": 0.0}  0.0911
0         train   62400/102467 386:48/248:22      1.696         {"mlm": 7.7164107683607215, "mse": 0.0}  0.0785
0         train   62500/102467 387:24/247:43      1.706         {"mlm": 7.715114274340307, "mse": 0.0}  0.0936
0         train   62600/102467 387:59/247:05      1.715         {"mlm": 7.713553051319663, "mse": 0.0}  0.0803
0         train   62700/102467 388:35/246:27      1.725         {"mlm": 7.71161875226808, "mse": 0.0}  0.081
0         train   62800/102467 389:11/245:49      1.734         {"mlm": 7.714899716001279, "mse": 0.0}  0.0887
0         train   62900/102467 389:46/245:11      1.744         {"mlm": 7.71496873437629, "mse": 0.0}  0.0777
0         train   63000/102467 390:22/244:33      1.753         {"mlm": 7.715418654757816, "mse": 0.0}  0.0833
0         train   63100/102467 390:57/243:54      1.763         {"mlm": 7.7147433503960565, "mse": 0.0}  0.0801
0         train   63200/102467 391:33/243:16      1.772         {"mlm": 7.71675769441619, "mse": 0.0}  0.0823
0         train   63300/102467 392:09/242:38      1.782         {"mlm": 7.718494667834369, "mse": 0.0}  0.1151
0         train   63400/102467 392:44/242:00      1.791         {"mlm": 7.718773077691428, "mse": 0.0}  0.0703
0         train   63500/102467 393:20/241:22      1.800         {"mlm": 7.71731074290565, "mse": 0.0}  0.08
0         train   63600/102467 393:55/240:44      1.810         {"mlm": 7.719084155790652, "mse": 0.0}  0.0814
0         train   63700/102467 394:31/240:06      1.819         {"mlm": 7.718630425013676, "mse": 0.0}  0.0922
0         train   63800/102467 395:06/239:27      1.828         {"mlm": 7.718572513735116, "mse": 0.0}  0.0746
0         train   63900/102467 395:42/238:49      1.837         {"mlm": 7.719021435597496, "mse": 0.0}  0.0762
0         train   64000/102467 396:17/238:11      1.847         {"mlm": 7.71867370915568, "mse": 0.0}  0.0652
0         train   64100/102467 396:53/237:33      1.856         {"mlm": 7.718913623264858, "mse": 0.0}  0.08
0         train   64200/102467 397:29/236:55      1.865         {"mlm": 7.716859381608288, "mse": 0.0}  0.0764
0         train   64300/102467 398:04/236:17      1.874         {"mlm": 7.714610994262183, "mse": 0.0}  0.0876
0         train   64400/102467 398:40/235:39      1.883         {"mlm": 7.712122236663972, "mse": 0.0}  0.0694
0         train   64500/102467 399:15/235:01      1.892         {"mlm": 7.709893035122668, "mse": 0.0}  0.0832
0         train   64600/102467 399:51/234:23      1.901         {"mlm": 7.715733908490592, "mse": 0.0}  0.0714
0         train   64700/102467 400:27/233:45      1.910         {"mlm": 7.717446988496534, "mse": 0.0}  0.0913
0         train   64800/102467 401:02/233:07      1.919         {"mlm": 7.719336783975587, "mse": 0.0}  0.0731
0         train   64900/102467 401:38/232:29      1.928         {"mlm": 7.720255038786041, "mse": 0.0}  0.091
0         train   65000/102467 402:13/231:51      1.937         {"mlm": 7.719649456784816, "mse": 0.0}  0.0801
0         train   65100/102467 402:49/231:13      1.946         {"mlm": 7.721713052638458, "mse": 0.0}  0.0886
0         train   65200/102467 403:25/230:35      1.955         {"mlm": 7.723166762289898, "mse": 0.0}  0.0752
0         train   65300/102467 404:00/229:57      1.964         {"mlm": 7.720741281156731, "mse": 0.0}  0.0696
0         train   65400/102467 404:36/229:19      1.972         {"mlm": 7.719640454850313, "mse": 0.0}  0.0672
0         train   65500/102467 405:11/228:41      1.981         {"mlm": 7.718266304726594, "mse": 0.0}  0.0725
0         train   65600/102467 405:47/228:03      1.990         {"mlm": 7.717660786064157, "mse": 0.0}  0.0662
0         train   65700/102467 406:22/227:25      1.999         {"mlm": 7.717864407807834, "mse": 0.0}  0.0667
0         train   65800/102467 406:58/226:47      2.007         {"mlm": 7.7177043838946515, "mse": 0.0}  0.0667
0         train   65900/102467 407:34/226:09      2.016         {"mlm": 7.717694675206384, "mse": 0.0}  0.0688
0         train   66000/102467 408:09/225:31      2.025         {"mlm": 7.718119957544902, "mse": 0.0}  0.072
0         train   66100/102467 408:45/224:53      2.033         {"mlm": 7.711065439833808, "mse": 0.0}  0.0942
0         train   66200/102467 409:20/224:15      2.042         {"mlm": 7.716559061544196, "mse": 0.0}  0.0839
0         train   66300/102467 409:56/223:37      2.050         {"mlm": 7.721193127359204, "mse": 0.0}  0.0947
0         train   66400/102467 410:31/222:59      2.059         {"mlm": 7.725113652515171, "mse": 0.0}  0.0967
0         train   66500/102467 411:07/222:21      2.067         {"mlm": 7.725512335717798, "mse": 0.0}  0.07
0         train   66600/102467 411:43/221:43      2.076         {"mlm": 7.727011424213199, "mse": 0.0}  0.0687
0         train   66700/102467 412:18/221:05      2.084         {"mlm": 7.727337011476842, "mse": 0.0}  0.0717
0         train   66800/102467 412:54/220:28      2.093         {"mlm": 7.729341435761493, "mse": 0.0}  0.0798
0         train   66900/102467 413:30/219:50      2.101         {"mlm": 7.730992337400166, "mse": 0.0}  0.0701
0         train   67000/102467 414:05/219:12      2.110         {"mlm": 7.728272163521203, "mse": 0.0}  0.0663
0         train   67100/102467 414:41/218:34      2.118         {"mlm": 7.727805535360804, "mse": 0.0}  0.0679
0         train   67200/102467 415:17/217:56      2.126         {"mlm": 7.726837382878277, "mse": 0.0}  0.0686
0         train   67300/102467 415:52/217:18      2.135         {"mlm": 7.726361699718279, "mse": 0.0}  0.0683
0         train   67400/102467 416:28/216:40      2.143         {"mlm": 7.725455854138734, "mse": 0.0}  0.0675
0         train   67500/102467 417:03/216:03      2.151         {"mlm": 7.726678515722852, "mse": 0.0}  0.094
0         train   67600/102467 417:39/215:25      2.159         {"mlm": 7.726000713570534, "mse": 0.0}  0.0672
0         train   67700/102467 418:15/214:47      2.168         {"mlm": 7.724090720880853, "mse": 0.0}  0.058
0         train   67800/102467 418:50/214:09      2.176         {"mlm": 7.723519483671364, "mse": 0.0}  0.0701
0         train   67900/102467 419:26/213:31      2.184         {"mlm": 7.721349898173926, "mse": 0.0}  0.069
0         train   68000/102467 420:01/212:53      2.192         {"mlm": 7.721750711403791, "mse": 0.0}  0.0744
0         train   68100/102467 420:37/212:16      2.200         {"mlm": 7.72403688232104, "mse": 0.0}  0.0656
0         train   68200/102467 421:12/211:38      2.208         {"mlm": 7.719699241677109, "mse": 0.0}  0.0762
0         train   68300/102467 421:48/211:00      2.216         {"mlm": 7.728325859920399, "mse": 0.0}  0.0781
0         train   68400/102467 422:24/210:22      2.224         {"mlm": 7.72946326539974, "mse": 0.0}  0.0775
0         train   68500/102467 422:59/209:44      2.232         {"mlm": 7.72775528892394, "mse": 0.0}  0.0799
0         train   68600/102467 423:35/209:07      2.240         {"mlm": 7.721825270044723, "mse": 0.0}  0.0718
0         train   68700/102467 424:10/208:29      2.248         {"mlm": 7.724438496019649, "mse": 0.0}  0.0781
0         train   68800/102467 424:46/207:51      2.256         {"mlm": 7.724049243495692, "mse": 0.0}  0.0749
0         train   68900/102467 425:21/207:13      2.264         {"mlm": 7.722076248909746, "mse": 0.0}  0.0628
0         train   69000/102467 425:57/206:36      2.272         {"mlm": 7.720737695215218, "mse": 0.0}  0.0723
0         train   69100/102467 426:33/205:58      2.280         {"mlm": 7.721948409602589, "mse": 0.0}  0.0669
0         train   69200/102467 427:08/205:20      2.288         {"mlm": 7.719578494196353, "mse": 0.0}  0.0613
0         train   69300/102467 427:44/204:43      2.296         {"mlm": 7.719395261119913, "mse": 0.0}  0.0684
0         train   69400/102467 428:20/204:05      2.304         {"mlm": 7.721347871345913, "mse": 0.0}  0.0674
0         train   69500/102467 428:55/203:27      2.311         {"mlm": 7.72074397203119, "mse": 0.0}  0.0838
0         train   69600/102467 429:31/202:49      2.319         {"mlm": 7.721610086006031, "mse": 0.0}  0.0848
0         train   69700/102467 430:07/202:12      2.327         {"mlm": 7.722677157172617, "mse": 0.0}  0.1183
0         train   69800/102467 430:42/201:34      2.335         {"mlm": 7.722475437385202, "mse": 0.0}  0.0868
0         train   69900/102467 431:18/200:56      2.342         {"mlm": 7.7231358324928125, "mse": 0.0}  0.0768
0         train   70000/102467 431:53/200:19      2.350         {"mlm": 7.723058675955198, "mse": 0.0}  0.068

10/04/2022 21:38:30 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step70000.pkl
0         valid   1/781        0:10/139:29        7.759           None
0         valid   101/781      0:25/ 2:54         7.694           None
0         valid   201/781      0:41/ 1:58         7.712           None
0         valid   301/781      0:56/ 1:30         7.714           None
0         valid   401/781      1:11/ 1:07         7.725           None
0         valid   501/781      1:26/ 0:48         7.724           None
0         valid   601/781      1:42/ 0:30         7.722           None
0         valid   701/781      1:57/ 0:13         7.726           None
0         valid   781/781      2:13/ 0:00         7.721         {"mlm": 7.720550576184379, "mse": 0.0, "train": 0.0}  None
0         train   70100/102467 434:42/200:43      2.358         {"mlm": 7.712127084732056, "mse": 0.0}  0.0692
0         train   70200/102467 435:18/200:05      2.365         {"mlm": 7.731766965389252, "mse": 0.0}  0.068
0         train   70300/102467 435:54/199:27      2.373         {"mlm": 7.724844598770142, "mse": 0.0}  0.0679
0         train   70400/102467 436:29/198:49      2.381         {"mlm": 7.720547649860382, "mse": 0.0}  0.0783
0         train   70500/102467 437:05/198:11      2.388         {"mlm": 7.71707405090332, "mse": 0.0}  0.0788
0         train   70600/102467 437:40/197:33      2.396         {"mlm": 7.7190431849161785, "mse": 0.0}  0.069
0         train   70700/102467 438:16/196:55      2.403         {"mlm": 7.720775709152222, "mse": 0.0}  0.0775
0         train   70800/102467 438:51/196:17      2.411         {"mlm": 7.719008242487908, "mse": 0.0}  0.0663
0         train   70900/102467 439:27/195:39      2.418         {"mlm": 7.718134979671902, "mse": 0.0}  0.0703
0         train   71000/102467 440:03/195:01      2.426         {"mlm": 7.717858752250671, "mse": 0.0}  0.0667
0         train   71100/102467 440:38/194:23      2.433         {"mlm": 7.719232659339905, "mse": 0.0}  0.0641
0         train   71200/102467 441:14/193:46      2.441         {"mlm": 7.720077781677246, "mse": 0.0}  0.0644
0         train   71300/102467 441:50/193:08      2.448         {"mlm": 7.721979442009559, "mse": 0.0}  0.0638
0         train   71400/102467 442:25/192:30      2.455         {"mlm": 7.722590625967298, "mse": 0.0}  0.0724
0         train   71500/102467 443:01/191:52      2.463         {"mlm": 7.723313029607137, "mse": 0.0}  0.0658
0         train   71600/102467 443:37/191:14      2.470         {"mlm": 7.722633529901504, "mse": 0.0}  0.0743
0         train   71700/102467 444:12/190:36      2.478         {"mlm": 7.723106090040768, "mse": 0.0}  0.0631
0         train   71800/102467 444:48/189:59      2.485         {"mlm": 7.723022241062588, "mse": 0.0}  0.0807
0         train   71900/102467 445:23/189:21      2.492         {"mlm": 7.723691740286978, "mse": 0.0}  0.0954
0         train   72000/102467 445:59/188:43      2.499         {"mlm": 7.722945508718491, "mse": 0.0}  0.0626
0         train   72100/102467 446:35/188:05      2.507         {"mlm": 7.72376738172589, "mse": 0.0}  0.0646
0         train   72200/102467 447:10/187:27      2.514         {"mlm": 7.72824634379478, "mse": 0.0}  0.0652
0         train   72300/102467 447:46/186:49      2.521         {"mlm": 7.734859729690297, "mse": 0.0}  0.0719
0         train   72400/102467 448:21/186:12      2.528         {"mlm": 7.728283904847645, "mse": 0.0}  0.0888
0         train   72500/102467 448:57/185:34      2.535         {"mlm": 7.725306674330412, "mse": 0.0}  0.0655
0         train   72600/102467 449:32/184:56      2.543         {"mlm": 7.725236922950299, "mse": 0.0}  0.0664
0         train   72700/102467 450:08/184:18      2.550         {"mlm": 7.7250719295550825, "mse": 0.0}  0.0611
0         train   72800/102467 450:43/183:40      2.557         {"mlm": 7.7225516686899045, "mse": 0.0}  0.059
0         train   72900/102467 451:19/183:03      2.564         {"mlm": 7.7227136614060115, "mse": 0.0}  0.0609
0         train   73000/102467 451:55/182:25      2.571         {"mlm": 7.7229315802619025, "mse": 0.0}  0.0822
0         train   73100/102467 452:30/181:47      2.578         {"mlm": 7.722393379089938, "mse": 0.0}  0.0744
0         train   73200/102467 453:06/181:09      2.585         {"mlm": 7.720483964835733, "mse": 0.0}  0.0731
0         train   73300/102467 453:41/180:31      2.592         {"mlm": 7.720517973426308, "mse": 0.0}  0.0659
0         train   73400/102467 454:17/179:54      2.599         {"mlm": 7.7212231621050345, "mse": 0.0}  0.0599
0         train   73500/102467 454:52/179:16      2.606         {"mlm": 7.72170746143537, "mse": 0.0}  0.0637
0         train   73600/102467 455:28/178:38      2.613         {"mlm": 7.721873665690944, "mse": 0.0}  0.0706
0         train   73700/102467 456:04/178:00      2.620         {"mlm": 7.720415284031626, "mse": 0.0}  0.0684
0         train   73800/102467 456:39/177:23      2.627         {"mlm": 7.719100562514962, "mse": 0.0}  0.0675
0         train   73900/102467 457:15/176:45      2.634         {"mlm": 7.7175668843989, "mse": 0.0}  0.0637
0         train   74000/102467 457:51/176:07      2.640         {"mlm": 7.717803519508014, "mse": 0.0}  0.0568
0         train   74100/102467 458:26/175:30      2.647         {"mlm": 7.6958847386496405, "mse": 0.0}  0.0586
0         train   74200/102467 459:02/174:52      2.654         {"mlm": 7.716110817109696, "mse": 0.0}  0.0827
0         train   74300/102467 459:37/174:14      2.661         {"mlm": 7.723033663410469, "mse": 0.0}  0.0658
0         train   74400/102467 460:13/173:36      2.668         {"mlm": 7.719671201466316, "mse": 0.0}  0.0772
0         train   74500/102467 460:48/172:59      2.674         {"mlm": 7.720821777022029, "mse": 0.0}  0.0618
0         train   74600/102467 461:24/172:21      2.681         {"mlm": 7.723968559284274, "mse": 0.0}  0.0716
0         train   74700/102467 461:59/171:43      2.688         {"mlm": 7.724767909692146, "mse": 0.0}  0.0618
0         train   74800/102467 462:35/171:06      2.695         {"mlm": 7.723168105409856, "mse": 0.0}  0.0625
0         train   74900/102467 463:11/170:28      2.701         {"mlm": 7.722536518207372, "mse": 0.0}  0.0699
0         train   75000/102467 463:46/169:50      2.708         {"mlm": 7.72210666364085, "mse": 0.0}  0.082
0         train   75100/102467 464:22/169:13      2.715         {"mlm": 7.719065300970998, "mse": 0.0}  0.0585
0         train   75200/102467 464:57/168:35      2.721         {"mlm": 7.71880450949247, "mse": 0.0}  0.0637
0         train   75300/102467 465:33/167:57      2.728         {"mlm": 7.719055983979823, "mse": 0.0}  0.0561
0         train   75400/102467 466:08/167:20      2.735         {"mlm": 7.719803873902568, "mse": 0.0}  0.0677
0         train   75500/102467 466:44/166:42      2.741         {"mlm": 7.718483678489247, "mse": 0.0}  0.068
0         train   75600/102467 467:19/166:04      2.748         {"mlm": 7.717663785542952, "mse": 0.0}  0.0708
0         train   75700/102467 467:55/165:27      2.754         {"mlm": 7.717669451335013, "mse": 0.0}  0.0603
0         train   75800/102467 468:31/164:49      2.761         {"mlm": 7.719087962181337, "mse": 0.0}  0.063
0         train   75900/102467 469:06/164:12      2.768         {"mlm": 7.719810830026834, "mse": 0.0}  0.0564
0         train   76000/102467 469:42/163:34      2.774         {"mlm": 7.722019239946887, "mse": 0.0}  0.0655
0         train   76100/102467 470:17/162:56      2.781         {"mlm": 7.719546735901194, "mse": 0.0}  0.0688
0         train   76200/102467 470:53/162:19      2.787         {"mlm": 7.7158948225418325, "mse": 0.0}  0.0717
0         train   76300/102467 471:28/161:41      2.794         {"mlm": 7.724993839007034, "mse": 0.0}  0.0743
0         train   76400/102467 472:04/161:04      2.800         {"mlm": 7.724473077644329, "mse": 0.0}  0.0708
0         train   76500/102467 472:40/160:26      2.806         {"mlm": 7.722186790865432, "mse": 0.0}  0.057
0         train   76600/102467 473:15/159:48      2.813         {"mlm": 7.720152134472002, "mse": 0.0}  0.0575
0         train   76700/102467 473:51/159:11      2.819         {"mlm": 7.721495860276297, "mse": 0.0}  0.0655
0         train   76800/102467 474:26/158:33      2.826         {"mlm": 7.721119032300999, "mse": 0.0}  0.0628
0         train   76900/102467 475:02/157:56      2.832         {"mlm": 7.722352133679682, "mse": 0.0}  0.0664
0         train   77000/102467 475:38/157:18      2.838         {"mlm": 7.720776246089036, "mse": 0.0}  0.0573
0         train   77100/102467 476:13/156:41      2.845         {"mlm": 7.722690194546361, "mse": 0.0}  0.0628
0         train   77200/102467 476:49/156:03      2.851         {"mlm": 7.721977040283662, "mse": 0.0}  0.0602
0         train   77300/102467 477:24/155:26      2.857         {"mlm": 7.7224277288250125, "mse": 0.0}  0.0695
0         train   77400/102467 478:00/154:48      2.864         {"mlm": 7.7211709595955345, "mse": 0.0}  0.0691
0         train   77500/102467 478:35/154:10      2.870         {"mlm": 7.721278551824107, "mse": 0.0}  0.0702
0         train   77600/102467 479:11/153:33      2.876         {"mlm": 7.721650185104303, "mse": 0.0}  0.0689
0         train   77700/102467 479:46/152:55      2.882         {"mlm": 7.720137303621542, "mse": 0.0}  0.0876
0         train   77800/102467 480:22/152:18      2.889         {"mlm": 7.718883553676361, "mse": 0.0}  0.062
0         train   77900/102467 480:58/151:40      2.895         {"mlm": 7.718894574412183, "mse": 0.0}  0.0822
0         train   78000/102467 481:33/151:03      2.901         {"mlm": 7.719504196881412, "mse": 0.0}  0.0612
0         train   78100/102467 482:09/150:25      2.907         {"mlm": 7.730739012360573, "mse": 0.0}  0.0595
0         train   78200/102467 482:44/149:48      2.913         {"mlm": 7.730772154671805, "mse": 0.0}  0.0746
0         train   78300/102467 483:20/149:10      2.919         {"mlm": 7.7222310449626, "mse": 0.0}  0.0662
0         train   78400/102467 483:55/148:33      2.926         {"mlm": 7.720284452342024, "mse": 0.0}  0.0587
0         train   78500/102467 484:31/147:55      2.932         {"mlm": 7.722514295770276, "mse": 0.0}  0.0588
0         train   78600/102467 485:06/147:18      2.938         {"mlm": 7.71941647913632, "mse": 0.0}  0.0618
0         train   78700/102467 485:42/146:40      2.944         {"mlm": 7.7241695399942065, "mse": 0.0}  0.0746
0         train   78800/102467 486:17/146:03      2.950         {"mlm": 7.723445783308403, "mse": 0.0}  0.0673
0         train   78900/102467 486:53/145:25      2.956         {"mlm": 7.722446354904345, "mse": 0.0}  0.0624
0         train   79000/102467 487:29/144:48      2.962         {"mlm": 7.720459822191292, "mse": 0.0}  0.0662
0         train   79100/102467 488:04/144:11      2.968         {"mlm": 7.72002790705131, "mse": 0.0}  0.0716
0         train   79200/102467 488:40/143:33      2.974         {"mlm": 7.719708604956152, "mse": 0.0}  0.0905
0         train   79300/102467 489:15/142:56      2.980         {"mlm": 7.720241972693691, "mse": 0.0}  0.0729
0         train   79400/102467 489:51/142:18      2.986         {"mlm": 7.720391897895616, "mse": 0.0}  0.0807
0         train   79500/102467 490:27/141:41      2.992         {"mlm": 7.720069841905073, "mse": 0.0}  0.0652
0         train   79600/102467 491:02/141:03      2.998         {"mlm": 7.721270013273808, "mse": 0.0}  0.0753
0         train   79700/102467 491:38/140:26      3.004         {"mlm": 7.72144859916759, "mse": 0.0}  0.0725
0         train   79800/102467 492:14/139:49      3.010         {"mlm": 7.7224405048153715, "mse": 0.0}  0.0591
0         train   79900/102467 492:49/139:11      3.016         {"mlm": 7.723112929973924, "mse": 0.0}  0.0598
0         train   80000/102467 493:25/138:34      3.021         {"mlm": 7.722532446972115, "mse": 0.0}  0.0653

10/04/2022 22:40:01 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step80000.pkl
0         valid   1/781        0:10/139:40        7.662           None
0         valid   101/781      0:25/ 2:54         7.709           None
0         valid   201/781      0:41/ 1:58         7.715           None
0         valid   301/781      0:56/ 1:29         7.718           None
0         valid   401/781      1:11/ 1:07         7.720           None
0         valid   501/781      1:26/ 0:48         7.721           None
0         valid   601/781      1:41/ 0:30         7.721           None
0         valid   701/781      1:57/ 0:13         7.725           None
0         valid   781/781      2:13/ 0:00         7.721         {"mlm": 7.721190781049936, "mse": 0.0, "train": 0.0}  None
0         train   80100/102467 496:14/138:34      3.027         {"mlm": 7.721660871505737, "mse": 0.0}  0.0731
0         train   80200/102467 496:49/137:56      3.033         {"mlm": 7.718121566772461, "mse": 0.0}  0.0776
0         train   80300/102467 497:25/137:18      3.039         {"mlm": 7.72061593691508, "mse": 0.0}  0.0605
0         train   80400/102467 498:00/136:41      3.045         {"mlm": 7.721445072889328, "mse": 0.0}  0.1108
0         train   80500/102467 498:36/136:03      3.051         {"mlm": 7.71876776599884, "mse": 0.0}  0.0657
0         train   80600/102467 499:11/135:25      3.056         {"mlm": 7.7185836911201475, "mse": 0.0}  0.0712
0         train   80700/102467 499:47/134:48      3.062         {"mlm": 7.719896555628095, "mse": 0.0}  0.0588
0         train   80800/102467 500:22/134:10      3.068         {"mlm": 7.721309300661087, "mse": 0.0}  0.0768
0         train   80900/102467 500:58/133:33      3.074         {"mlm": 7.720730770958794, "mse": 0.0}  0.0554
0         train   81000/102467 501:34/132:55      3.079         {"mlm": 7.720120606422424, "mse": 0.0}  0.0627
0         train   81100/102467 502:09/132:18      3.085         {"mlm": 7.720213330008767, "mse": 0.0}  0.0618
0         train   81200/102467 502:45/131:40      3.091         {"mlm": 7.716479584376017, "mse": 0.0}  0.059
0         train   81300/102467 503:20/131:02      3.097         {"mlm": 7.716895438340994, "mse": 0.0}  0.0647
0         train   81400/102467 503:56/130:25      3.102         {"mlm": 7.716449151039123, "mse": 0.0}  0.0683
0         train   81500/102467 504:32/129:47      3.108         {"mlm": 7.715061829884847, "mse": 0.0}  0.0889
0         train   81600/102467 505:07/129:10      3.114         {"mlm": 7.7156242457032205, "mse": 0.0}  0.0594
0         train   81700/102467 505:43/128:32      3.119         {"mlm": 7.716221183328067, "mse": 0.0}  0.0585
0         train   81800/102467 506:18/127:55      3.125         {"mlm": 7.7171182211240135, "mse": 0.0}  0.0648
0         train   81900/102467 506:54/127:17      3.130         {"mlm": 7.716831117429232, "mse": 0.0}  0.0555
0         train   82000/102467 507:30/126:40      3.136         {"mlm": 7.716329559087753, "mse": 0.0}  0.0636
0         train   82100/102467 508:05/126:02      3.142         {"mlm": 7.726677716380418, "mse": 0.0}  0.0648
0         train   82200/102467 508:41/125:25      3.147         {"mlm": 7.732123708006125, "mse": 0.0}  0.0549
0         train   82300/102467 509:17/124:47      3.153         {"mlm": 7.725993350995425, "mse": 0.0}  0.0604
0         train   82400/102467 509:52/124:10      3.158         {"mlm": 7.724704226156823, "mse": 0.0}  0.0666
0         train   82500/102467 510:28/123:32      3.164         {"mlm": 7.721322875701354, "mse": 0.0}  0.072
0         train   82600/102467 511:04/122:55      3.169         {"mlm": 7.720095474453323, "mse": 0.0}  0.0765
0         train   82700/102467 511:39/122:17      3.175         {"mlm": 7.71926517377425, "mse": 0.0}  0.0656
0         train   82800/102467 512:15/121:40      3.180         {"mlm": 7.7185263168229925, "mse": 0.0}  0.0656
0         train   82900/102467 512:50/121:02      3.186         {"mlm": 7.719731368530579, "mse": 0.0}  0.0686
0         train   83000/102467 513:26/120:25      3.191         {"mlm": 7.720299709785928, "mse": 0.0}  0.0562
0         train   83100/102467 514:01/119:47      3.197         {"mlm": 7.72016627491334, "mse": 0.0}  0.0592
0         train   83200/102467 514:37/119:10      3.202         {"mlm": 7.720022670023634, "mse": 0.0}  0.0623
0         train   83300/102467 515:13/118:32      3.207         {"mlm": 7.718647524060608, "mse": 0.0}  0.0583
0         train   83400/102467 515:48/117:55      3.213         {"mlm": 7.7193821194003185, "mse": 0.0}  0.0597
0         train   83500/102467 516:24/117:18      3.218         {"mlm": 7.719155309675852, "mse": 0.0}  0.062
0         train   83600/102467 516:59/116:40      3.224         {"mlm": 7.719177279493226, "mse": 0.0}  0.0709
0         train   83700/102467 517:35/116:03      3.229         {"mlm": 7.719152662177308, "mse": 0.0}  0.0548
0         train   83800/102467 518:10/115:25      3.234         {"mlm": 7.719440384663894, "mse": 0.0}  0.0752
0         train   83900/102467 518:46/114:48      3.240         {"mlm": 7.719258957501774, "mse": 0.0}  0.069
0         train   84000/102467 519:21/114:10      3.245         {"mlm": 7.718382806525104, "mse": 0.0}  0.0664
0         train   84100/102467 519:57/113:33      3.250         {"mlm": 7.735649488410171, "mse": 0.0}  0.0713
0         train   84200/102467 520:33/112:55      3.256         {"mlm": 7.721249618915596, "mse": 0.0}  0.0582
0         train   84300/102467 521:08/112:18      3.261         {"mlm": 7.7223740008053365, "mse": 0.0}  0.0598
0         train   84400/102467 521:44/111:41      3.266         {"mlm": 7.716667938472039, "mse": 0.0}  0.0709
0         train   84500/102467 522:19/111:03      3.272         {"mlm": 7.718960927672176, "mse": 0.0}  0.0631
0         train   84600/102467 522:55/110:26      3.277         {"mlm": 7.720847017390273, "mse": 0.0}  0.0598
0         train   84700/102467 523:31/109:48      3.282         {"mlm": 7.719895829443945, "mse": 0.0}  0.0581
0         train   84800/102467 524:06/109:11      3.287         {"mlm": 7.721307008188769, "mse": 0.0}  0.0665
0         train   84900/102467 524:42/108:34      3.293         {"mlm": 7.722340698497067, "mse": 0.0}  0.0542
0         train   85000/102467 525:18/107:56      3.298         {"mlm": 7.721006284973664, "mse": 0.0}  0.0591
0         train   85100/102467 525:53/107:19      3.303         {"mlm": 7.721061605790491, "mse": 0.0}  0.0643
0         train   85200/102467 526:29/106:42      3.308         {"mlm": 7.720888222995306, "mse": 0.0}  0.0512
0         train   85300/102467 527:05/106:04      3.313         {"mlm": 7.721653213486282, "mse": 0.0}  0.0684
0         train   85400/102467 527:40/105:27      3.318         {"mlm": 7.719767731828922, "mse": 0.0}  0.069
0         train   85500/102467 528:16/104:49      3.324         {"mlm": 7.720232233982061, "mse": 0.0}  0.068
0         train   85600/102467 528:52/104:12      3.329         {"mlm": 7.721172592666779, "mse": 0.0}  0.0601
0         train   85700/102467 529:27/103:35      3.334         {"mlm": 7.721309387782156, "mse": 0.0}  0.0819
0         train   85800/102467 530:03/102:57      3.339         {"mlm": 7.720186725209102, "mse": 0.0}  0.0657
0         train   85900/102467 530:38/102:20      3.344         {"mlm": 7.720888014714007, "mse": 0.0}  0.0633
0         train   86000/102467 531:14/101:43      3.349         {"mlm": 7.722564321857792, "mse": 0.0}  0.0785
0         train   86100/102467 531:50/101:05      3.354         {"mlm": 7.715933047619062, "mse": 0.0}  0.0649
0         train   86200/102467 532:25/100:28      3.359         {"mlm": 7.702072431593377, "mse": 0.0}  0.0606
0         train   86300/102467 533:01/99:51       3.364         {"mlm": 7.716064592804572, "mse": 0.0}  0.0602
0         train   86400/102467 533:36/99:13       3.369         {"mlm": 7.720967082592942, "mse": 0.0}  0.0624
0         train   86500/102467 534:12/98:36       3.374         {"mlm": 7.718231678968464, "mse": 0.0}  0.0535
0         train   86600/102467 534:47/97:59       3.379         {"mlm": 7.719551074444948, "mse": 0.0}  0.0621
0         train   86700/102467 535:23/97:21       3.384         {"mlm": 7.718078895141952, "mse": 0.0}  0.0664
0         train   86800/102467 535:59/96:44       3.389         {"mlm": 7.7175491427537635, "mse": 0.0}  0.0603
0         train   86900/102467 536:34/96:07       3.394         {"mlm": 7.717999966514017, "mse": 0.0}  0.0746
0         train   87000/102467 537:10/95:29       3.399         {"mlm": 7.718567253713503, "mse": 0.0}  0.0634
0         train   87100/102467 537:45/94:52       3.404         {"mlm": 7.72159756066697, "mse": 0.0}  0.0566
0         train   87200/102467 538:21/94:15       3.409         {"mlm": 7.720726810700712, "mse": 0.0}  0.0572
0         train   87300/102467 538:56/93:37       3.414         {"mlm": 7.719219989379553, "mse": 0.0}  0.0595
0         train   87400/102467 539:32/93:00       3.419         {"mlm": 7.718624416384086, "mse": 0.0}  0.0574
0         train   87500/102467 540:07/92:23       3.424         {"mlm": 7.718076209346692, "mse": 0.0}  0.0558
0         train   87600/102467 540:43/91:46       3.429         {"mlm": 7.717215532650407, "mse": 0.0}  0.0485
0         train   87700/102467 541:19/91:08       3.434         {"mlm": 7.717161062821685, "mse": 0.0}  0.0552
0         train   87800/102467 541:54/90:31       3.439         {"mlm": 7.718474715035958, "mse": 0.0}  0.0559
0         train   87900/102467 542:30/89:54       3.444         {"mlm": 7.719657752409569, "mse": 0.0}  0.0628
0         train   88000/102467 543:05/89:17       3.449         {"mlm": 7.718751756680507, "mse": 0.0}  0.058
0         train   88100/102467 543:41/88:39       3.453         {"mlm": 7.700040106972058, "mse": 0.0}  0.056
0         train   88200/102467 544:17/88:02       3.458         {"mlm": 7.706046486387447, "mse": 0.0}  0.0591
0         train   88300/102467 544:52/87:25       3.463         {"mlm": 7.706668365646053, "mse": 0.0}  0.0636
0         train   88400/102467 545:28/86:48       3.468         {"mlm": 7.7092929103157735, "mse": 0.0}  0.0604
0         train   88500/102467 546:03/86:10       3.473         {"mlm": 7.714898307477275, "mse": 0.0}  0.0561
0         train   88600/102467 546:39/85:33       3.477         {"mlm": 7.716240924476777, "mse": 0.0}  0.0805
0         train   88700/102467 547:15/84:56       3.482         {"mlm": 7.719734173396538, "mse": 0.0}  0.0566
0         train   88800/102467 547:50/84:19       3.487         {"mlm": 7.720935783793578, "mse": 0.0}  0.1103
0         train   88900/102467 548:26/83:41       3.492         {"mlm": 7.720606353133917, "mse": 0.0}  0.0597
0         train   89000/102467 549:01/83:04       3.497         {"mlm": 7.719157042752308, "mse": 0.0}  0.0551
0         train   89100/102467 549:37/82:27       3.501         {"mlm": 7.7203425282109395, "mse": 0.0}  0.0594
0         train   89200/102467 550:12/81:50       3.506         {"mlm": 7.718269006464394, "mse": 0.0}  0.0556
0         train   89300/102467 550:48/81:12       3.511         {"mlm": 7.721116966303484, "mse": 0.0}  0.0602
0         train   89400/102467 551:23/80:35       3.515         {"mlm": 7.720502953474706, "mse": 0.0}  0.0647
0         train   89500/102467 551:59/79:58       3.520         {"mlm": 7.719420318935007, "mse": 0.0}  0.0574
0         train   89600/102467 552:34/79:21       3.525         {"mlm": 7.719339062994285, "mse": 0.0}  0.056
0         train   89700/102467 553:10/78:43       3.529         {"mlm": 7.718315022452822, "mse": 0.0}  0.0705
0         train   89800/102467 553:45/78:06       3.534         {"mlm": 7.718083809107077, "mse": 0.0}  0.0806
0         train   89900/102467 554:21/77:29       3.539         {"mlm": 7.717187503973643, "mse": 0.0}  0.0645
0         train   90000/102467 554:57/76:52       3.543         {"mlm": 7.716297142969105, "mse": 0.0}  0.0621

10/04/2022 23:41:33 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step90000.pkl
0         valid   1/781        0:10/141:21        7.931           None
0         valid   101/781      0:26/ 2:55         7.699           None
0         valid   201/781      0:41/ 1:59         7.721           None
0         valid   301/781      0:56/ 1:30         7.719           None
0         valid   401/781      1:11/ 1:07         7.715           None
0         valid   501/781      1:26/ 0:48         7.712           None
0         valid   601/781      1:42/ 0:30         7.718           None
0         valid   701/781      1:57/ 0:13         7.718           None
0         valid   781/781      2:13/ 0:00         7.719         {"mlm": 7.719430217669654, "mse": 0.0, "train": 0.0}  None
0         train   90100/102467 557:46/76:33       3.548         {"mlm": 7.708663291931153, "mse": 0.0}  0.0633
0         train   90200/102467 558:21/75:56       3.553         {"mlm": 7.715900626182556, "mse": 0.0}  0.0702
0         train   90300/102467 558:57/75:18       3.557         {"mlm": 7.720106380780538, "mse": 0.0}  0.0593
0         train   90400/102467 559:33/74:41       3.562         {"mlm": 7.7211448609828945, "mse": 0.0}  0.0572
0         train   90500/102467 560:08/74:04       3.566         {"mlm": 7.720025513648987, "mse": 0.0}  0.0542
0         train   90600/102467 560:44/73:26       3.571         {"mlm": 7.716463841597239, "mse": 0.0}  0.0572
0         train   90700/102467 561:19/72:49       3.576         {"mlm": 7.716230844770159, "mse": 0.0}  0.0588
0         train   90800/102467 561:55/72:12       3.580         {"mlm": 7.71777981877327, "mse": 0.0}  0.0631
0         train   90900/102467 562:30/71:34       3.585         {"mlm": 7.716155231793722, "mse": 0.0}  0.0556
0         train   91000/102467 563:06/70:57       3.589         {"mlm": 7.7174582843780515, "mse": 0.0}  0.0558
0         train   91100/102467 563:41/70:20       3.594         {"mlm": 7.7188503130999475, "mse": 0.0}  0.0629
0         train   91200/102467 564:17/69:42       3.598         {"mlm": 7.718231023152669, "mse": 0.0}  0.0587
0         train   91300/102467 564:52/69:05       3.603         {"mlm": 7.716726945730356, "mse": 0.0}  0.0679
0         train   91400/102467 565:28/68:28       3.607         {"mlm": 7.717379268237523, "mse": 0.0}  0.0518
0         train   91500/102467 566:03/67:50       3.612         {"mlm": 7.717323562304179, "mse": 0.0}  0.054
0         train   91600/102467 566:39/67:13       3.616         {"mlm": 7.71690570384264, "mse": 0.0}  0.0623
0         train   91700/102467 567:14/66:36       3.621         {"mlm": 7.716791178198422, "mse": 0.0}  0.0567
0         train   91800/102467 567:50/65:58       3.625         {"mlm": 7.716703335179223, "mse": 0.0}  0.0603
0         train   91900/102467 568:25/65:21       3.630         {"mlm": 7.718597780779788, "mse": 0.0}  0.0577
0         train   92000/102467 569:01/64:44       3.634         {"mlm": 7.718258529424667, "mse": 0.0}  0.0658
0         train   92100/102467 569:36/64:07       3.639         {"mlm": 7.719279385576344, "mse": 0.0}  0.0667
0         train   92200/102467 570:12/63:29       3.643         {"mlm": 7.734602077522469, "mse": 0.0}  0.0616
0         train   92300/102467 570:47/62:52       3.647         {"mlm": 7.721346749908549, "mse": 0.0}  0.0511
0         train   92400/102467 571:23/62:15       3.652         {"mlm": 7.720832406428823, "mse": 0.0}  0.0555
0         train   92500/102467 571:58/61:37       3.656         {"mlm": 7.722409055323783, "mse": 0.0}  0.0547
0         train   92600/102467 572:34/61:00       3.661         {"mlm": 7.718679736174009, "mse": 0.0}  0.0553
0         train   92700/102467 573:09/60:23       3.665         {"mlm": 7.716548529476226, "mse": 0.0}  0.0631
0         train   92800/102467 573:45/59:46       3.669         {"mlm": 7.715499427948189, "mse": 0.0}  0.0592
0         train   92900/102467 574:20/59:08       3.674         {"mlm": 7.715023630585633, "mse": 0.0}  0.0741
0         train   93000/102467 574:56/58:31       3.678         {"mlm": 7.7187383754833325, "mse": 0.0}  0.0564
0         train   93100/102467 575:32/57:54       3.682         {"mlm": 7.718887638026091, "mse": 0.0}  0.0728
0         train   93200/102467 576:07/57:17       3.687         {"mlm": 7.717542721093745, "mse": 0.0}  0.0644
0         train   93300/102467 576:43/56:39       3.691         {"mlm": 7.716573861307874, "mse": 0.0}  0.0536
0         train   93400/102467 577:18/56:02       3.695         {"mlm": 7.716333918949807, "mse": 0.0}  0.053
0         train   93500/102467 577:54/55:25       3.700         {"mlm": 7.716245668104603, "mse": 0.0}  0.058
0         train   93600/102467 578:29/54:48       3.704         {"mlm": 7.716826687908829, "mse": 0.0}  0.0601
0         train   93700/102467 579:05/54:10       3.708         {"mlm": 7.7170365517668476, "mse": 0.0}  0.0552
0         train   93800/102467 579:40/53:33       3.712         {"mlm": 7.716123348742872, "mse": 0.0}  0.0647
0         train   93900/102467 580:16/52:56       3.717         {"mlm": 7.716451619536453, "mse": 0.0}  0.0535
0         train   94000/102467 580:52/52:19       3.721         {"mlm": 7.717253288786193, "mse": 0.0}  0.05
0         train   94100/102467 581:27/51:42       3.725         {"mlm": 7.708709512438093, "mse": 0.0}  0.051
0         train   94200/102467 582:03/51:04       3.729         {"mlm": 7.712081723743015, "mse": 0.0}  0.0685
0         train   94300/102467 582:38/50:27       3.734         {"mlm": 7.71312933960217, "mse": 0.0}  0.0483
0         train   94400/102467 583:14/49:50       3.738         {"mlm": 7.720292731146118, "mse": 0.0}  0.0609
0         train   94500/102467 583:49/49:13       3.742         {"mlm": 7.717838882921211, "mse": 0.0}  0.0567
0         train   94600/102467 584:25/48:36       3.746         {"mlm": 7.717370076323034, "mse": 0.0}  0.0574
0         train   94700/102467 585:00/47:58       3.751         {"mlm": 7.716781131176006, "mse": 0.0}  0.0559
0         train   94800/102467 585:36/47:21       3.755         {"mlm": 7.716613981060516, "mse": 0.0}  0.0547
0         train   94900/102467 586:11/46:44       3.759         {"mlm": 7.715935074141402, "mse": 0.0}  0.055
0         train   95000/102467 586:47/46:07       3.763         {"mlm": 7.715170840701025, "mse": 0.0}  0.0526
0         train   95100/102467 587:22/45:30       3.767         {"mlm": 7.714719715882738, "mse": 0.0}  0.058
0         train   95200/102467 587:58/44:52       3.771         {"mlm": 7.715388235146295, "mse": 0.0}  0.0568
0         train   95300/102467 588:33/44:15       3.775         {"mlm": 7.714197551891874, "mse": 0.0}  0.0618
0         train   95400/102467 589:09/43:38       3.780         {"mlm": 7.714463296025267, "mse": 0.0}  0.0525
0         train   95500/102467 589:44/43:01       3.784         {"mlm": 7.714706116270159, "mse": 0.0}  0.0555
0         train   95600/102467 590:20/42:24       3.788         {"mlm": 7.714933368828478, "mse": 0.0}  0.0571
0         train   95700/102467 590:55/41:47       3.792         {"mlm": 7.714742105335734, "mse": 0.0}  0.0531
0         train   95800/102467 591:31/41:09       3.796         {"mlm": 7.715546644304167, "mse": 0.0}  0.0914
0         train   95900/102467 592:07/40:32       3.800         {"mlm": 7.716502825502852, "mse": 0.0}  0.0533
0         train   96000/102467 592:42/39:55       3.804         {"mlm": 7.716678443255725, "mse": 0.0}  0.0585
0         train   96100/102467 593:18/39:18       3.808         {"mlm": 7.712816071264522, "mse": 0.0}  0.0644
0         train   96200/102467 593:53/38:41       3.812         {"mlm": 7.720089733298055, "mse": 0.0}  0.0644
0         train   96300/102467 594:29/38:04       3.816         {"mlm": 7.723625814071809, "mse": 0.0}  0.056
0         train   96400/102467 595:04/37:27       3.820         {"mlm": 7.719404084856624, "mse": 0.0}  0.0521
0         train   96500/102467 595:40/36:49       3.825         {"mlm": 7.714568270524023, "mse": 0.0}  0.0581
0         train   96600/102467 596:16/36:12       3.829         {"mlm": 7.713210694554263, "mse": 0.0}  0.0521
0         train   96700/102467 596:51/35:35       3.833         {"mlm": 7.70986823679898, "mse": 0.0}  0.0562
0         train   96800/102467 597:27/34:58       3.837         {"mlm": 7.711294095220051, "mse": 0.0}  0.0601
0         train   96900/102467 598:02/34:21       3.841         {"mlm": 7.711175810134531, "mse": 0.0}  0.0507
0         train   97000/102467 598:38/33:44       3.845         {"mlm": 7.712538372430065, "mse": 0.0}  0.0589
0         train   97100/102467 599:13/33:07       3.849         {"mlm": 7.712253367563976, "mse": 0.0}  0.0572
0         train   97200/102467 599:49/32:30       3.853         {"mlm": 7.714611073783167, "mse": 0.0}  0.0546
0         train   97300/102467 600:24/31:53       3.856         {"mlm": 7.714576673397397, "mse": 0.0}  0.0548
0         train   97400/102467 601:00/31:15       3.860         {"mlm": 7.7145468135689015, "mse": 0.0}  0.0756
0         train   97500/102467 601:36/30:38       3.864         {"mlm": 7.71507674372029, "mse": 0.0}  0.0543
0         train   97600/102467 602:11/30:01       3.868         {"mlm": 7.71524398358225, "mse": 0.0}  0.0577
0         train   97700/102467 602:47/29:24       3.872         {"mlm": 7.716077045616011, "mse": 0.0}  0.0643
0         train   97800/102467 603:22/28:47       3.876         {"mlm": 7.716360842577934, "mse": 0.0}  0.0586
0         train   97900/102467 603:58/28:10       3.880         {"mlm": 7.715700664329227, "mse": 0.0}  0.0623
0         train   98000/102467 604:33/27:33       3.884         {"mlm": 7.716394324629798, "mse": 0.0}  0.0571
0         train   98100/102467 605:09/26:56       3.888         {"mlm": 7.7232678184906645, "mse": 0.0}  0.0543
0         train   98200/102467 605:44/26:19       3.892         {"mlm": 7.717263282561789, "mse": 0.0}  0.0488
0         train   98300/102467 606:20/25:42       3.896         {"mlm": 7.724062205971898, "mse": 0.0}  0.0546
0         train   98400/102467 606:55/25:05       3.900         {"mlm": 7.725324964282488, "mse": 0.0}  0.0657
0         train   98500/102467 607:31/24:28       3.904         {"mlm": 7.72497022151947, "mse": 0.0}  0.0485
0         train   98600/102467 608:06/23:50       3.907         {"mlm": 7.7211077005271145, "mse": 0.0}  0.0574
0         train   98700/102467 608:42/23:13       3.911         {"mlm": 7.720023107254642, "mse": 0.0}  0.0499
0         train   98800/102467 609:17/22:36       3.915         {"mlm": 7.717929549552688, "mse": 0.0}  0.0528
0         train   98900/102467 609:53/21:59       3.919         {"mlm": 7.713664308190346, "mse": 0.0}  0.0542
0         train   99000/102467 610:28/21:22       3.923         {"mlm": 7.715006138426233, "mse": 0.0}  0.0614
0         train   99100/102467 611:04/20:45       3.927         {"mlm": 7.715443627677694, "mse": 0.0}  0.0611
0         train   99200/102467 611:39/20:08       3.930         {"mlm": 7.7152647051125465, "mse": 0.0}  0.055
0         train   99300/102467 612:15/19:31       3.934         {"mlm": 7.71583804929698, "mse": 0.0}  0.0497
0         train   99400/102467 612:50/18:54       3.938         {"mlm": 7.716764517363299, "mse": 0.0}  0.0491
0         train   99500/102467 613:26/18:17       3.942         {"mlm": 7.71655039545049, "mse": 0.0}  0.0489
0         train   99600/102467 614:02/17:40       3.946         {"mlm": 7.717606244529399, "mse": 0.0}  0.0528
0         train   99700/102467 614:37/17:03       3.949         {"mlm": 7.717556668340035, "mse": 0.0}  0.0547
0         train   99800/102467 615:13/16:26       3.953         {"mlm": 7.717807211429876, "mse": 0.0}  0.0593
0         train   99900/102467 615:48/15:49       3.957         {"mlm": 7.718269982166934, "mse": 0.0}  0.0519
0         train   100000/102467 616:24/15:12      3.961         {"mlm": 7.719769096804526, "mse": 0.0}  0.0551

10/05/2022 00:43:00 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/0-step100000.pkl
0         valid   1/781        0:10/137:02        7.682           None
0         valid   101/781      0:25/ 2:52         7.693           None
0         valid   201/781      0:40/ 1:57         7.711           None
0         valid   301/781      0:55/ 1:29         7.715           None
0         valid   401/781      1:11/ 1:07         7.724           None
0         valid   501/781      1:26/ 0:48         7.724           None
0         valid   601/781      1:41/ 0:30         7.727           None
0         valid   701/781      1:56/ 0:13         7.730           None
0         valid   781/781      2:12/ 0:00         7.720         {"mlm": 7.72039052496799, "mse": 0.0, "train": 0.0}  None
0         train   100100/102467 619:12/14:38      3.965         {"mlm": 7.732677550315857, "mse": 0.0}  0.0561
0         train   100200/102467 619:48/14:01      3.968         {"mlm": 7.726791865825653, "mse": 0.0}  0.0615
0         train   100300/102467 620:23/13:24      3.972         {"mlm": 7.71538445631663, "mse": 0.0}  0.0473
0         train   100400/102467 620:59/12:47      3.976         {"mlm": 7.710993834733963, "mse": 0.0}  0.0511
0         train   100500/102467 621:34/12:09      3.979         {"mlm": 7.713199306488037, "mse": 0.0}  0.0593
0         train   100600/102467 622:10/11:32      3.983         {"mlm": 7.716720600922902, "mse": 0.0}  0.053
0         train   100700/102467 622:45/10:55      3.987         {"mlm": 7.716790755135673, "mse": 0.0}  0.0569
0         train   100800/102467 623:21/10:18      3.991         {"mlm": 7.719188199043274, "mse": 0.0}  0.0536
0         train   100900/102467 623:56/ 9:41      3.994         {"mlm": 7.718301643795438, "mse": 0.0}  0.0495
0         train   101000/102467 624:32/ 9:04      3.998         {"mlm": 7.7225790691375735, "mse": 0.0}  0.0539
0         train   101100/102467 625:07/ 8:27      4.002         {"mlm": 7.721626765077764, "mse": 0.0}  0.0474
0         train   101200/102467 625:43/ 7:50      4.005         {"mlm": 7.72165785908699, "mse": 0.0}  0.0537
0         train   101300/102467 626:18/ 7:12      4.009         {"mlm": 7.720977011827322, "mse": 0.0}  0.0772
0         train   101400/102467 626:54/ 6:35      4.013         {"mlm": 7.723161377566202, "mse": 0.0}  0.0594
0         train   101500/102467 627:30/ 5:58      4.016         {"mlm": 7.721827608426412, "mse": 0.0}  0.0559
0         train   101600/102467 628:05/ 5:21      4.020         {"mlm": 7.721011189818382, "mse": 0.0}  0.0537
0         train   101700/102467 628:41/ 4:44      4.024         {"mlm": 7.72100717123817, "mse": 0.0}  0.0561
0         train   101800/102467 629:16/ 4:07      4.027         {"mlm": 7.721141571468777, "mse": 0.0}  0.0554
0         train   101900/102467 629:52/ 3:30      4.031         {"mlm": 7.721027117528413, "mse": 0.0}  0.0503
0         train   102000/102467 630:28/ 2:53      4.035         {"mlm": 7.719910797119141, "mse": 0.0}  0.0876
0         train   102100/102467 631:03/ 2:16      4.038         {"mlm": 7.7223585880163945, "mse": 0.0}  0.0499
0         train   102200/102467 631:39/ 1:39      4.042         {"mlm": 7.724343661686883, "mse": 0.0}  0.0545
0         train   102300/102467 632:14/ 1:01      4.045         {"mlm": 7.72699678382746, "mse": 0.0}  0.0571
0         train   102400/102467 632:50/ 0:24      4.049         {"mlm": 7.726657379838757, "mse": 0.0}  0.0654
True False
skip validation
1         train   100/102467   0:51/872:51        7.731         {"mlm": 7.731177129745483, "mse": 0.0}  0.0743
1         train   200/102467   1:26/740:28        7.728         {"mlm": 7.727635998725891, "mse": 0.0}  0.0516
1         train   300/102467   2:02/695:39        7.730         {"mlm": 7.730494772593181, "mse": 0.0}  0.0493
1         train   400/102467   2:38/672:42        7.730         {"mlm": 7.729752827882766, "mse": 0.0}  0.0667
1         train   500/102467   3:13/658:55        7.728         {"mlm": 7.727869069099426, "mse": 0.0}  0.0489
1         train   600/102467   3:49/649:13        7.724         {"mlm": 7.723940387566884, "mse": 0.0}  0.0544
1         train   700/102467   4:24/642:05        7.722         {"mlm": 7.721844054630824, "mse": 0.0}  0.0523
1         train   800/102467   5:00/636:37        7.721         {"mlm": 7.720704390406609, "mse": 0.0}  0.06
1         train   900/102467   5:36/632:17        7.722         {"mlm": 7.721589458783468, "mse": 0.0}  0.0457
1         train   1000/102467  6:11/628:41        7.719         {"mlm": 7.719400773048401, "mse": 0.0}  0.0476
1         train   1100/102467  6:47/625:40        7.720         {"mlm": 7.719652025482871, "mse": 0.0}  0.0575
1         train   1200/102467  7:22/623:03        7.721         {"mlm": 7.720823633670807, "mse": 0.0}  0.048
1         train   1300/102467  7:58/620:46        7.720         {"mlm": 7.720471949210534, "mse": 0.0}  0.0669
1         train   1400/102467  8:34/618:41        7.721         {"mlm": 7.721396659442357, "mse": 0.0}  0.052
1         train   1500/102467  9:09/616:49        7.721         {"mlm": 7.721345594088237, "mse": 0.0}  0.076
1         train   1600/102467  9:45/615:07        7.721         {"mlm": 7.720925926566124, "mse": 0.0}  0.0509
1         train   1700/102467 10:21/613:35        7.722         {"mlm": 7.721733505866107, "mse": 0.0}  0.055
1         train   1800/102467 10:56/612:11        7.721         {"mlm": 7.721290188365512, "mse": 0.0}  0.0605
1         train   1900/102467 11:32/610:45        7.720         {"mlm": 7.720306408781754, "mse": 0.0}  0.0585
1         train   2000/102467 12:07/609:25        7.720         {"mlm": 7.720272901296616, "mse": 0.0}  0.0608
1         train   2100/102467 12:43/608:09        7.720         {"mlm": 7.7203123352744365, "mse": 0.0}  0.0567
1         train   2200/102467 13:19/606:59        7.720         {"mlm": 7.718141939172793, "mse": 0.0}  0.0463
1         train   2300/102467 13:54/605:46        7.721         {"mlm": 7.72331222004715, "mse": 0.0}  0.0556
1         train   2400/102467 14:30/604:39        7.720         {"mlm": 7.719841462328918, "mse": 0.0}  0.0531
1         train   2500/102467 15:05/603:35        7.720         {"mlm": 7.718812873702728, "mse": 0.0}  0.0541
1         train   2600/102467 15:41/602:30        7.719         {"mlm": 7.714054199212382, "mse": 0.0}  0.0554
1         train   2700/102467 16:16/601:30        7.718         {"mlm": 7.712131633267382, "mse": 0.0}  0.0546
1         train   2800/102467 16:52/600:32        7.718         {"mlm": 7.713913396541705, "mse": 0.0}  0.0468
1         train   2900/102467 17:27/599:36        7.718         {"mlm": 7.7127684491362265, "mse": 0.0}  0.0536
1         train   3000/102467 18:03/598:41        7.718         {"mlm": 7.71440114297189, "mse": 0.0}  0.0471
1         train   3100/102467 18:38/597:47        7.718         {"mlm": 7.712862415678182, "mse": 0.0}  0.0559
1         train   3200/102467 19:14/596:54        7.717         {"mlm": 7.7127781061454055, "mse": 0.0}  0.0562
1         train   3300/102467 19:50/596:01        7.718         {"mlm": 7.713633395232449, "mse": 0.0}  0.0487
1         train   3400/102467 20:25/595:09        7.717         {"mlm": 7.711315479851178, "mse": 0.0}  0.0601
1         train   3500/102467 21:01/594:20        7.717         {"mlm": 7.712574769847786, "mse": 0.0}  0.0533
1         train   3600/102467 21:36/593:31        7.717         {"mlm": 7.712058715331248, "mse": 0.0}  0.0481
1         train   3700/102467 22:12/592:40        7.716         {"mlm": 7.711835278561004, "mse": 0.0}  0.0468
1         train   3800/102467 22:47/591:52        7.716         {"mlm": 7.7110680049495475, "mse": 0.0}  0.0574
1         train   3900/102467 23:23/591:04        7.715         {"mlm": 7.710408675538797, "mse": 0.0}  0.0673
1         train   4000/102467 23:58/590:18        7.715         {"mlm": 7.709696174323887, "mse": 0.0}  0.051
1         train   4100/102467 24:34/589:32        7.715         {"mlm": 7.726795391160614, "mse": 0.0}  0.0496
1         train   4200/102467 25:09/588:46        7.715         {"mlm": 7.715501850301569, "mse": 0.0}  0.0629
1         train   4300/102467 25:45/588:03        7.715         {"mlm": 7.712703324004307, "mse": 0.0}  0.0616
1         train   4400/102467 26:21/587:20        7.715         {"mlm": 7.714164385244475, "mse": 0.0}  0.06
1         train   4500/102467 26:56/586:37        7.715         {"mlm": 7.715582370758057, "mse": 0.0}  0.0592
1         train   4600/102467 27:32/585:54        7.715         {"mlm": 7.717000845682661, "mse": 0.0}  0.0548
1         train   4700/102467 28:07/585:11        7.716         {"mlm": 7.718956200646124, "mse": 0.0}  0.0478
1         train   4800/102467 28:43/584:27        7.715         {"mlm": 7.715727715265183, "mse": 0.0}  0.0478
1         train   4900/102467 29:19/583:45        7.715         {"mlm": 7.715912110024943, "mse": 0.0}  0.0679
1         train   5000/102467 29:54/583:03        7.715         {"mlm": 7.716365469720416, "mse": 0.0}  0.0561
1         train   5100/102467 30:30/582:21        7.715         {"mlm": 7.716494271881159, "mse": 0.0}  0.0523
1         train   5200/102467 31:05/581:38        7.715         {"mlm": 7.7155774905406975, "mse": 0.0}  0.0564
1         train   5300/102467 31:41/580:57        7.716         {"mlm": 7.718115178021518, "mse": 0.0}  0.0495
1         train   5400/102467 32:16/580:15        7.716         {"mlm": 7.719381894505928, "mse": 0.0}  0.0566
1         train   5500/102467 32:52/579:33        7.716         {"mlm": 7.720689993834146, "mse": 0.0}  0.0525
1         train   5600/102467 33:27/578:51        7.717         {"mlm": 7.721310784133415, "mse": 0.0}  0.0513
1         train   5700/102467 34:03/578:10        7.717         {"mlm": 7.7212670559877505, "mse": 0.0}  0.0556
1         train   5800/102467 34:38/577:28        7.717         {"mlm": 7.7211716692227546, "mse": 0.0}  0.0515
1         train   5900/102467 35:14/576:47        7.717         {"mlm": 7.720841967770624, "mse": 0.0}  0.0513
1         train   6000/102467 35:49/576:06        7.717         {"mlm": 7.721075848416165, "mse": 0.0}  0.0505
1         train   6100/102467 36:25/575:24        7.717         {"mlm": 7.716741144042654, "mse": 0.0}  0.055
1         train   6200/102467 37:00/574:44        7.717         {"mlm": 7.7288010471363355, "mse": 0.0}  0.0505
1         train   6300/102467 37:36/574:04        7.717         {"mlm": 7.724264006823402, "mse": 0.0}  0.0486
1         train   6400/102467 38:12/573:24        7.717         {"mlm": 7.7187751330416505, "mse": 0.0}  0.0585
1         train   6500/102467 38:47/572:45        7.717         {"mlm": 7.723120079194036, "mse": 0.0}  0.0598
1         train   6600/102467 39:23/572:06        7.717         {"mlm": 7.721468050875257, "mse": 0.0}  0.0493
1         train   6700/102467 39:58/571:27        7.718         {"mlm": 7.72272589695847, "mse": 0.0}  0.0505
1         train   6800/102467 40:34/570:48        7.717         {"mlm": 7.720888247902749, "mse": 0.0}  0.0498
1         train   6900/102467 41:09/570:10        7.717         {"mlm": 7.717550769431668, "mse": 0.0}  0.0504
1         train   7000/102467 41:45/569:32        7.717         {"mlm": 7.718832036556906, "mse": 0.0}  0.0478
1         train   7100/102467 42:21/568:54        7.717         {"mlm": 7.716226356075151, "mse": 0.0}  0.0662
1         train   7200/102467 42:56/568:16        7.717         {"mlm": 7.715237034293344, "mse": 0.0}  0.0604
1         train   7300/102467 43:32/567:38        7.717         {"mlm": 7.71677319012703, "mse": 0.0}  0.051
1         train   7400/102467 44:08/567:00        7.717         {"mlm": 7.715691918377886, "mse": 0.0}  0.0537
1         train   7500/102467 44:43/566:22        7.717         {"mlm": 7.71602106795123, "mse": 0.0}  0.0484
1         train   7600/102467 45:19/565:45        7.717         {"mlm": 7.716997348448599, "mse": 0.0}  0.0719
1         train   7700/102467 45:55/565:08        7.717         {"mlm": 7.716004396932014, "mse": 0.0}  0.054
1         train   7800/102467 46:30/564:30        7.717         {"mlm": 7.7154592239133635, "mse": 0.0}  0.0539
1         train   7900/102467 47:06/563:52        7.717         {"mlm": 7.715232745307838, "mse": 0.0}  0.0499
1         train   8000/102467 47:41/563:13        7.716         {"mlm": 7.714905742412218, "mse": 0.0}  0.0592
1         train   8100/102467 48:17/562:35        7.716         {"mlm": 7.702184314529101, "mse": 0.0}  0.066
1         train   8200/102467 48:52/561:57        7.716         {"mlm": 7.704955901418414, "mse": 0.0}  0.0534
1         train   8300/102467 49:28/561:19        7.716         {"mlm": 7.709215676462328, "mse": 0.0}  0.0444
1         train   8400/102467 50:04/560:41        7.716         {"mlm": 7.712375468677944, "mse": 0.0}  0.0518
1         train   8500/102467 50:39/560:02        7.717         {"mlm": 7.718346832260009, "mse": 0.0}  0.0539
1         train   8600/102467 51:15/559:24        7.716         {"mlm": 7.714227194754069, "mse": 0.0}  0.0484
1         train   8700/102467 51:50/558:46        7.717         {"mlm": 7.717588973456416, "mse": 0.0}  0.0677
1         train   8800/102467 52:26/558:08        7.716         {"mlm": 7.716370423834528, "mse": 0.0}  0.0689
1         train   8900/102467 53:01/557:30        7.716         {"mlm": 7.713314187313829, "mse": 0.0}  0.0498
1         train   9000/102467 53:37/556:51        7.716         {"mlm": 7.714462854776038, "mse": 0.0}  0.0457
1         train   9100/102467 54:12/556:13        7.716         {"mlm": 7.715838113405408, "mse": 0.0}  0.0537
1         train   9200/102467 54:48/555:35        7.716         {"mlm": 7.715823175915109, "mse": 0.0}  0.0531
1         train   9300/102467 55:23/554:57        7.717         {"mlm": 7.716956816337727, "mse": 0.0}  0.0594
1         train   9400/102467 55:59/554:18        7.716         {"mlm": 7.715055168870527, "mse": 0.0}  0.0467
1         train   9500/102467 56:34/553:40        7.716         {"mlm": 7.716668313199824, "mse": 0.0}  0.0547
1         train   9600/102467 57:10/553:03        7.717         {"mlm": 7.7178350803547335, "mse": 0.0}  0.0559
1         train   9700/102467 57:45/552:25        7.717         {"mlm": 7.718467394979495, "mse": 0.0}  0.055
1         train   9800/102467 58:21/551:48        7.717         {"mlm": 7.7180922928791, "mse": 0.0}  0.0535
1         train   9900/102467 58:56/551:11        7.717         {"mlm": 7.718067167680474, "mse": 0.0}  0.0521
1         train   10000/102467 59:32/550:35       7.717         {"mlm": 7.718104019910395, "mse": 0.0}  0.0469

10/05/2022 01:59:24 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step10000.pkl
1         valid   1/781        0:10/136:31        7.619           None
1         valid   101/781      0:25/ 2:52         7.710           None
1         valid   201/781      0:40/ 1:57         7.723           None
1         valid   301/781      0:55/ 1:29         7.721           None
1         valid   401/781      1:11/ 1:07         7.723           None
1         valid   501/781      1:26/ 0:48         7.721           None
1         valid   601/781      1:41/ 0:30         7.720           None
1         valid   701/781      1:56/ 0:13         7.721           None
1         valid   781/781      2:12/ 0:00         7.719         {"mlm": 7.719270166453265, "mse": 0.0, "train": 0.0}  None
1         train   10100/102467 62:20/570:10       7.717         {"mlm": 7.723641839027405, "mse": 0.0}  0.0532
1         train   10200/102467 62:56/569:18       7.717         {"mlm": 7.711105773448944, "mse": 0.0}  0.0562
1         train   10300/102467 63:31/568:26       7.717         {"mlm": 7.712662612597148, "mse": 0.0}  0.0519
1         train   10400/102467 64:06/567:35       7.717         {"mlm": 7.713134634494781, "mse": 0.0}  0.0619
1         train   10500/102467 64:42/566:45       7.716         {"mlm": 7.7109646291732785, "mse": 0.0}  0.0437
1         train   10600/102467 65:17/565:55       7.716         {"mlm": 7.708716725508372, "mse": 0.0}  0.0488
1         train   10700/102467 65:53/565:06       7.716         {"mlm": 7.7125602572304865, "mse": 0.0}  0.0474
1         train   10800/102467 66:28/564:16       7.716         {"mlm": 7.709835020899773, "mse": 0.0}  0.0514
1         train   10900/102467 67:04/563:27       7.716         {"mlm": 7.710865147908529, "mse": 0.0}  0.0502
1         train   11000/102467 67:39/562:38       7.716         {"mlm": 7.711478377819061, "mse": 0.0}  0.0506
1         train   11100/102467 68:15/561:49       7.716         {"mlm": 7.712771712216464, "mse": 0.0}  0.0558
1         train   11200/102467 68:50/561:01       7.716         {"mlm": 7.714176457722981, "mse": 0.0}  0.0465
1         train   11300/102467 69:26/560:13       7.716         {"mlm": 7.714286294717055, "mse": 0.0}  0.0473
1         train   11400/102467 70:01/559:24       7.717         {"mlm": 7.715108460017613, "mse": 0.0}  0.0437
1         train   11500/102467 70:37/558:36       7.717         {"mlm": 7.715692994435628, "mse": 0.0}  0.0504
1         train   11600/102467 71:12/557:47       7.717         {"mlm": 7.714919940829277, "mse": 0.0}  0.0498
1         train   11700/102467 71:47/557:00       7.716         {"mlm": 7.713963333017686, "mse": 0.0}  0.0463
1         train   11800/102467 72:23/556:12       7.716         {"mlm": 7.713443743122949, "mse": 0.0}  0.0489
1         train   11900/102467 72:58/555:25       7.716         {"mlm": 7.713527008357801, "mse": 0.0}  0.0546
1         train   12000/102467 73:34/554:38       7.716         {"mlm": 7.712852788686752, "mse": 0.0}  0.0473
1         train   12100/102467 74:09/553:51       7.716         {"mlm": 7.7068055615280615, "mse": 0.0}  0.0423
1         train   12200/102467 74:45/553:04       7.716         {"mlm": 7.716740265563505, "mse": 0.0}  0.0515
1         train   12300/102467 75:20/552:18       7.716         {"mlm": 7.720082828432421, "mse": 0.0}  0.0478
1         train   12400/102467 75:56/551:33       7.716         {"mlm": 7.720877737030947, "mse": 0.0}  0.0572
1         train   12500/102467 76:31/550:47       7.716         {"mlm": 7.7204357158683825, "mse": 0.0}  0.0511
1         train   12600/102467 77:07/550:01       7.716         {"mlm": 7.7190158832849365, "mse": 0.0}  0.0478
1         train   12700/102467 77:42/549:15       7.716         {"mlm": 7.718483464400656, "mse": 0.0}  0.0527
1         train   12800/102467 78:18/548:30       7.716         {"mlm": 7.717778096061774, "mse": 0.0}  0.0499
1         train   12900/102467 78:53/547:45       7.716         {"mlm": 7.715164841215921, "mse": 0.0}  0.0644
1         train   13000/102467 79:28/547:00       7.716         {"mlm": 7.7152375038918315, "mse": 0.0}  0.0443
1         train   13100/102467 80:04/546:15       7.716         {"mlm": 7.717374100915078, "mse": 0.0}  0.0461
1         train   13200/102467 80:39/545:30       7.716         {"mlm": 7.716280329913473, "mse": 0.0}  0.0471
1         train   13300/102467 81:15/544:45       7.716         {"mlm": 7.715824438848341, "mse": 0.0}  0.0566
1         train   13400/102467 81:50/544:01       7.716         {"mlm": 7.71625826851311, "mse": 0.0}  0.0523
1         train   13500/102467 82:26/543:17       7.716         {"mlm": 7.718156875332966, "mse": 0.0}  0.0434
1         train   13600/102467 83:01/542:33       7.716         {"mlm": 7.719135268618719, "mse": 0.0}  0.0511
1         train   13700/102467 83:37/541:49       7.717         {"mlm": 7.7197938450930605, "mse": 0.0}  0.0452
1         train   13800/102467 84:12/541:04       7.717         {"mlm": 7.719901549279392, "mse": 0.0}  0.0496
1         train   13900/102467 84:48/540:21       7.717         {"mlm": 7.71944388971635, "mse": 0.0}  0.059
1         train   14000/102467 85:23/539:36       7.716         {"mlm": 7.718491190251497, "mse": 0.0}  0.0488
1         train   14100/102467 85:59/538:52       7.716         {"mlm": 7.715656801145904, "mse": 0.0}  0.0534
1         train   14200/102467 86:34/538:08       7.716         {"mlm": 7.718481921186351, "mse": 0.0}  0.0553
1         train   14300/102467 87:09/537:24       7.717         {"mlm": 7.7214466021364965, "mse": 0.0}  0.0546
1         train   14400/102467 87:45/536:41       7.717         {"mlm": 7.721732620018811, "mse": 0.0}  0.0478
1         train   14500/102467 88:20/535:57       7.717         {"mlm": 7.722235850062236, "mse": 0.0}  0.0534
1         train   14600/102467 88:56/535:14       7.717         {"mlm": 7.72160175333055, "mse": 0.0}  0.0462
1         train   14700/102467 89:31/534:31       7.717         {"mlm": 7.721172659991463, "mse": 0.0}  0.0619
1         train   14800/102467 90:07/533:48       7.717         {"mlm": 7.721442087551107, "mse": 0.0}  0.0466
1         train   14900/102467 90:42/533:05       7.717         {"mlm": 7.7207495741429994, "mse": 0.0}  0.0448
1         train   15000/102467 91:17/532:22       7.717         {"mlm": 7.719920565465648, "mse": 0.0}  0.0481
1         train   15100/102467 91:53/531:39       7.717         {"mlm": 7.718908620879516, "mse": 0.0}  0.0532
1         train   15200/102467 92:28/530:57       7.717         {"mlm": 7.718789727540566, "mse": 0.0}  0.0459
1         train   15300/102467 93:04/530:14       7.717         {"mlm": 7.719475388710598, "mse": 0.0}  0.0466
1         train   15400/102467 93:39/529:31       7.717         {"mlm": 7.720769943256405, "mse": 0.0}  0.0559
1         train   15500/102467 94:15/528:49       7.717         {"mlm": 7.719350619055082, "mse": 0.0}  0.0476
1         train   15600/102467 94:50/528:07       7.717         {"mlm": 7.719739324906293, "mse": 0.0}  0.0426
1         train   15700/102467 95:26/527:25       7.717         {"mlm": 7.7197049486903895, "mse": 0.0}  0.0549
1         train   15800/102467 96:01/526:43       7.717         {"mlm": 7.720843300273076, "mse": 0.0}  0.0471
1         train   15900/102467 96:37/526:02       7.717         {"mlm": 7.720333465409354, "mse": 0.0}  0.0496
1         train   16000/102467 97:12/525:20       7.717         {"mlm": 7.720134421273156, "mse": 0.0}  0.0433
1         train   16100/102467 97:48/524:38       7.717         {"mlm": 7.692386371573222, "mse": 0.0}  0.0435
1         train   16200/102467 98:23/523:56       7.717         {"mlm": 7.701414715820158, "mse": 0.0}  0.0578
1         train   16300/102467 98:58/523:14       7.717         {"mlm": 7.70475523239033, "mse": 0.0}  0.0542
1         train   16400/102467 99:34/522:33       7.717         {"mlm": 7.706368189314451, "mse": 0.0}  0.0484
1         train   16500/102467 100:09/521:51      7.717         {"mlm": 7.70890992412145, "mse": 0.0}  0.0406
1         train   16600/102467 100:45/521:10      7.717         {"mlm": 7.709394812783604, "mse": 0.0}  0.0509
1         train   16700/102467 101:20/520:28      7.717         {"mlm": 7.712113116358753, "mse": 0.0}  0.0493
1         train   16800/102467 101:56/519:47      7.717         {"mlm": 7.708895825083313, "mse": 0.0}  0.0513
1         train   16900/102467 102:31/519:06      7.716         {"mlm": 7.707963998766913, "mse": 0.0}  0.0475
1         train   17000/102467 103:07/518:25      7.716         {"mlm": 7.70932289925119, "mse": 0.0}  0.0486
1         train   17100/102467 103:42/517:43      7.716         {"mlm": 7.709376755038933, "mse": 0.0}  0.0588
1         train   17200/102467 104:17/517:02      7.716         {"mlm": 7.709711201906005, "mse": 0.0}  0.048
1         train   17300/102467 104:53/516:21      7.717         {"mlm": 7.712083712852084, "mse": 0.0}  0.0442
1         train   17400/102467 105:28/515:40      7.717         {"mlm": 7.711880382505075, "mse": 0.0}  0.0448
1         train   17500/102467 106:04/514:59      7.717         {"mlm": 7.712868599710101, "mse": 0.0}  0.0493
1         train   17600/102467 106:39/514:19      7.717         {"mlm": 7.714747336631875, "mse": 0.0}  0.0482
1         train   17700/102467 107:15/513:38      7.717         {"mlm": 7.714119500388942, "mse": 0.0}  0.0449
1         train   17800/102467 107:50/512:57      7.717         {"mlm": 7.713428862969213, "mse": 0.0}  0.047
1         train   17900/102467 108:26/512:17      7.717         {"mlm": 7.7130794510064655, "mse": 0.0}  0.0466
1         train   18000/102467 109:01/511:36      7.717         {"mlm": 7.714309583500617, "mse": 0.0}  0.0426
1         train   18100/102467 109:36/510:56      7.717         {"mlm": 7.709596251447995, "mse": 0.0}  0.0514
1         train   18200/102467 110:12/510:16      7.717         {"mlm": 7.713148214379135, "mse": 0.0}  0.0515
1         train   18300/102467 110:48/509:36      7.717         {"mlm": 7.723482884265281, "mse": 0.0}  0.0496
1         train   18400/102467 111:23/508:56      7.717         {"mlm": 7.717948244075583, "mse": 0.0}  0.0465
1         train   18500/102467 111:59/508:16      7.717         {"mlm": 7.717925448571482, "mse": 0.0}  0.0471
1         train   18600/102467 112:34/507:36      7.717         {"mlm": 7.718366355703981, "mse": 0.0}  0.0596
1         train   18700/102467 113:10/506:57      7.717         {"mlm": 7.716375952479483, "mse": 0.0}  0.0453
1         train   18800/102467 113:45/506:17      7.717         {"mlm": 7.71804564502371, "mse": 0.0}  0.0463
1         train   18900/102467 114:21/505:38      7.717         {"mlm": 7.717055648565292, "mse": 0.0}  0.0656
1         train   19000/102467 114:57/504:59      7.717         {"mlm": 7.717018182976657, "mse": 0.0}  0.045
1         train   19100/102467 115:32/504:20      7.717         {"mlm": 7.717480539405433, "mse": 0.0}  0.0468
1         train   19200/102467 116:08/503:41      7.717         {"mlm": 7.716832373453223, "mse": 0.0}  0.0533
1         train   19300/102467 116:44/503:02      7.717         {"mlm": 7.71676324583866, "mse": 0.0}  0.0469
1         train   19400/102467 117:19/502:22      7.717         {"mlm": 7.716499845756159, "mse": 0.0}  0.0493
1         train   19500/102467 117:55/501:43      7.717         {"mlm": 7.7170869140701495, "mse": 0.0}  0.0504
1         train   19600/102467 118:30/501:04      7.717         {"mlm": 7.717239122641714, "mse": 0.0}  0.0524
1         train   19700/102467 119:06/500:24      7.717         {"mlm": 7.718941955915037, "mse": 0.0}  0.0557
1         train   19800/102467 119:41/499:45      7.717         {"mlm": 7.719070725292299, "mse": 0.0}  0.0565
1         train   19900/102467 120:17/499:06      7.717         {"mlm": 7.7186459388410995, "mse": 0.0}  0.0485
1         train   20000/102467 120:53/498:26      7.717         {"mlm": 7.719180873735157, "mse": 0.0}  0.041

10/05/2022 03:00:44 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step20000.pkl
1         valid   1/781        0:10/139:18        7.767           None
1         valid   101/781      0:25/ 2:53         7.715           None
1         valid   201/781      0:40/ 1:58         7.718           None
1         valid   301/781      0:56/ 1:29         7.721           None
1         valid   401/781      1:11/ 1:07         7.726           None
1         valid   501/781      1:26/ 0:48         7.722           None
1         valid   601/781      1:41/ 0:30         7.723           None
1         valid   701/781      1:56/ 0:13         7.721           None
1         valid   781/781      2:12/ 0:00         7.717         {"mlm": 7.717349551856594, "mse": 0.0, "train": 0.0}  None
1         train   20100/102467 123:41/506:50      7.717         {"mlm": 7.712754769325256, "mse": 0.0}  0.0463
1         train   20200/102467 124:16/506:08      7.717         {"mlm": 7.705283329486847, "mse": 0.0}  0.0553
1         train   20300/102467 124:52/505:25      7.717         {"mlm": 7.710427967707316, "mse": 0.0}  0.0549
1         train   20400/102467 125:27/504:42      7.717         {"mlm": 7.711307075023651, "mse": 0.0}  0.0444
1         train   20500/102467 126:02/503:59      7.717         {"mlm": 7.70660048866272, "mse": 0.0}  0.0395
1         train   20600/102467 126:38/503:17      7.717         {"mlm": 7.710667819976806, "mse": 0.0}  0.0512
1         train   20700/102467 127:13/502:34      7.717         {"mlm": 7.710732417787824, "mse": 0.0}  0.0589
1         train   20800/102467 127:49/501:52      7.717         {"mlm": 7.7109480738639835, "mse": 0.0}  0.049
1         train   20900/102467 128:24/501:10      7.716         {"mlm": 7.705630324151781, "mse": 0.0}  0.0566
1         train   21000/102467 129:00/500:28      7.716         {"mlm": 7.704131373405456, "mse": 0.0}  0.0488
1         train   21100/102467 129:36/499:46      7.716         {"mlm": 7.703437660824169, "mse": 0.0}  0.0516
1         train   21200/102467 130:11/499:04      7.716         {"mlm": 7.7077965009212495, "mse": 0.0}  0.0457
1         train   21300/102467 130:47/498:22      7.716         {"mlm": 7.7084730452757615, "mse": 0.0}  0.0421
1         train   21400/102467 131:22/497:41      7.716         {"mlm": 7.709968013763428, "mse": 0.0}  0.0409
1         train   21500/102467 131:58/496:59      7.717         {"mlm": 7.712063188234965, "mse": 0.0}  0.045
1         train   21600/102467 132:33/496:18      7.717         {"mlm": 7.713535352051258, "mse": 0.0}  0.0452
1         train   21700/102467 133:09/495:36      7.717         {"mlm": 7.715650851866778, "mse": 0.0}  0.0439
1         train   21800/102467 133:45/494:55      7.717         {"mlm": 7.714313877158695, "mse": 0.0}  0.0467
1         train   21900/102467 134:20/494:13      7.717         {"mlm": 7.715485755518863, "mse": 0.0}  0.0537
1         train   22000/102467 134:56/493:32      7.717         {"mlm": 7.717001205444336, "mse": 0.0}  0.0453
1         train   22100/102467 135:31/492:51      7.717         {"mlm": 7.739931915745591, "mse": 0.0}  0.0434
1         train   22200/102467 136:07/492:10      7.717         {"mlm": 7.717427452604975, "mse": 0.0}  0.0452
1         train   22300/102467 136:42/491:28      7.717         {"mlm": 7.720250978118998, "mse": 0.0}  0.0463
1         train   22400/102467 137:18/490:47      7.717         {"mlm": 7.722207449432602, "mse": 0.0}  0.0439
1         train   22500/102467 137:53/490:06      7.717         {"mlm": 7.723341702937124, "mse": 0.0}  0.0488
1         train   22600/102467 138:29/489:24      7.717         {"mlm": 7.721944754828197, "mse": 0.0}  0.0509
1         train   22700/102467 139:04/488:43      7.717         {"mlm": 7.72092993303771, "mse": 0.0}  0.0483
1         train   22800/102467 139:40/488:02      7.717         {"mlm": 7.721802494254369, "mse": 0.0}  0.0527
1         train   22900/102467 140:16/487:21      7.717         {"mlm": 7.720782005216707, "mse": 0.0}  0.0523
1         train   23000/102467 140:51/486:41      7.717         {"mlm": 7.720007618626317, "mse": 0.0}  0.061
1         train   23100/102467 141:27/486:00      7.717         {"mlm": 7.719206346176883, "mse": 0.0}  0.0499
1         train   23200/102467 142:02/485:19      7.717         {"mlm": 7.718103077532949, "mse": 0.0}  0.0522
1         train   23300/102467 142:38/484:38      7.717         {"mlm": 7.718707589758094, "mse": 0.0}  0.0466
1         train   23400/102467 143:13/483:58      7.717         {"mlm": 7.71869451067463, "mse": 0.0}  0.0488
1         train   23500/102467 143:49/483:17      7.717         {"mlm": 7.719275664774237, "mse": 0.0}  0.0451
1         train   23600/102467 144:25/482:37      7.717         {"mlm": 7.719127064276069, "mse": 0.0}  0.0463
1         train   23700/102467 145:00/481:56      7.717         {"mlm": 7.71891596038037, "mse": 0.0}  0.0436
1         train   23800/102467 145:36/481:16      7.717         {"mlm": 7.7181322597145305, "mse": 0.0}  0.0422
1         train   23900/102467 146:11/480:35      7.717         {"mlm": 7.718884445981894, "mse": 0.0}  0.047
1         train   24000/102467 146:47/479:55      7.717         {"mlm": 7.718188746444222, "mse": 0.0}  0.045
1         train   24100/102467 147:22/479:14      7.717         {"mlm": 7.747024755088651, "mse": 0.0}  0.0446
1         train   24200/102467 147:58/478:34      7.717         {"mlm": 7.737430370215214, "mse": 0.0}  0.0526
1         train   24300/102467 148:33/477:53      7.717         {"mlm": 7.725934430256786, "mse": 0.0}  0.0449
1         train   24400/102467 149:09/477:13      7.717         {"mlm": 7.722640563495195, "mse": 0.0}  0.046
1         train   24500/102467 149:44/476:33      7.717         {"mlm": 7.721728944395441, "mse": 0.0}  0.042
1         train   24600/102467 150:20/475:52      7.717         {"mlm": 7.72117978035407, "mse": 0.0}  0.0572
1         train   24700/102467 150:55/475:12      7.717         {"mlm": 7.719144373705189, "mse": 0.0}  0.0477
1         train   24800/102467 151:31/474:32      7.717         {"mlm": 7.716981214389467, "mse": 0.0}  0.0441
1         train   24900/102467 152:07/473:52      7.717         {"mlm": 7.716517817469642, "mse": 0.0}  0.0424
1         train   25000/102467 152:42/473:11      7.717         {"mlm": 7.714915559860413, "mse": 0.0}  0.0453
1         train   25100/102467 153:18/472:31      7.717         {"mlm": 7.713353694245247, "mse": 0.0}  0.0481
1         train   25200/102467 153:53/471:51      7.717         {"mlm": 7.711999929807023, "mse": 0.0}  0.0411
1         train   25300/102467 154:29/471:11      7.717         {"mlm": 7.713839947534453, "mse": 0.0}  0.0433
1         train   25400/102467 155:04/470:31      7.717         {"mlm": 7.713833654387314, "mse": 0.0}  0.0518
1         train   25500/102467 155:40/469:52      7.717         {"mlm": 7.714879536023923, "mse": 0.0}  0.0504
1         train   25600/102467 156:15/469:12      7.717         {"mlm": 7.7156665805582705, "mse": 0.0}  0.0462
1         train   25700/102467 156:51/468:32      7.717         {"mlm": 7.716073309434457, "mse": 0.0}  0.0486
1         train   25800/102467 157:27/467:52      7.717         {"mlm": 7.715361807582376, "mse": 0.0}  0.0472
1         train   25900/102467 158:02/467:13      7.717         {"mlm": 7.715856348375374, "mse": 0.0}  0.0537
1         train   26000/102467 158:38/466:33      7.717         {"mlm": 7.716512059544897, "mse": 0.0}  0.0455
1         train   26100/102467 159:13/465:53      7.717         {"mlm": 7.707156638509219, "mse": 0.0}  0.0401
1         train   26200/102467 159:49/465:13      7.717         {"mlm": 7.715608337808987, "mse": 0.0}  0.0575
1         train   26300/102467 160:24/464:34      7.717         {"mlm": 7.713916569847852, "mse": 0.0}  0.0438
1         train   26400/102467 161:00/463:54      7.717         {"mlm": 7.7083373778412865, "mse": 0.0}  0.0452
1         train   26500/102467 161:35/463:15      7.717         {"mlm": 7.709162889591885, "mse": 0.0}  0.0547
1         train   26600/102467 162:11/462:35      7.717         {"mlm": 7.709121922152725, "mse": 0.0}  0.0558
1         train   26700/102467 162:47/461:55      7.717         {"mlm": 7.712726402145889, "mse": 0.0}  0.0463
1         train   26800/102467 163:22/461:16      7.717         {"mlm": 7.715565808893297, "mse": 0.0}  0.0427
1         train   26900/102467 163:58/460:37      7.717         {"mlm": 7.714387111174754, "mse": 0.0}  0.0433
1         train   27000/102467 164:33/459:57      7.717         {"mlm": 7.71301568259925, "mse": 0.0}  0.0412
1         train   27100/102467 165:09/459:18      7.717         {"mlm": 7.7126656130649005, "mse": 0.0}  0.055
1         train   27200/102467 165:44/458:38      7.717         {"mlm": 7.713345159962462, "mse": 0.0}  0.0479
1         train   27300/102467 166:20/457:59      7.717         {"mlm": 7.713735636694209, "mse": 0.0}  0.0649
1         train   27400/102467 166:55/457:20      7.717         {"mlm": 7.71393551106272, "mse": 0.0}  0.0469
1         train   27500/102467 167:31/456:40      7.717         {"mlm": 7.713915213315425, "mse": 0.0}  0.0465
1         train   27600/102467 168:06/456:01      7.717         {"mlm": 7.7159062420194715, "mse": 0.0}  0.0405
1         train   27700/102467 168:42/455:22      7.717         {"mlm": 7.715312220731341, "mse": 0.0}  0.0435
1         train   27800/102467 169:18/454:43      7.717         {"mlm": 7.715955849680956, "mse": 0.0}  0.0473
1         train   27900/102467 169:53/454:03      7.717         {"mlm": 7.716094257081252, "mse": 0.0}  0.0492
1         train   28000/102467 170:29/453:24      7.717         {"mlm": 7.716972733594563, "mse": 0.0}  0.0416
1         train   28100/102467 171:04/452:45      7.717         {"mlm": 7.7270592500766115, "mse": 0.0}  0.0419
1         train   28200/102467 171:40/452:06      7.717         {"mlm": 7.718556114605495, "mse": 0.0}  0.0467
1         train   28300/102467 172:15/451:26      7.717         {"mlm": 7.720906007934261, "mse": 0.0}  0.0441
1         train   28400/102467 172:51/450:47      7.717         {"mlm": 7.724865860409206, "mse": 0.0}  0.0475
1         train   28500/102467 173:26/450:08      7.717         {"mlm": 7.724143251296012, "mse": 0.0}  0.0537
1         train   28600/102467 174:02/449:29      7.717         {"mlm": 7.721579450888922, "mse": 0.0}  0.0426
1         train   28700/102467 174:37/448:50      7.717         {"mlm": 7.722980169044144, "mse": 0.0}  0.0469
1         train   28800/102467 175:13/448:11      7.717         {"mlm": 7.722144104128507, "mse": 0.0}  0.0452
1         train   28900/102467 175:48/447:32      7.717         {"mlm": 7.723770214510815, "mse": 0.0}  0.0553
1         train   29000/102467 176:24/446:54      7.717         {"mlm": 7.724400009017393, "mse": 0.0}  0.0508
1         train   29100/102467 177:00/446:15      7.717         {"mlm": 7.722926034544506, "mse": 0.0}  0.0443
1         train   29200/102467 177:35/445:36      7.717         {"mlm": 7.723393152390036, "mse": 0.0}  0.0501
1         train   29300/102467 178:11/444:57      7.717         {"mlm": 7.723438874806887, "mse": 0.0}  0.0468
1         train   29400/102467 178:46/444:19      7.717         {"mlm": 7.723086691517543, "mse": 0.0}  0.0451
1         train   29500/102467 179:22/443:40      7.717         {"mlm": 7.722252430125354, "mse": 0.0}  0.0479
1         train   29600/102467 179:58/443:01      7.717         {"mlm": 7.7226234829814215, "mse": 0.0}  0.0432
1         train   29700/102467 180:33/442:23      7.717         {"mlm": 7.721777828796855, "mse": 0.0}  0.0546
1         train   29800/102467 181:09/441:44      7.717         {"mlm": 7.720697889349243, "mse": 0.0}  0.0462
1         train   29900/102467 181:44/441:06      7.717         {"mlm": 7.720495193064967, "mse": 0.0}  0.0547
1         train   30000/102467 182:20/440:27      7.717         {"mlm": 7.720984987362114, "mse": 0.0}  0.0472

10/05/2022 04:02:12 - INFO - tools.train_tool -   Checkpoint saved to checkpoints/BERT-Adapter/1-step30000.pkl
1         valid   1/781        0:10/137:14        7.865           None
1         valid   101/781      0:25/ 2:52         7.710           None
1         valid   201/781      0:40/ 1:57         7.712           None
1         valid   301/781      0:55/ 1:29         7.713           None
1         valid   401/781      1:11/ 1:07         7.718           None
1         valid   501/781      1:26/ 0:48         7.715           None
1         valid   601/781      1:41/ 0:30         7.714           None
1         valid   701/781      1:56/ 0:13         7.716           None
1         valid   781/781      2:12/ 0:00         7.718         {"mlm": 7.717669654289373, "mse": 0.0, "train": 0.0}  None
1         train   30100/102467 185:08/445:08      7.717         {"mlm": 7.729976644515991, "mse": 0.0}  0.0566
1         train   30200/102467 185:44/444:28      7.717         {"mlm": 7.726857454776764, "mse": 0.0}  0.0447
1         train   30300/102467 186:20/443:48      7.717         {"mlm": 7.7189349826176965, "mse": 0.0}  0.0512
1         train   30400/102467 186:55/443:07      7.717         {"mlm": 7.7191808784008025, "mse": 0.0}  0.0562
1         train   30500/102467 187:31/442:27      7.717         {"mlm": 7.716662088394165, "mse": 0.0}  0.0462
1         train   30600/102467 188:06/441:47      7.717         {"mlm": 7.7182632764180505, "mse": 0.0}  0.0451
1         train   30700/102467 188:42/441:07      7.717         {"mlm": 7.718080010414123, "mse": 0.0}  0.0443
1         train   30800/102467 189:17/440:27      7.717         {"mlm": 7.7205019891262054, "mse": 0.0}  0.0524
1         train   30900/102467 189:53/439:47      7.717         {"mlm": 7.7195612086190115, "mse": 0.0}  0.0539
1         train   31000/102467 190:28/439:07      7.717         {"mlm": 7.7201920161247255, "mse": 0.0}  0.0587
1         train   31100/102467 191:04/438:27      7.717         {"mlm": 7.719266516078602, "mse": 0.0}  0.0428
1         train   31200/102467 191:39/437:47      7.717         {"mlm": 7.7211475149790445, "mse": 0.0}  0.0489
1         train   31300/102467 192:15/437:07      7.717         {"mlm": 7.721740718621474, "mse": 0.0}  0.0542
1         train   31400/102467 192:50/436:27      7.717         {"mlm": 7.721351643630436, "mse": 0.0}  0.0448
1         train   31500/102467 193:26/435:48      7.717         {"mlm": 7.720781852404277, "mse": 0.0}  0.0509
1         train   31600/102467 194:01/435:08      7.717         {"mlm": 7.7205190074443815, "mse": 0.0}  0.0542
